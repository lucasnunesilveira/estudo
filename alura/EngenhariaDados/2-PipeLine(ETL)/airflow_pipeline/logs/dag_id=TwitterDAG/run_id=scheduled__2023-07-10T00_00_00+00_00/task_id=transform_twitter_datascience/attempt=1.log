[2023-07-13T22:25:19.795-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:25:19.802-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:25:19.802-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:25:19.812-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-13T22:25:19.817-0300] {standard_task_runner.py:57} INFO - Started process 36392 to run task
[2023-07-13T22:25:19.821-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpbw6s3c7r']
[2023-07-13T22:25:19.822-0300] {standard_task_runner.py:85} INFO - Job 8: Subtask transform_twitter_datascience
[2023-07-13T22:25:19.861-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:25:19.921-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-13T22:25:19.924-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:25:19.925-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-10
[2023-07-13T22:25:21.256-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:21 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:25:21.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:25:21.269-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:25:21.269-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:25:21.270-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:25:21.270-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:25:21.270-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:25:21.308-0300] {spark_submit.py:492} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:631)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:271)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:234)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:119)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1022)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1022)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
[2023-07-13T22:25:21.309-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-07-13T22:25:21.339-0300] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 423, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-10. Error code is: 1.
[2023-07-13T22:25:21.341-0300] {taskinstance.py:1345} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T012519, end_date=20230714T012521
[2023-07-13T22:25:21.351-0300] {standard_task_runner.py:104} ERROR - Failed to execute job 8 for task transform_twitter_datascience (Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-10. Error code is: 1.; 36392)
[2023-07-13T22:25:21.357-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-13T22:25:21.365-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:31:10.152-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:31:10.156-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:31:10.157-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:31:10.165-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-13T22:31:10.167-0300] {standard_task_runner.py:57} INFO - Started process 38848 to run task
[2023-07-13T22:31:10.169-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpvlazazlf']
[2023-07-13T22:31:10.169-0300] {standard_task_runner.py:85} INFO - Job 8: Subtask transform_twitter_datascience
[2023-07-13T22:31:10.193-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:31:10.242-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-13T22:31:10.248-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:31:10.250-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-10
[2023-07-13T22:31:11.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:11 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:31:11.330-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:31:11.340-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:31:11.340-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:31:11.340-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:31:11.340-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:31:11.341-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:31:11.662-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:31:12.136-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-13T22:31:12.143-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SparkContext: Running Spark version 3.1.3
[2023-07-13T22:31:12.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:12.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:31:12.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:12.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:31:12.191-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:31:12.201-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:31:12.202-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:31:12.247-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:31:12.248-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:31:12.248-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:31:12.248-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:31:12.248-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:31:12.462-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO Utils: Successfully started service 'sparkDriver' on port 34501.
[2023-07-13T22:31:12.486-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:31:12.517-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:31:12.533-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:31:12.533-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:31:12.537-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:31:12.547-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0d960732-8569-475e-96ea-34a5821d3a16
[2023-07-13T22:31:12.565-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:31:12.577-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:31:12.728-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:31:12.775-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4040
[2023-07-13T22:31:12.908-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:31:12.925-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43419.
[2023-07-13T22:31:12.926-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO NettyBlockTransferService: Server created on 192.168.0.177:43419
[2023-07-13T22:31:12.927-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:31:12.932-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 43419, None)
[2023-07-13T22:31:12.935-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:43419 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 43419, None)
[2023-07-13T22:31:12.936-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 43419, None)
[2023-07-13T22:31:12.937-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 43419, None)
[2023-07-13T22:31:13.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-13T22:31:13.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:13 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:31:13.924-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:13 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
[2023-07-13T22:31:14.019-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:14 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 8 paths.
[2023-07-13T22:31:15.230-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:15.230-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:15.233-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:31:15.435-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-13T22:31:15.475-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-13T22:31:15.478-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:43419 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:31:15.481-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:15.488-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33608727 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:15.607-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:15.618-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:15.619-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:15.619-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:15.620-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:15.623-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:15.684-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-13T22:31:15.687-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-13T22:31:15.688-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:43419 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-13T22:31:15.688-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:15.697-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:15.698-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:31:15.732-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6291 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:15.742-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:31:15.824-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:31:16.022-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO CodeGenerator: Code generated in 114.274494 ms
[2023-07-13T22:31:16.047-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [empty row]
[2023-07-13T22:31:16.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:31:16.055-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9012, partition values: [empty row]
[2023-07-13T22:31:16.058-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [empty row]
[2023-07-13T22:31:16.060-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [empty row]
[2023-07-13T22:31:16.062-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4506, partition values: [empty row]
[2023-07-13T22:31:16.064-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [empty row]
[2023-07-13T22:31:16.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2023-07-13T22:31:16.083-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 359 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:16.085-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:31:16.089-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,457 s
[2023-07-13T22:31:16.091-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:16.091-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:31:16.093-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,485387 s
[2023-07-13T22:31:16.379-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:31:16.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:31:16.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-13T22:31:16.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:31:16.432-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:16.432-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:16.433-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:16.504-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:43419 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-13T22:31:16.505-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO CodeGenerator: Code generated in 34.080099 ms
[2023-07-13T22:31:16.508-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:43419 in memory (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:31:16.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO CodeGenerator: Code generated in 36.428312 ms
[2023-07-13T22:31:16.562-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.2 MiB)
[2023-07-13T22:31:16.569-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-13T22:31:16.570-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:43419 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:31:16.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:16.575-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33608727 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:16.624-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:16.626-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:16.626-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:16.626-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:16.626-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:16.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:16.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 195.4 KiB, free 434.0 MiB)
[2023-07-13T22:31:16.694-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 68.5 KiB, free 433.9 MiB)
[2023-07-13T22:31:16.695-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:43419 (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:31:16.695-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:16.696-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:16.696-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:31:16.701-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:16.702-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:31:16.747-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:16.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:16.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:16.797-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO CodeGenerator: Code generated in 17.866462 ms
[2023-07-13T22:31:16.799-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:31:16.820-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO CodeGenerator: Code generated in 18.351043 ms
[2023-07-13T22:31:16.837-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO CodeGenerator: Code generated in 3.908463 ms
[2023-07-13T22:31:16.856-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:31:16.862-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:31:16.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9012, partition values: [19549]
[2023-07-13T22:31:16.874-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:31:16.879-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:31:16.883-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4506, partition values: [19550]
[2023-07-13T22:31:16.887-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [19551]
[2023-07-13T22:31:16.897-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_202307132231162115516945715594172_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-10/_temporary/0/task_202307132231162115516945715594172_0001_m_000000
[2023-07-13T22:31:16.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO SparkHadoopMapRedUtil: attempt_202307132231162115516945715594172_0001_m_000000_1: Committed
[2023-07-13T22:31:16.902-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3076 bytes result sent to driver
[2023-07-13T22:31:16.904-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 206 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:16.904-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:31:16.905-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,277 s
[2023-07-13T22:31:16.905-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:16.905-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:31:16.906-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,281663 s
[2023-07-13T22:31:16.920-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileFormatWriter: Write Job e3f09712-0177-4702-9c02-33e2bd4cdad2 committed.
[2023-07-13T22:31:16.922-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileFormatWriter: Finished processing stats for write job e3f09712-0177-4702-9c02-33e2bd4cdad2.
[2023-07-13T22:31:16.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:31:16.952-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:16.952-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:16.952-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:31:16.960-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:16.960-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:16.961-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:16.980-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO CodeGenerator: Code generated in 8.964717 ms
[2023-07-13T22:31:16.994-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO CodeGenerator: Code generated in 10.855727 ms
[2023-07-13T22:31:16.997-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-13T22:31:17.005-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.7 MiB)
[2023-07-13T22:31:17.006-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:43419 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:31:17.007-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:17.008-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33608727 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:17.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:17.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:17.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:17.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:17.035-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:43419 in memory (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:31:17.035-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:17.035-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:17.039-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:43419 in memory (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:31:17.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 184.3 KiB, free 434.0 MiB)
[2023-07-13T22:31:17.061-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 434.0 MiB)
[2023-07-13T22:31:17.062-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:43419 (size: 65.7 KiB, free: 434.3 MiB)
[2023-07-13T22:31:17.062-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:17.063-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:17.063-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:31:17.064-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:17.064-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:31:17.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:17.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:17.077-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:17.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO CodeGenerator: Code generated in 7.86605 ms
[2023-07-13T22:31:17.100-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:31:17.113-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO CodeGenerator: Code generated in 10.490067 ms
[2023-07-13T22:31:17.117-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:31:17.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:31:17.123-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9012, partition values: [19549]
[2023-07-13T22:31:17.125-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:31:17.127-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:31:17.129-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4506, partition values: [19550]
[2023-07-13T22:31:17.131-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [19551]
[2023-07-13T22:31:17.135-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_202307132231177602428545608334131_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-10/_temporary/0/task_202307132231177602428545608334131_0002_m_000000
[2023-07-13T22:31:17.135-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO SparkHadoopMapRedUtil: attempt_202307132231177602428545608334131_0002_m_000000_2: Committed
[2023-07-13T22:31:17.137-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2971 bytes result sent to driver
[2023-07-13T22:31:17.138-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 74 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:17.138-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:31:17.139-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,102 s
[2023-07-13T22:31:17.139-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:17.139-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:31:17.139-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,105843 s
[2023-07-13T22:31:17.148-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileFormatWriter: Write Job 7d2409c1-3c96-4eef-b851-e5c64b14e33a committed.
[2023-07-13T22:31:17.149-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO FileFormatWriter: Finished processing stats for write job 7d2409c1-3c96-4eef-b851-e5c64b14e33a.
[2023-07-13T22:31:17.177-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:31:17.183-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:31:17.191-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:31:17.198-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:31:17.198-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO BlockManager: BlockManager stopped
[2023-07-13T22:31:17.200-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:31:17.201-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:31:17.204-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:31:17.205-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:31:17.205-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-406a08aa-baf5-4972-9ee7-69b6d39aa45b
[2023-07-13T22:31:17.207-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-406a08aa-baf5-4972-9ee7-69b6d39aa45b/pyspark-78913b2b-3989-45e9-8228-3f403225ba80
[2023-07-13T22:31:17.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-f36da36f-0411-4337-901e-0469d4d1afb7
[2023-07-13T22:31:17.250-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T013110, end_date=20230714T013117
[2023-07-13T22:31:17.298-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:31:17.305-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:43:21.338-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:43:21.342-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:43:21.342-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:43:21.350-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-13T22:43:21.352-0300] {standard_task_runner.py:57} INFO - Started process 43985 to run task
[2023-07-13T22:43:21.354-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '28', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpglmsiqa_']
[2023-07-13T22:43:21.355-0300] {standard_task_runner.py:85} INFO - Job 28: Subtask transform_twitter_datascience
[2023-07-13T22:43:21.377-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:43:21.438-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-13T22:43:21.443-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:43:21.445-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-10
[2023-07-13T22:43:22.824-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:22 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:43:22.826-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:43:23.486-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:43:23.538-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:43:23.612-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO ResourceUtils: ==============================================================
[2023-07-13T22:43:23.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:43:23.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO ResourceUtils: ==============================================================
[2023-07-13T22:43:23.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:43:23.631-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:43:23.641-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:43:23.642-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:43:23.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:43:23.681-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:43:23.681-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:43:23.681-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:43:23.681-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:43:23.873-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO Utils: Successfully started service 'sparkDriver' on port 39633.
[2023-07-13T22:43:23.895-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:43:23.925-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:43:23.941-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:43:23.942-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:43:23.946-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:43:23.968-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0f084c9a-837a-4d56-aba3-c662884b72bb
[2023-07-13T22:43:23.982-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:43:23.997-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:23 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:43:24.174-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:43:24.260-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:43:24.265-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:43:24.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44325.
[2023-07-13T22:43:24.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO NettyBlockTransferService: Server created on 192.168.0.177:44325
[2023-07-13T22:43:24.281-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:43:24.286-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 44325, None)
[2023-07-13T22:43:24.289-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:44325 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 44325, None)
[2023-07-13T22:43:24.291-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 44325, None)
[2023-07-13T22:43:24.292-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 44325, None)
[2023-07-13T22:43:24.648-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:43:24.654-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:24 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:43:25.437-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:25 INFO InMemoryFileIndex: It took 47 ms to list leaf files for 1 paths.
[2023-07-13T22:43:25.540-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:25 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:43:26.995-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:26 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:43:26.997-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:26 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:43:26.999-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:43:27.215-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:43:27.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:27.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:44325 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:27.263-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:27.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33622074 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:27.404-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:27.416-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:27.417-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:27.417-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:27.418-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:27.422-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:27.524-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:43:27.525-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:27.526-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:44325 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:27.527-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:27.536-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:27.537-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:43:27.573-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:27.584-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:43:27.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [empty row]
[2023-07-13T22:43:27.833-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO CodeGenerator: Code generated in 122.657787 ms
[2023-07-13T22:43:27.867-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [empty row]
[2023-07-13T22:43:27.875-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [empty row]
[2023-07-13T22:43:27.880-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:43:27.884-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:43:27.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [empty row]
[2023-07-13T22:43:27.894-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [empty row]
[2023-07-13T22:43:27.899-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [empty row]
[2023-07-13T22:43:27.921-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:43:27.934-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 368 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:27.936-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:43:27.944-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,508 s
[2023-07-13T22:43:27.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:27.949-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:43:27.956-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:27 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,547258 s
[2023-07-13T22:43:28.459-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:43:28.461-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:43:28.462-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:43:28.463-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:43:28.521-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:28.522-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:28.523-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:28.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:44325 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:28.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:44325 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:28.695-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO CodeGenerator: Code generated in 93.658534 ms
[2023-07-13T22:43:28.701-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:28.716-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:43:28.717-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:44325 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:43:28.718-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:28.722-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33622074 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:28.795-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:28.796-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:28.796-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:28.796-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:28.797-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:28.797-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:28.820-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.5 KiB, free 433.9 MiB)
[2023-07-13T22:43:28.823-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:43:28.824-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:44325 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:43:28.824-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:28.825-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:28.825-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:43:28.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:28.830-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:43:28.870-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:28.871-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:28.871-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:28.895-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:43:28.914-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO CodeGenerator: Code generated in 16.002112 ms
[2023-07-13T22:43:28.931-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO CodeGenerator: Code generated in 3.758041 ms
[2023-07-13T22:43:28.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:43:28.964-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:43:28.970-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:43:28.977-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:43:28.984-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:43:28.988-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:43:28.993-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:43:29.005-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_202307132243285129240176480362654_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-10/_temporary/0/task_202307132243285129240176480362654_0001_m_000000
[2023-07-13T22:43:29.006-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SparkHadoopMapRedUtil: attempt_202307132243285129240176480362654_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:43:29.013-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:43:29.015-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 188 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:29.015-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:43:29.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,217 s
[2023-07-13T22:43:29.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:29.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:43:29.017-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,221620 s
[2023-07-13T22:43:29.019-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileFormatWriter: Start to commit write Job 8da6f203-c034-4bc4-8d46-bcf2befec96c.
[2023-07-13T22:43:29.031-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileFormatWriter: Write Job 8da6f203-c034-4bc4-8d46-bcf2befec96c committed. Elapsed time: 11 ms.
[2023-07-13T22:43:29.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileFormatWriter: Finished processing stats for write job 8da6f203-c034-4bc4-8d46-bcf2befec96c.
[2023-07-13T22:43:29.067-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:43:29.067-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:43:29.067-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:43:29.068-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:43:29.079-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:29.079-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:29.080-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:29.106-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO CodeGenerator: Code generated in 10.789035 ms
[2023-07-13T22:43:29.108-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:43:29.119-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:43:29.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:44325 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:43:29.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:29.121-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33622074 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:29.145-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:29.146-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:29.146-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:29.146-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:29.147-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:29.147-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:29.164-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.5 KiB, free 433.4 MiB)
[2023-07-13T22:43:29.167-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:43:29.167-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:44325 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:43:29.168-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:29.168-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:29.168-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:43:29.170-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:29.170-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:43:29.183-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:29.184-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:29.184-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:29.197-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:43:29.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO CodeGenerator: Code generated in 9.422518 ms
[2023-07-13T22:43:29.214-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:43:29.218-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:43:29.221-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:43:29.224-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:43:29.227-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:43:29.229-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:43:29.232-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:43:29.236-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_202307132243295507627971388209882_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-10/_temporary/0/task_202307132243295507627971388209882_0002_m_000000
[2023-07-13T22:43:29.236-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SparkHadoopMapRedUtil: attempt_202307132243295507627971388209882_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:43:29.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:43:29.240-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 70 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:29.240-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:43:29.240-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,092 s
[2023-07-13T22:43:29.240-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:29.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:43:29.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,095241 s
[2023-07-13T22:43:29.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileFormatWriter: Start to commit write Job d0f1884f-939f-4d22-96e1-3b553a52f40a.
[2023-07-13T22:43:29.250-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileFormatWriter: Write Job d0f1884f-939f-4d22-96e1-3b553a52f40a committed. Elapsed time: 8 ms.
[2023-07-13T22:43:29.250-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO FileFormatWriter: Finished processing stats for write job d0f1884f-939f-4d22-96e1-3b553a52f40a.
[2023-07-13T22:43:29.274-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:43:29.282-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:43:29.289-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:43:29.295-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:43:29.295-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO BlockManager: BlockManager stopped
[2023-07-13T22:43:29.298-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:43:29.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:43:29.302-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:43:29.302-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:43:29.302-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-65b8250b-fe28-41fa-99a5-99b9c4b56c7c
[2023-07-13T22:43:29.304-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-89c8e840-2d5c-497b-82c3-94035697ec4f
[2023-07-13T22:43:29.305-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-89c8e840-2d5c-497b-82c3-94035697ec4f/pyspark-49c4aa36-e150-441d-bfce-cf330728ba5f
[2023-07-13T22:43:29.354-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T014321, end_date=20230714T014329
[2023-07-13T22:43:29.378-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:43:29.383-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:49:42.156-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:49:42.161-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:49:42.161-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:49:42.169-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-13T22:49:42.172-0300] {standard_task_runner.py:57} INFO - Started process 47141 to run task
[2023-07-13T22:49:42.174-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '28', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmppj0g4fql']
[2023-07-13T22:49:42.174-0300] {standard_task_runner.py:85} INFO - Job 28: Subtask transform_twitter_datascience
[2023-07-13T22:49:42.203-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:49:42.265-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-13T22:49:42.268-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:49:42.269-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest FormacaoAirflow/dados_transformation --process-date 2023-07-10
[2023-07-13T22:49:43.910-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:43 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:49:43.914-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:49:44.693-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:49:44.767-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:49:44.870-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO ResourceUtils: ==============================================================
[2023-07-13T22:49:44.871-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:49:44.871-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO ResourceUtils: ==============================================================
[2023-07-13T22:49:44.871-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:49:44.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:49:44.903-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:49:44.903-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:49:44.950-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:49:44.950-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:49:44.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:49:44.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:49:44.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:49:45.156-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO Utils: Successfully started service 'sparkDriver' on port 44241.
[2023-07-13T22:49:45.189-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:49:45.223-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:49:45.243-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:49:45.244-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:49:45.253-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:49:45.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-37e7eb48-bfa5-4ca3-9036-0188a5615a7a
[2023-07-13T22:49:45.295-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:49:45.311-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:49:45.507-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:49:45.602-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:49:45.609-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:49:45.631-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38293.
[2023-07-13T22:49:45.631-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO NettyBlockTransferService: Server created on 192.168.0.177:38293
[2023-07-13T22:49:45.636-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:49:45.643-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 38293, None)
[2023-07-13T22:49:45.646-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:38293 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 38293, None)
[2023-07-13T22:49:45.650-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 38293, None)
[2023-07-13T22:49:45.652-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 38293, None)
[2023-07-13T22:49:46.086-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:49:46.092-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:46 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:49:46.787-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:46 INFO InMemoryFileIndex: It took 50 ms to list leaf files for 1 paths.
[2023-07-13T22:49:46.878-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:46 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 8 paths.
[2023-07-13T22:49:48.401-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:49:48.402-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:49:48.404-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:49:48.605-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:49:48.643-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:49:48.645-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:38293 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:48.648-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:48.655-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33640787 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:49:48.783-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:48.797-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:49:48.798-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:49:48.798-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:49:48.799-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:49:48.801-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:49:48.887-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:49:48.889-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:49:48.889-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:38293 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:48.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:49:48.901-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:49:48.902-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:49:48.943-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:49:48.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:49:49.054-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:49:49.200-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO CodeGenerator: Code generated in 116.954847 ms
[2023-07-13T22:49:49.246-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [empty row]
[2023-07-13T22:49:49.258-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [empty row]
[2023-07-13T22:49:49.262-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:49:49.267-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:49:49.273-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [empty row]
[2023-07-13T22:49:49.276-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [empty row]
[2023-07-13T22:49:49.280-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [empty row]
[2023-07-13T22:49:49.296-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:49:49.306-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 372 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:49:49.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:49:49.317-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,494 s
[2023-07-13T22:49:49.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:49:49.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:49:49.332-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:49 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,544729 s
[2023-07-13T22:49:50.047-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:49:50.051-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:49:50.054-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:49:50.054-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:49:50.246-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:38293 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:50.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:49:50.276-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:49:50.278-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:38293 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:50.278-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:49:50.498-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO CodeGenerator: Code generated in 87.154334 ms
[2023-07-13T22:49:50.503-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:49:50.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:49:50.513-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:38293 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:49:50.514-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:50.517-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33640787 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:49:50.606-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:50.608-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:49:50.609-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:49:50.609-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:49:50.609-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:49:50.610-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:49:50.638-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.4 KiB, free 433.9 MiB)
[2023-07-13T22:49:50.641-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:49:50.642-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:38293 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:49:50.642-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:49:50.643-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:49:50.643-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:49:50.646-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:49:50.647-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:49:50.686-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:49:50.686-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:49:50.686-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:49:50.709-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:49:50.729-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO CodeGenerator: Code generated in 16.993721 ms
[2023-07-13T22:49:50.745-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO CodeGenerator: Code generated in 4.124344 ms
[2023-07-13T22:49:50.769-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [19546]
[2023-07-13T22:49:50.776-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [19548]
[2023-07-13T22:49:50.782-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:49:50.789-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:49:50.795-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [19549]
[2023-07-13T22:49:50.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:49:50.805-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [19547]
[2023-07-13T22:49:50.817-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileOutputCommitter: Saved output of task 'attempt_202307132249508953956253804769965_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-10/_temporary/0/task_202307132249508953956253804769965_0001_m_000000
[2023-07-13T22:49:50.818-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SparkHadoopMapRedUtil: attempt_202307132249508953956253804769965_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:49:50.828-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:49:50.830-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 185 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:49:50.830-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:49:50.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,219 s
[2023-07-13T22:49:50.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:49:50.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:49:50.832-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,225398 s
[2023-07-13T22:49:50.834-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileFormatWriter: Start to commit write Job 4a792e7c-9191-4864-b24a-276cd1c503b1.
[2023-07-13T22:49:50.845-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileFormatWriter: Write Job 4a792e7c-9191-4864-b24a-276cd1c503b1 committed. Elapsed time: 10 ms.
[2023-07-13T22:49:50.848-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileFormatWriter: Finished processing stats for write job 4a792e7c-9191-4864-b24a-276cd1c503b1.
[2023-07-13T22:49:50.879-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:49:50.879-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:49:50.879-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:49:50.879-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:49:50.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:49:50.891-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:49:50.891-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:49:50.933-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO CodeGenerator: Code generated in 18.85735 ms
[2023-07-13T22:49:50.938-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:49:50.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:49:50.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:38293 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:49:50.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:50.955-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33640787 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:49:50.978-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:50.979-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:49:50.979-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:49:50.979-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:49:50.979-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:49:50.980-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:50 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:49:51.003-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:49:51.007-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:49:51.007-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:38293 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:49:51.008-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:49:51.009-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:49:51.009-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:49:51.010-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:49:51.011-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:49:51.024-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:49:51.024-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:49:51.025-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:49:51.038-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:49:51.053-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:38293 in memory (size: 33.9 KiB, free: 434.2 MiB)
[2023-07-13T22:49:51.055-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:38293 in memory (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:49:51.062-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO CodeGenerator: Code generated in 21.501893 ms
[2023-07-13T22:49:51.069-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [19546]
[2023-07-13T22:49:51.073-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [19548]
[2023-07-13T22:49:51.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:49:51.079-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:49:51.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [19549]
[2023-07-13T22:49:51.085-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:49:51.088-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [19547]
[2023-07-13T22:49:51.093-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileOutputCommitter: Saved output of task 'attempt_202307132249506003361484342516952_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/user/process_date=2023-07-10/_temporary/0/task_202307132249506003361484342516952_0002_m_000000
[2023-07-13T22:49:51.093-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO SparkHadoopMapRedUtil: attempt_202307132249506003361484342516952_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-13T22:49:51.095-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2622 bytes result sent to driver
[2023-07-13T22:49:51.096-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 87 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:49:51.097-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:49:51.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,116 s
[2023-07-13T22:49:51.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:49:51.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:49:51.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,120355 s
[2023-07-13T22:49:51.099-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileFormatWriter: Start to commit write Job 2f8bcaf0-4e20-41b1-8967-1a250fbf0550.
[2023-07-13T22:49:51.108-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileFormatWriter: Write Job 2f8bcaf0-4e20-41b1-8967-1a250fbf0550 committed. Elapsed time: 8 ms.
[2023-07-13T22:49:51.108-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO FileFormatWriter: Finished processing stats for write job 2f8bcaf0-4e20-41b1-8967-1a250fbf0550.
[2023-07-13T22:49:51.134-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:49:51.142-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:49:51.155-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:49:51.162-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:49:51.162-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO BlockManager: BlockManager stopped
[2023-07-13T22:49:51.165-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:49:51.167-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:49:51.171-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:49:51.171-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:49:51.172-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6d88fcb-ac97-4cb3-909f-04997e3cd72b
[2023-07-13T22:49:51.174-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-c216b4c5-b9ec-424f-8535-e970dc58cd2d/pyspark-ce5e54c6-2880-483e-bc62-90e1f03aaa8f
[2023-07-13T22:49:51.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-c216b4c5-b9ec-424f-8535-e970dc58cd2d
[2023-07-13T22:49:51.223-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T014942, end_date=20230714T014951
[2023-07-13T22:49:51.268-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:49:51.273-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:51:33.313-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:51:33.317-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:51:33.318-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:51:33.325-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-13T22:51:33.327-0300] {standard_task_runner.py:57} INFO - Started process 48841 to run task
[2023-07-13T22:51:33.329-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp7q4up11w']
[2023-07-13T22:51:33.330-0300] {standard_task_runner.py:85} INFO - Job 27: Subtask transform_twitter_datascience
[2023-07-13T22:51:33.360-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:51:33.414-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-13T22:51:33.419-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:51:33.420-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest FormacaoAirflow/dados_transformation --process-date 2023-07-10
[2023-07-13T22:51:35.024-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:51:35.026-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:51:35.769-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:51:35.822-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:51:35.918-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 INFO ResourceUtils: ==============================================================
[2023-07-13T22:51:35.918-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:51:35.919-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 INFO ResourceUtils: ==============================================================
[2023-07-13T22:51:35.919-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:51:35.955-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:51:35.973-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:51:35.974-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:51:36.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:51:36.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:51:36.050-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:51:36.050-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:51:36.050-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:51:36.293-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO Utils: Successfully started service 'sparkDriver' on port 41533.
[2023-07-13T22:51:36.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:51:36.365-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:51:36.381-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:51:36.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:51:36.385-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:51:36.405-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54905c9e-86eb-490f-a641-1bef5e174f4e
[2023-07-13T22:51:36.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:51:36.438-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:51:36.652-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:51:36.750-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:51:36.756-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:51:36.775-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34529.
[2023-07-13T22:51:36.776-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO NettyBlockTransferService: Server created on 192.168.0.177:34529
[2023-07-13T22:51:36.779-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:51:36.785-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 34529, None)
[2023-07-13T22:51:36.788-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:34529 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 34529, None)
[2023-07-13T22:51:36.791-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 34529, None)
[2023-07-13T22:51:36.792-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 34529, None)
[2023-07-13T22:51:37.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:51:37.187-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:37 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:51:38.136-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:38 INFO InMemoryFileIndex: It took 57 ms to list leaf files for 1 paths.
[2023-07-13T22:51:38.262-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:38 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 8 paths.
[2023-07-13T22:51:40.229-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:51:40.230-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:51:40.232-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:51:40.475-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:51:40.520-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:51:40.522-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:34529 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:40.530-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:40.538-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649517 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:40.682-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:40.696-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:40.696-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:40.696-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:40.698-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:40.703-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:40.809-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:51:40.812-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:51:40.813-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:34529 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:40.813-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:40.826-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:40.827-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:51:40.870-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:40.883-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:51:40.988-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:40 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:51:41.123-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO CodeGenerator: Code generated in 106.607876 ms
[2023-07-13T22:51:41.160-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [empty row]
[2023-07-13T22:51:41.166-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [empty row]
[2023-07-13T22:51:41.170-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:51:41.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:51:41.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [empty row]
[2023-07-13T22:51:41.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [empty row]
[2023-07-13T22:51:41.180-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [empty row]
[2023-07-13T22:51:41.201-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:51:41.219-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 356 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:41.250-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:51:41.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,539 s
[2023-07-13T22:51:41.261-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:41.262-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:51:41.264-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,582221 s
[2023-07-13T22:51:41.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:51:41.600-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:51:41.601-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:51:41.602-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:51:41.686-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:41.686-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:41.687-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:41.878-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO CodeGenerator: Code generated in 105.612829 ms
[2023-07-13T22:51:41.884-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-13T22:51:41.893-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-13T22:51:41.894-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:34529 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:51:41.896-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:41.901-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649517 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:41.958-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:41.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:41.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:41.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:41.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:41.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:41.993-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.4 KiB, free 433.7 MiB)
[2023-07-13T22:51:41.999-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.6 MiB)
[2023-07-13T22:51:42.000-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:34529 (size: 83.4 KiB, free: 434.2 MiB)
[2023-07-13T22:51:42.001-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:42.002-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:42.002-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:51:42.006-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:42.007-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:51:42.080-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:34529 in memory (size: 34.0 KiB, free: 434.3 MiB)
[2023-07-13T22:51:42.118-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:34529 in memory (size: 7.0 KiB, free: 434.3 MiB)
[2023-07-13T22:51:42.119-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:42.119-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:42.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:42.161-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:51:42.187-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO CodeGenerator: Code generated in 20.265917 ms
[2023-07-13T22:51:42.211-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO CodeGenerator: Code generated in 5.761706 ms
[2023-07-13T22:51:42.242-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [19546]
[2023-07-13T22:51:42.254-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [19548]
[2023-07-13T22:51:42.263-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:51:42.269-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:51:42.275-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [19550]
[2023-07-13T22:51:42.283-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [19547]
[2023-07-13T22:51:42.287-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [19549]
[2023-07-13T22:51:42.301-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileOutputCommitter: Saved output of task 'attempt_202307132251415114394622103113454_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-10/_temporary/0/task_202307132251415114394622103113454_0001_m_000000
[2023-07-13T22:51:42.302-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkHadoopMapRedUtil: attempt_202307132251415114394622103113454_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:51:42.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:51:42.311-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 307 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:42.311-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:51:42.312-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,347 s
[2023-07-13T22:51:42.313-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:42.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:51:42.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,356741 s
[2023-07-13T22:51:42.317-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileFormatWriter: Start to commit write Job b42d1c77-640e-4f77-a6d3-9a81c9d5eb59.
[2023-07-13T22:51:42.334-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileFormatWriter: Write Job b42d1c77-640e-4f77-a6d3-9a81c9d5eb59 committed. Elapsed time: 15 ms.
[2023-07-13T22:51:42.337-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileFormatWriter: Finished processing stats for write job b42d1c77-640e-4f77-a6d3-9a81c9d5eb59.
[2023-07-13T22:51:42.374-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:51:42.374-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:51:42.374-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:51:42.374-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:51:42.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:42.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:42.387-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:42.425-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO CodeGenerator: Code generated in 17.801266 ms
[2023-07-13T22:51:42.431-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:51:42.440-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:51:42.440-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:34529 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:51:42.441-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:42.442-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649517 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:42.476-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:42.476-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:42.477-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:42.477-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:42.477-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:42.481-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:42.497-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:51:42.500-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:51:42.501-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:34529 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:51:42.502-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:42.502-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:42.502-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:51:42.503-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:42.504-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:51:42.518-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:42.518-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:42.519-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:42.532-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:51:42.545-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO CodeGenerator: Code generated in 9.703747 ms
[2023-07-13T22:51:42.557-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [19546]
[2023-07-13T22:51:42.563-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [19548]
[2023-07-13T22:51:42.568-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:51:42.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:51:42.573-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [19550]
[2023-07-13T22:51:42.576-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [19547]
[2023-07-13T22:51:42.579-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [19549]
[2023-07-13T22:51:42.585-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileOutputCommitter: Saved output of task 'attempt_202307132251424720568078597088308_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/user/process_date=2023-07-10/_temporary/0/task_202307132251424720568078597088308_0002_m_000000
[2023-07-13T22:51:42.585-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkHadoopMapRedUtil: attempt_202307132251424720568078597088308_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:51:42.587-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:51:42.588-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 85 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:42.589-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:51:42.589-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,107 s
[2023-07-13T22:51:42.590-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:42.590-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:51:42.590-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,114198 s
[2023-07-13T22:51:42.590-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileFormatWriter: Start to commit write Job 13ac3956-e94f-4ee6-be9c-711e668f73c1.
[2023-07-13T22:51:42.602-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileFormatWriter: Write Job 13ac3956-e94f-4ee6-be9c-711e668f73c1 committed. Elapsed time: 11 ms.
[2023-07-13T22:51:42.602-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO FileFormatWriter: Finished processing stats for write job 13ac3956-e94f-4ee6-be9c-711e668f73c1.
[2023-07-13T22:51:42.653-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:51:42.669-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:51:42.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:51:42.689-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:51:42.689-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO BlockManager: BlockManager stopped
[2023-07-13T22:51:42.692-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:51:42.695-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:51:42.700-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:51:42.701-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:51:42.701-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-bfb9fe7e-4334-4d32-8388-6e15e566dac2
[2023-07-13T22:51:42.703-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-bfb9fe7e-4334-4d32-8388-6e15e566dac2/pyspark-dbc0012f-0be7-4ea1-925f-d6e9642952c5
[2023-07-13T22:51:42.705-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-3fd0f180-649f-406f-aba4-325d632ea7c7
[2023-07-13T22:51:42.755-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T015133, end_date=20230714T015142
[2023-07-13T22:51:42.805-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:51:42.810-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:53:08.263-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:53:08.267-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-13T22:53:08.267-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:53:08.275-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-13T22:53:08.278-0300] {standard_task_runner.py:57} INFO - Started process 50772 to run task
[2023-07-13T22:53:08.279-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '28', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1v0er3zn']
[2023-07-13T22:53:08.280-0300] {standard_task_runner.py:85} INFO - Job 28: Subtask transform_twitter_datascience
[2023-07-13T22:53:08.300-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:53:08.353-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-13T22:53:08.358-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:53:08.360-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest dados_transformation --process-date 2023-07-10
[2023-07-13T22:53:09.517-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:09 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:53:09.519-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:53:10.195-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:53:10.246-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:53:10.343-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO ResourceUtils: ==============================================================
[2023-07-13T22:53:10.343-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:53:10.343-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO ResourceUtils: ==============================================================
[2023-07-13T22:53:10.344-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:53:10.361-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:53:10.371-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:53:10.372-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:53:10.410-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:53:10.411-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:53:10.411-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:53:10.411-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:53:10.412-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:53:10.610-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO Utils: Successfully started service 'sparkDriver' on port 45749.
[2023-07-13T22:53:10.640-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:53:10.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:53:10.697-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:53:10.698-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:53:10.701-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:53:10.727-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e47a2005-7c6b-45d1-917a-8ad0e027d808
[2023-07-13T22:53:10.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:53:10.768-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:53:10.938-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:53:11.023-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:53:11.027-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:53:11.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41423.
[2023-07-13T22:53:11.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO NettyBlockTransferService: Server created on 192.168.0.177:41423
[2023-07-13T22:53:11.041-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:53:11.046-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 41423, None)
[2023-07-13T22:53:11.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:41423 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 41423, None)
[2023-07-13T22:53:11.051-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 41423, None)
[2023-07-13T22:53:11.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 41423, None)
[2023-07-13T22:53:11.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:53:11.387-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:11 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:53:12.054-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:12 INFO InMemoryFileIndex: It took 45 ms to list leaf files for 1 paths.
[2023-07-13T22:53:12.168-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:12 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:53:13.544-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:13.544-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:13.547-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:53:13.750-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:53:13.788-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:13.790-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:41423 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:13.793-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:13.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33686118 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:13.921-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:13.933-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:13.933-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:13.933-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:13.934-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:13.937-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:14.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:53:14.035-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:14.036-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:41423 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:14.036-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:14.046-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:14.047-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:53:14.086-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:14.095-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:53:14.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [empty row]
[2023-07-13T22:53:14.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO CodeGenerator: Code generated in 111.720379 ms
[2023-07-13T22:53:14.358-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:53:14.370-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [empty row]
[2023-07-13T22:53:14.377-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [empty row]
[2023-07-13T22:53:14.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [empty row]
[2023-07-13T22:53:14.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:53:14.389-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:53:14.393-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [empty row]
[2023-07-13T22:53:14.409-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:53:14.414-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 336 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:14.416-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:53:14.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,474 s
[2023-07-13T22:53:14.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:14.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:53:14.426-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,504779 s
[2023-07-13T22:53:14.617-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:41423 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:14.623-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:41423 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:14.779-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:14.781-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:53:14.782-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:53:14.783-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:53:14.837-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:14.838-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:14.838-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:14.972-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO CodeGenerator: Code generated in 76.776151 ms
[2023-07-13T22:53:14.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:14.984-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:53:14.985-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:41423 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:53:14.986-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:14.989-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33686118 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:15.038-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:15.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:15.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:15.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:15.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:15.041-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:15.061-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.3 KiB, free 433.9 MiB)
[2023-07-13T22:53:15.064-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:53:15.065-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:41423 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:53:15.065-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:15.066-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:15.066-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:53:15.070-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:15.071-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:53:15.109-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:15.110-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:15.110-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:15.133-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:15.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO CodeGenerator: Code generated in 15.305888 ms
[2023-07-13T22:53:15.169-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO CodeGenerator: Code generated in 3.866985 ms
[2023-07-13T22:53:15.193-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:53:15.202-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [19549]
[2023-07-13T22:53:15.217-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:15.222-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:15.228-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:15.233-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:15.237-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [19550]
[2023-07-13T22:53:15.248-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253153414826431075855688_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-10/_temporary/0/task_202307132253153414826431075855688_0001_m_000000
[2023-07-13T22:53:15.249-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkHadoopMapRedUtil: attempt_202307132253153414826431075855688_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:53:15.254-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:53:15.256-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 189 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:15.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:53:15.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,216 s
[2023-07-13T22:53:15.258-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:15.258-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:53:15.258-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,219727 s
[2023-07-13T22:53:15.261-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileFormatWriter: Start to commit write Job 83b9ef4c-14a6-44ce-a572-340ff682a2e5.
[2023-07-13T22:53:15.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileFormatWriter: Write Job 83b9ef4c-14a6-44ce-a572-340ff682a2e5 committed. Elapsed time: 9 ms.
[2023-07-13T22:53:15.274-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileFormatWriter: Finished processing stats for write job 83b9ef4c-14a6-44ce-a572-340ff682a2e5.
[2023-07-13T22:53:15.298-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:15.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:15.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:15.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:53:15.306-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:15.307-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:15.307-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:15.331-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO CodeGenerator: Code generated in 10.395567 ms
[2023-07-13T22:53:15.334-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:53:15.343-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:53:15.343-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:41423 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:53:15.344-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:15.345-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33686118 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:15.376-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:15.377-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:15.377-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:15.377-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:15.377-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:15.378-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:15.392-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:53:15.394-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:53:15.395-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:41423 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:53:15.395-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:15.395-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:15.395-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:53:15.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:15.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:53:15.410-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:15.410-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:15.410-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:15.420-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:15.436-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO CodeGenerator: Code generated in 12.225032 ms
[2023-07-13T22:53:15.444-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:53:15.450-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [19549]
[2023-07-13T22:53:15.455-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:15.459-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:15.462-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:15.465-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:15.468-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [19550]
[2023-07-13T22:53:15.474-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253151825032779441890268_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-10/_temporary/0/task_202307132253151825032779441890268_0002_m_000000
[2023-07-13T22:53:15.475-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkHadoopMapRedUtil: attempt_202307132253151825032779441890268_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-13T22:53:15.476-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:53:15.478-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 81 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:15.478-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:53:15.478-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,099 s
[2023-07-13T22:53:15.479-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:15.479-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:53:15.479-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,103139 s
[2023-07-13T22:53:15.480-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileFormatWriter: Start to commit write Job cb0e9d31-b5d2-4be1-84a2-f006d61d4193.
[2023-07-13T22:53:15.488-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileFormatWriter: Write Job cb0e9d31-b5d2-4be1-84a2-f006d61d4193 committed. Elapsed time: 8 ms.
[2023-07-13T22:53:15.488-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO FileFormatWriter: Finished processing stats for write job cb0e9d31-b5d2-4be1-84a2-f006d61d4193.
[2023-07-13T22:53:15.511-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:53:15.518-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:53:15.525-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:53:15.531-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:53:15.531-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO BlockManager: BlockManager stopped
[2023-07-13T22:53:15.533-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:53:15.534-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:53:15.537-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:53:15.537-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:53:15.538-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-7bffb866-ffeb-4b37-9a1e-2b589279b48f
[2023-07-13T22:53:15.539-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-7bffb866-ffeb-4b37-9a1e-2b589279b48f/pyspark-ba7954db-b8ec-4260-8bee-553d6546adf5
[2023-07-13T22:53:15.541-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7f36bf3-db2f-44e4-83aa-8be58b3e80e6
[2023-07-13T22:53:15.572-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T015308, end_date=20230714T015315
[2023-07-13T22:53:15.617-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:53:15.623-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T15:26:32.521-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T15:26:32.539-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T15:26:32.540-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T15:26:32.567-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-14T15:26:32.576-0300] {standard_task_runner.py:57} INFO - Started process 14757 to run task
[2023-07-14T15:26:32.581-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpnr2bf1aw']
[2023-07-14T15:26:32.583-0300] {standard_task_runner.py:85} INFO - Job 46: Subtask transform_twitter_datascience
[2023-07-14T15:26:32.662-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T15:26:32.809-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-14T15:26:32.819-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T15:26:32.821-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10 --dest /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10 --process-date 2023-07-10
[2023-07-14T15:26:36.811-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:36 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T15:26:36.817-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T15:26:38.657-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:38 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T15:26:38.883-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T15:26:38.990-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:38 INFO ResourceUtils: ==============================================================
[2023-07-14T15:26:38.991-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T15:26:38.991-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:38 INFO ResourceUtils: ==============================================================
[2023-07-14T15:26:38.992-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:38 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T15:26:39.057-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T15:26:39.092-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T15:26:39.093-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T15:26:39.225-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T15:26:39.225-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T15:26:39.226-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T15:26:39.227-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T15:26:39.227-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T15:26:39.761-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO Utils: Successfully started service 'sparkDriver' on port 42319.
[2023-07-14T15:26:39.831-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T15:26:39.916-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T15:26:39.959-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T15:26:39.960-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T15:26:39.972-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T15:26:40.018-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8780c134-ffa4-44a7-a3c8-1d532e140a5a
[2023-07-14T15:26:40.059-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:40 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T15:26:40.106-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:40 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T15:26:40.783-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T15:26:40.959-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:40 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T15:26:40.972-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T15:26:41.006-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44083.
[2023-07-14T15:26:41.006-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:41 INFO NettyBlockTransferService: Server created on 192.168.0.102:44083
[2023-07-14T15:26:41.008-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T15:26:41.017-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 44083, None)
[2023-07-14T15:26:41.021-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:44083 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 44083, None)
[2023-07-14T15:26:41.026-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 44083, None)
[2023-07-14T15:26:41.031-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 44083, None)
[2023-07-14T15:26:42.582-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T15:26:42.607-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:42 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T15:26:44.885-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:44 INFO InMemoryFileIndex: It took 97 ms to list leaf files for 1 paths.
[2023-07-14T15:26:44.983-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:44 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2023-07-14T15:26:48.759-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:48 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:26:48.762-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:48 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:26:48.771-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T15:26:49.242-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T15:26:49.363-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T15:26:49.373-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:44083 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:26:49.380-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:49.396-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207873 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:26:49.798-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:49.826-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:26:49.826-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:26:49.828-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:26:49.831-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:26:49.843-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:26:49.996-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T15:26:50.005-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T15:26:50.006-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:44083 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:26:50.007-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:26:50.026-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:26:50.027-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T15:26:50.111-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5007 bytes) taskResourceAssignments Map()
[2023-07-14T15:26:50.140-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T15:26:50.358-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13569, partition values: [empty row]
[2023-07-14T15:26:50.643-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO CodeGenerator: Code generated in 221.440492 ms
[2023-07-14T15:26:50.739-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T15:26:50.750-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 657 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:26:50.755-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T15:26:50.766-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,899 s
[2023-07-14T15:26:50.774-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:26:50.775-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T15:26:50.781-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:50 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,981470 s
[2023-07-14T15:26:51.557-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T15:26:51.559-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:51 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T15:26:51.560-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:51 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T15:26:51.739-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:26:51.740-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:26:51.742-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:26:52.091-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:44083 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:26:52.100-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:44083 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:26:52.116-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO CodeGenerator: Code generated in 189.132282 ms
[2023-07-14T15:26:52.130-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T15:26:52.148-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T15:26:52.151-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:44083 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T15:26:52.154-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:52.158-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207873 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:26:52.259-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:52.260-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:26:52.261-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:26:52.261-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:26:52.261-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:26:52.263-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:26:52.307-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T15:26:52.313-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T15:26:52.314-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:44083 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T15:26:52.315-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:26:52.318-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:26:52.319-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T15:26:52.327-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:26:52.329-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T15:26:52.456-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:26:52.456-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:26:52.458-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:26:52.532-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13569, partition values: [empty row]
[2023-07-14T15:26:52.575-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO CodeGenerator: Code generated in 33.693409 ms
[2023-07-14T15:26:52.620-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO CodeGenerator: Code generated in 12.941636 ms
[2023-07-14T15:26:52.697-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileOutputCommitter: Saved output of task 'attempt_202307141526528270219579668196226_0001_m_000000_1' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10/tweet/process_date=2023-07-10/_temporary/0/task_202307141526528270219579668196226_0001_m_000000
[2023-07-14T15:26:52.698-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO SparkHadoopMapRedUtil: attempt_202307141526528270219579668196226_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T15:26:52.720-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T15:26:52.730-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 407 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:26:52.735-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,468 s
[2023-07-14T15:26:52.736-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:26:52.737-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T15:26:52.741-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T15:26:52.746-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,483908 s
[2023-07-14T15:26:52.752-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileFormatWriter: Start to commit write Job 4d87371c-93ae-4639-9236-1e9297c65e22.
[2023-07-14T15:26:52.795-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileFormatWriter: Write Job 4d87371c-93ae-4639-9236-1e9297c65e22 committed. Elapsed time: 43 ms.
[2023-07-14T15:26:52.808-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileFormatWriter: Finished processing stats for write job 4d87371c-93ae-4639-9236-1e9297c65e22.
[2023-07-14T15:26:52.875-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:26:52.875-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:26:52.877-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T15:26:52.889-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:26:52.890-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:26:52.891-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:26:52.944-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO CodeGenerator: Code generated in 18.14471 ms
[2023-07-14T15:26:52.951-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T15:26:52.969-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T15:26:52.971-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:44083 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T15:26:52.972-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:52.974-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207873 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:26:53.010-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:53.011-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:26:53.012-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:26:53.012-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:26:53.013-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:26:53.015-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:26:53.051-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T15:26:53.057-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T15:26:53.059-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:44083 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T15:26:53.060-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:26:53.062-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:26:53.062-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T15:26:53.064-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:26:53.066-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T15:26:53.106-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:26:53.106-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:26:53.107-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:26:53.132-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13569, partition values: [empty row]
[2023-07-14T15:26:53.164-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO CodeGenerator: Code generated in 24.023747 ms
[2023-07-14T15:26:53.191-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO FileOutputCommitter: Saved output of task 'attempt_202307141526526350506445530417083_0002_m_000000_2' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10/user/process_date=2023-07-10/_temporary/0/task_202307141526526350506445530417083_0002_m_000000
[2023-07-14T15:26:53.191-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO SparkHadoopMapRedUtil: attempt_202307141526526350506445530417083_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-07-14T15:26:53.193-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T15:26:53.197-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 133 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:26:53.197-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T15:26:53.198-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,182 s
[2023-07-14T15:26:53.199-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:26:53.200-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T15:26:53.203-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,190394 s
[2023-07-14T15:26:53.205-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO FileFormatWriter: Start to commit write Job 323884f0-0b48-40c6-af79-1ee73798b8fe.
[2023-07-14T15:26:53.220-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO FileFormatWriter: Write Job 323884f0-0b48-40c6-af79-1ee73798b8fe committed. Elapsed time: 14 ms.
[2023-07-14T15:26:53.221-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO FileFormatWriter: Finished processing stats for write job 323884f0-0b48-40c6-af79-1ee73798b8fe.
[2023-07-14T15:26:53.282-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T15:26:53.299-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T15:26:53.321-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T15:26:53.332-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO MemoryStore: MemoryStore cleared
[2023-07-14T15:26:53.332-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO BlockManager: BlockManager stopped
[2023-07-14T15:26:53.341-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T15:26:53.343-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T15:26:53.353-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T15:26:53.354-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T15:26:53.355-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-96765ec9-7f0f-4fee-83fd-61e57380c9af
[2023-07-14T15:26:53.357-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-29177c1d-8849-4679-a6ee-611a29647322
[2023-07-14T15:26:53.360-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-96765ec9-7f0f-4fee-83fd-61e57380c9af/pyspark-be3f79c6-b310-4f3f-93b7-efccbb763ed2
[2023-07-14T15:26:53.453-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T182632, end_date=20230714T182653
[2023-07-14T15:26:53.509-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T15:26:53.538-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:11:29.610-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T16:11:29.619-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T16:11:29.619-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:11:29.636-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-14T16:11:29.640-0300] {standard_task_runner.py:57} INFO - Started process 21926 to run task
[2023-07-14T16:11:29.643-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '47', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpxaxvs09f']
[2023-07-14T16:11:29.644-0300] {standard_task_runner.py:85} INFO - Job 47: Subtask transform_twitter_datascience
[2023-07-14T16:11:29.686-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:11:29.754-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-14T16:11:29.758-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T16:11:29.759-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10 --process-date 2023-07-10
[2023-07-14T16:11:31.909-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:31 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T16:11:31.912-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T16:11:32.882-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:32 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T16:11:32.943-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T16:11:33.044-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO ResourceUtils: ==============================================================
[2023-07-14T16:11:33.044-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T16:11:33.045-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO ResourceUtils: ==============================================================
[2023-07-14T16:11:33.045-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T16:11:33.077-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T16:11:33.093-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T16:11:33.094-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T16:11:33.153-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T16:11:33.153-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T16:11:33.154-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T16:11:33.154-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T16:11:33.155-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T16:11:33.494-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO Utils: Successfully started service 'sparkDriver' on port 46383.
[2023-07-14T16:11:33.539-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T16:11:33.583-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T16:11:33.603-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T16:11:33.603-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T16:11:33.607-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T16:11:33.633-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f73332f9-3fed-4ed1-a094-5a3911a907ec
[2023-07-14T16:11:33.658-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T16:11:33.678-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T16:11:33.894-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T16:11:34.014-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T16:11:34.021-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T16:11:34.038-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41791.
[2023-07-14T16:11:34.038-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO NettyBlockTransferService: Server created on 192.168.0.102:41791
[2023-07-14T16:11:34.040-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T16:11:34.045-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 41791, None)
[2023-07-14T16:11:34.049-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:41791 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 41791, None)
[2023-07-14T16:11:34.052-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 41791, None)
[2023-07-14T16:11:34.053-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 41791, None)
[2023-07-14T16:11:34.408-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T16:11:34.413-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:34 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T16:11:35.400-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:35 INFO InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.
[2023-07-14T16:11:35.458-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:35 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2023-07-14T16:11:37.207-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:11:37.208-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:11:37.210-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T16:11:37.495-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T16:11:37.556-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T16:11:37.559-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:41791 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:37.564-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:37.575-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208109 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:11:37.758-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:37.781-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:11:37.782-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:11:37.783-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:11:37.785-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:11:37.792-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:11:37.958-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T16:11:37.960-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T16:11:37.961-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:41791 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:37.961-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:11:37.972-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:11:37.973-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T16:11:38.025-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T16:11:38.056-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T16:11:38.238-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13805, partition values: [empty row]
[2023-07-14T16:11:38.400-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO CodeGenerator: Code generated in 125.642727 ms
[2023-07-14T16:11:38.447-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T16:11:38.456-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 443 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:11:38.457-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T16:11:38.461-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,653 s
[2023-07-14T16:11:38.463-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:11:38.463-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T16:11:38.465-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,706589 s
[2023-07-14T16:11:38.802-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:41791 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:38.810-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:41791 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:38.906-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T16:11:38.908-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T16:11:38.908-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T16:11:38.995-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:11:38.996-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:11:38.998-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:11:39.290-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO CodeGenerator: Code generated in 170.829673 ms
[2023-07-14T16:11:39.297-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T16:11:39.312-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T16:11:39.314-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:41791 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:11:39.316-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:39.321-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208109 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:11:39.385-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:39.387-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:11:39.387-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:11:39.387-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:11:39.387-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:11:39.388-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:11:39.409-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T16:11:39.411-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T16:11:39.412-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:41791 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T16:11:39.412-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:11:39.413-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:11:39.413-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T16:11:39.420-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:11:39.421-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T16:11:39.474-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:11:39.475-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:11:39.475-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:11:39.506-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13805, partition values: [empty row]
[2023-07-14T16:11:39.540-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO CodeGenerator: Code generated in 30.452617 ms
[2023-07-14T16:11:39.563-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO CodeGenerator: Code generated in 4.015014 ms
[2023-07-14T16:11:39.596-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileOutputCommitter: Saved output of task 'attempt_202307141611395268920126948283094_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10/tweet/process_date=2023-07-10/_temporary/0/task_202307141611395268920126948283094_0001_m_000000
[2023-07-14T16:11:39.597-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkHadoopMapRedUtil: attempt_202307141611395268920126948283094_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T16:11:39.605-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T16:11:39.607-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 192 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:11:39.607-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T16:11:39.608-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,219 s
[2023-07-14T16:11:39.609-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:11:39.609-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T16:11:39.609-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,224233 s
[2023-07-14T16:11:39.611-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileFormatWriter: Start to commit write Job 6c1257a6-2357-429b-8178-2d59c277ae1d.
[2023-07-14T16:11:39.628-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileFormatWriter: Write Job 6c1257a6-2357-429b-8178-2d59c277ae1d committed. Elapsed time: 15 ms.
[2023-07-14T16:11:39.630-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileFormatWriter: Finished processing stats for write job 6c1257a6-2357-429b-8178-2d59c277ae1d.
[2023-07-14T16:11:39.672-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:11:39.672-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:11:39.673-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T16:11:39.681-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:11:39.682-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:11:39.683-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:11:39.711-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO CodeGenerator: Code generated in 11.903183 ms
[2023-07-14T16:11:39.714-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T16:11:39.738-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:41791 in memory (size: 82.1 KiB, free: 434.4 MiB)
[2023-07-14T16:11:39.741-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.0 MiB)
[2023-07-14T16:11:39.741-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:41791 in memory (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:11:39.742-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:41791 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:11:39.743-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:39.744-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208109 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:11:39.772-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:39.773-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:11:39.773-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:11:39.773-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:11:39.773-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:11:39.774-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:11:39.789-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 434.0 MiB)
[2023-07-14T16:11:39.791-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.9 MiB)
[2023-07-14T16:11:39.791-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:41791 (size: 76.9 KiB, free: 434.3 MiB)
[2023-07-14T16:11:39.792-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:11:39.792-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:11:39.792-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T16:11:39.794-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:11:39.794-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T16:11:39.811-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:11:39.811-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:11:39.812-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:11:39.829-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13805, partition values: [empty row]
[2023-07-14T16:11:39.848-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO CodeGenerator: Code generated in 16.379828 ms
[2023-07-14T16:11:39.864-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileOutputCommitter: Saved output of task 'attempt_202307141611393129871020000297714_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10/user/process_date=2023-07-10/_temporary/0/task_202307141611393129871020000297714_0002_m_000000
[2023-07-14T16:11:39.864-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkHadoopMapRedUtil: attempt_202307141611393129871020000297714_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-14T16:11:39.866-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T16:11:39.868-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 74 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:11:39.869-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T16:11:39.872-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,096 s
[2023-07-14T16:11:39.873-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:11:39.873-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T16:11:39.874-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,101789 s
[2023-07-14T16:11:39.875-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileFormatWriter: Start to commit write Job a82d6b38-21e9-43bb-8d21-475e25bff8e7.
[2023-07-14T16:11:39.883-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileFormatWriter: Write Job a82d6b38-21e9-43bb-8d21-475e25bff8e7 committed. Elapsed time: 8 ms.
[2023-07-14T16:11:39.884-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO FileFormatWriter: Finished processing stats for write job a82d6b38-21e9-43bb-8d21-475e25bff8e7.
[2023-07-14T16:11:39.919-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T16:11:39.929-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T16:11:39.940-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T16:11:39.945-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO MemoryStore: MemoryStore cleared
[2023-07-14T16:11:39.946-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO BlockManager: BlockManager stopped
[2023-07-14T16:11:39.948-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T16:11:39.950-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T16:11:39.955-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T16:11:39.955-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T16:11:39.955-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-fafb8bcd-5345-4332-8fe5-d46feed89ee3
[2023-07-14T16:11:39.957-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-226295f4-e25e-4247-aab7-e828931786dc
[2023-07-14T16:11:39.958-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-fafb8bcd-5345-4332-8fe5-d46feed89ee3/pyspark-36bf73da-6e8f-41ff-89a7-0bb070d44cd2
[2023-07-14T16:11:39.994-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T191129, end_date=20230714T191139
[2023-07-14T16:11:40.024-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:11:40.033-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:29:10.256-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T16:29:10.282-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T16:29:10.283-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:29:10.321-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-14T16:29:10.334-0300] {standard_task_runner.py:57} INFO - Started process 28681 to run task
[2023-07-14T16:29:10.339-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '49', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpkadr91a2']
[2023-07-14T16:29:10.341-0300] {standard_task_runner.py:85} INFO - Job 49: Subtask transform_twitter_datascience
[2023-07-14T16:29:10.422-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:29:10.575-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-14T16:29:10.586-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T16:29:10.588-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10 --process-date 2023-07-10
[2023-07-14T16:29:14.429-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:14 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T16:29:14.433-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T16:29:16.515-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:16 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T16:29:16.789-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T16:29:17.015-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO ResourceUtils: ==============================================================
[2023-07-14T16:29:17.015-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T16:29:17.015-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO ResourceUtils: ==============================================================
[2023-07-14T16:29:17.016-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T16:29:17.049-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T16:29:17.068-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T16:29:17.069-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T16:29:17.164-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T16:29:17.164-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T16:29:17.165-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T16:29:17.167-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T16:29:17.169-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T16:29:17.631-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO Utils: Successfully started service 'sparkDriver' on port 41205.
[2023-07-14T16:29:17.681-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T16:29:17.746-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T16:29:17.777-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T16:29:17.779-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T16:29:17.785-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T16:29:17.843-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0f45b242-c762-4ee8-be90-37d3c18e52ed
[2023-07-14T16:29:17.907-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T16:29:17.935-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T16:29:18.486-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T16:29:18.499-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T16:29:18.644-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T16:29:18.651-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T16:29:18.678-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45113.
[2023-07-14T16:29:18.678-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO NettyBlockTransferService: Server created on 192.168.0.102:45113
[2023-07-14T16:29:18.680-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T16:29:18.693-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 45113, None)
[2023-07-14T16:29:18.697-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:45113 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 45113, None)
[2023-07-14T16:29:18.699-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 45113, None)
[2023-07-14T16:29:18.700-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 45113, None)
[2023-07-14T16:29:19.294-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T16:29:19.300-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:19 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T16:29:20.393-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:20 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
[2023-07-14T16:29:20.480-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-07-14T16:29:23.143-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:29:23.144-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:29:23.147-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T16:29:23.538-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T16:29:23.600-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T16:29:23.606-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:45113 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:29:23.613-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:23.627-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203307 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:29:23.829-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:23.856-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:29:23.857-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:29:23.860-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:29:23.863-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:29:23.866-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:29:24.164-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T16:29:24.167-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T16:29:24.170-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:45113 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:29:24.173-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:29:24.197-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:29:24.199-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T16:29:24.324-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T16:29:24.367-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T16:29:24.593-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9003, partition values: [empty row]
[2023-07-14T16:29:24.847-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO CodeGenerator: Code generated in 200.338191 ms
[2023-07-14T16:29:24.929-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T16:29:24.938-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 640 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:29:24.945-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T16:29:24.953-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,064 s
[2023-07-14T16:29:24.961-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:29:24.963-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T16:29:24.965-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:24 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,135620 s
[2023-07-14T16:29:25.342-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:45113 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:29:25.354-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:45113 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:29:25.671-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T16:29:25.676-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:25 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T16:29:25.676-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:25 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T16:29:25.777-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:29:25.778-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:29:25.779-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:29:26.035-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO CodeGenerator: Code generated in 112.087533 ms
[2023-07-14T16:29:26.043-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T16:29:26.055-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T16:29:26.056-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:45113 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:29:26.057-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:26.062-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203307 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:29:26.139-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:26.141-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:29:26.141-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:29:26.141-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:29:26.141-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:29:26.142-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:29:26.182-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T16:29:26.188-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T16:29:26.190-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:45113 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T16:29:26.193-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:29:26.199-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:29:26.200-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T16:29:26.215-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:29:26.217-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T16:29:26.281-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:29:26.282-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:29:26.283-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:29:26.326-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9003, partition values: [empty row]
[2023-07-14T16:29:26.362-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO CodeGenerator: Code generated in 31.682861 ms
[2023-07-14T16:29:26.418-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO CodeGenerator: Code generated in 5.634487 ms
[2023-07-14T16:29:26.492-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileOutputCommitter: Saved output of task 'attempt_202307141629267721110816130666273_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10/tweet/process_date=2023-07-10/_temporary/0/task_202307141629267721110816130666273_0001_m_000000
[2023-07-14T16:29:26.497-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SparkHadoopMapRedUtil: attempt_202307141629267721110816130666273_0001_m_000000_1: Committed. Elapsed time: 5 ms.
[2023-07-14T16:29:26.522-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-14T16:29:26.529-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 323 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:29:26.533-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T16:29:26.539-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,390 s
[2023-07-14T16:29:26.542-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:29:26.544-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T16:29:26.546-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,406240 s
[2023-07-14T16:29:26.556-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileFormatWriter: Start to commit write Job e92c8d0d-f12a-4a9b-81cb-e8508c455b42.
[2023-07-14T16:29:26.607-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileFormatWriter: Write Job e92c8d0d-f12a-4a9b-81cb-e8508c455b42 committed. Elapsed time: 47 ms.
[2023-07-14T16:29:26.626-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileFormatWriter: Finished processing stats for write job e92c8d0d-f12a-4a9b-81cb-e8508c455b42.
[2023-07-14T16:29:26.723-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:29:26.724-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:29:26.725-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T16:29:26.765-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:29:26.766-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:29:26.770-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:29:26.843-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO CodeGenerator: Code generated in 29.178787 ms
[2023-07-14T16:29:26.856-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T16:29:26.875-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T16:29:26.879-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:45113 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T16:29:26.881-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:26.882-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203307 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:29:26.917-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:26.920-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:29:26.920-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:29:26.921-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:29:26.921-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:29:26.922-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:29:26.952-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T16:29:26.955-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T16:29:26.960-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:45113 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T16:29:26.962-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:29:26.963-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:29:26.963-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T16:29:26.965-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:29:26.968-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T16:29:27.000-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:29:27.001-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:29:27.003-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:29:27.036-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9003, partition values: [empty row]
[2023-07-14T16:29:27.072-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO CodeGenerator: Code generated in 29.311577 ms
[2023-07-14T16:29:27.106-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO FileOutputCommitter: Saved output of task 'attempt_202307141629263605853456587886124_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-10/user/process_date=2023-07-10/_temporary/0/task_202307141629263605853456587886124_0002_m_000000
[2023-07-14T16:29:27.107-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO SparkHadoopMapRedUtil: attempt_202307141629263605853456587886124_0002_m_000000_2: Committed. Elapsed time: 5 ms.
[2023-07-14T16:29:27.116-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T16:29:27.123-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 157 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:29:27.125-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T16:29:27.133-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,199 s
[2023-07-14T16:29:27.133-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:29:27.133-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T16:29:27.134-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,216083 s
[2023-07-14T16:29:27.146-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO FileFormatWriter: Start to commit write Job 6f09ac73-9db3-4f20-99d7-a04bc6111c77.
[2023-07-14T16:29:27.207-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO FileFormatWriter: Write Job 6f09ac73-9db3-4f20-99d7-a04bc6111c77 committed. Elapsed time: 60 ms.
[2023-07-14T16:29:27.207-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO FileFormatWriter: Finished processing stats for write job 6f09ac73-9db3-4f20-99d7-a04bc6111c77.
[2023-07-14T16:29:27.330-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T16:29:27.362-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T16:29:27.395-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T16:29:27.415-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO MemoryStore: MemoryStore cleared
[2023-07-14T16:29:27.416-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO BlockManager: BlockManager stopped
[2023-07-14T16:29:27.428-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T16:29:27.432-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T16:29:27.447-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T16:29:27.448-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T16:29:27.448-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc7208a4-d0b7-43e4-afc3-3eb68d99ef6b
[2023-07-14T16:29:27.452-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-fce4c26f-baff-4cb6-88e9-99688bea4c42/pyspark-ccd536e0-ccea-4936-be79-091746a66761
[2023-07-14T16:29:27.461-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-fce4c26f-baff-4cb6-88e9-99688bea4c42
[2023-07-14T16:29:27.547-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T192910, end_date=20230714T192927
[2023-07-14T16:29:27.593-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:29:27.599-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:30:11.209-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T17:30:11.220-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T17:30:11.221-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:30:11.236-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-14T17:30:11.238-0300] {standard_task_runner.py:57} INFO - Started process 35964 to run task
[2023-07-14T17:30:11.242-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '58', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpu8_ndwrr']
[2023-07-14T17:30:11.244-0300] {standard_task_runner.py:85} INFO - Job 58: Subtask transform_twitter_datascience
[2023-07-14T17:30:11.337-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:30:11.523-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-14T17:30:11.538-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T17:30:11.541-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-10
[2023-07-14T17:30:15.292-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:15 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T17:30:15.297-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T17:30:16.721-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:16 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T17:30:16.824-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T17:30:16.969-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:16 INFO ResourceUtils: ==============================================================
[2023-07-14T17:30:16.970-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T17:30:16.971-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:16 INFO ResourceUtils: ==============================================================
[2023-07-14T17:30:16.971-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:16 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T17:30:17.016-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T17:30:17.036-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T17:30:17.036-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T17:30:17.105-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T17:30:17.106-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T17:30:17.106-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T17:30:17.108-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T17:30:17.108-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T17:30:17.474-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO Utils: Successfully started service 'sparkDriver' on port 40335.
[2023-07-14T17:30:17.518-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T17:30:17.584-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T17:30:17.611-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T17:30:17.612-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T17:30:17.622-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T17:30:17.659-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c3b1a645-66ae-4c6c-9599-2aaf00973d96
[2023-07-14T17:30:17.684-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T17:30:17.704-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T17:30:17.984-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T17:30:17.993-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:17 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T17:30:18.137-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T17:30:18.150-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T17:30:18.177-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39193.
[2023-07-14T17:30:18.178-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO NettyBlockTransferService: Server created on 192.168.0.102:39193
[2023-07-14T17:30:18.183-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T17:30:18.192-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 39193, None)
[2023-07-14T17:30:18.200-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:39193 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 39193, None)
[2023-07-14T17:30:18.203-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 39193, None)
[2023-07-14T17:30:18.204-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 39193, None)
[2023-07-14T17:30:18.822-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T17:30:18.834-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:18 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T17:30:20.226-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:20 INFO InMemoryFileIndex: It took 81 ms to list leaf files for 1 paths.
[2023-07-14T17:30:20.347-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:20 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2023-07-14T17:30:24.261-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:24 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:30:24.268-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:30:24.272-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:24 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T17:30:24.990-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T17:30:25.073-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T17:30:25.082-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:39193 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:25.089-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:25.102-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207825 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:25.429-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:25.471-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:25.471-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:25.472-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:25.475-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:25.494-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:25.796-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T17:30:25.806-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T17:30:25.810-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:39193 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:25.813-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:25.872-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:25.877-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T17:30:25.963-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:25.984-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T17:30:26.157-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13521, partition values: [empty row]
[2023-07-14T17:30:26.474-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO CodeGenerator: Code generated in 212.099007 ms
[2023-07-14T17:30:26.570-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T17:30:26.604-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 629 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:26.607-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T17:30:26.619-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,067 s
[2023-07-14T17:30:26.621-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:26.622-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T17:30:26.625-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:26 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,194828 s
[2023-07-14T17:30:27.252-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T17:30:27.254-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T17:30:27.254-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T17:30:27.367-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:27.367-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:27.368-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:27.575-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO CodeGenerator: Code generated in 120.312981 ms
[2023-07-14T17:30:27.586-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-14T17:30:27.603-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-14T17:30:27.604-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:39193 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:30:27.605-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:27.610-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207825 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:27.693-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:39193 in memory (size: 7.0 KiB, free: 434.3 MiB)
[2023-07-14T17:30:27.703-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:27.705-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:27.705-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:27.705-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:27.705-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:27.706-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:27.739-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.7 MiB)
[2023-07-14T17:30:27.742-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.6 MiB)
[2023-07-14T17:30:27.744-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:39193 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T17:30:27.747-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:27.749-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:27.750-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T17:30:27.753-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:27.754-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T17:30:27.825-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:27.825-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:27.829-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:27.872-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13521, partition values: [empty row]
[2023-07-14T17:30:27.917-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO CodeGenerator: Code generated in 39.313888 ms
[2023-07-14T17:30:27.950-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:27 INFO CodeGenerator: Code generated in 10.38648 ms
[2023-07-14T17:30:28.028-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileOutputCommitter: Saved output of task 'attempt_202307141730274667324881911968598_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-10/_temporary/0/task_202307141730274667324881911968598_0001_m_000000
[2023-07-14T17:30:28.032-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SparkHadoopMapRedUtil: attempt_202307141730274667324881911968598_0001_m_000000_1: Committed. Elapsed time: 2 ms.
[2023-07-14T17:30:28.050-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T17:30:28.055-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 304 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:28.056-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T17:30:28.059-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,350 s
[2023-07-14T17:30:28.061-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:28.061-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T17:30:28.062-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,358379 s
[2023-07-14T17:30:28.073-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileFormatWriter: Start to commit write Job cfbbfe1e-c258-4429-9ed4-765c29105230.
[2023-07-14T17:30:28.104-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileFormatWriter: Write Job cfbbfe1e-c258-4429-9ed4-765c29105230 committed. Elapsed time: 29 ms.
[2023-07-14T17:30:28.108-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileFormatWriter: Finished processing stats for write job cfbbfe1e-c258-4429-9ed4-765c29105230.
[2023-07-14T17:30:28.211-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:30:28.211-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:30:28.215-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T17:30:28.232-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:28.232-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:28.233-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:28.284-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO CodeGenerator: Code generated in 24.856137 ms
[2023-07-14T17:30:28.299-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.4 MiB)
[2023-07-14T17:30:28.355-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.4 MiB)
[2023-07-14T17:30:28.356-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:39193 in memory (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:30:28.358-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:39193 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:30:28.361-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:28.365-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207825 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:28.370-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:39193 in memory (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T17:30:28.426-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:39193 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:28.453-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:28.455-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:28.455-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:28.456-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:28.456-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:28.459-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:28.501-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 434.0 MiB)
[2023-07-14T17:30:28.520-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.9 MiB)
[2023-07-14T17:30:28.520-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:39193 (size: 76.9 KiB, free: 434.3 MiB)
[2023-07-14T17:30:28.521-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:28.522-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:28.522-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T17:30:28.526-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:28.527-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T17:30:28.561-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:28.562-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:28.567-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:28.643-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13521, partition values: [empty row]
[2023-07-14T17:30:28.693-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO CodeGenerator: Code generated in 39.917744 ms
[2023-07-14T17:30:28.718-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileOutputCommitter: Saved output of task 'attempt_202307141730285571857840455037484_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-10/_temporary/0/task_202307141730285571857840455037484_0002_m_000000
[2023-07-14T17:30:28.719-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SparkHadoopMapRedUtil: attempt_202307141730285571857840455037484_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-07-14T17:30:28.725-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T17:30:28.728-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 202 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:28.728-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T17:30:28.733-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,267 s
[2023-07-14T17:30:28.734-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:28.735-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T17:30:28.735-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,281742 s
[2023-07-14T17:30:28.737-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileFormatWriter: Start to commit write Job ce661811-e653-4c4e-ad75-0970e5cb15d6.
[2023-07-14T17:30:28.773-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileFormatWriter: Write Job ce661811-e653-4c4e-ad75-0970e5cb15d6 committed. Elapsed time: 35 ms.
[2023-07-14T17:30:28.774-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO FileFormatWriter: Finished processing stats for write job ce661811-e653-4c4e-ad75-0970e5cb15d6.
[2023-07-14T17:30:28.940-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T17:30:28.976-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:28 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T17:30:29.017-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T17:30:29.044-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO MemoryStore: MemoryStore cleared
[2023-07-14T17:30:29.047-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO BlockManager: BlockManager stopped
[2023-07-14T17:30:29.059-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T17:30:29.075-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T17:30:29.092-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T17:30:29.093-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T17:30:29.094-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-75ce6311-c10b-4b3d-9e58-6d19ecc30844/pyspark-8b03515d-227f-4217-9874-736d2cb5d30c
[2023-07-14T17:30:29.106-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c56f6f4-7eb2-4f28-a502-d1ec8bdb4f37
[2023-07-14T17:30:29.109-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-75ce6311-c10b-4b3d-9e58-6d19ecc30844
[2023-07-14T17:30:29.206-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230714T203011, end_date=20230714T203029
[2023-07-14T17:30:29.261-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:30:29.283-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:38:24.637-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T23:38:24.650-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T23:38:24.651-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:38:24.672-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-14T23:38:24.677-0300] {standard_task_runner.py:57} INFO - Started process 21852 to run task
[2023-07-14T23:38:24.682-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '73', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpsy56rnir']
[2023-07-14T23:38:24.684-0300] {standard_task_runner.py:85} INFO - Job 73: Subtask transform_twitter_datascience
[2023-07-14T23:38:24.776-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:38:24.941-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-14T23:38:24.954-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T23:38:24.959-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-10
[2023-07-14T23:38:26.849-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:26 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-14T23:38:26.852-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T23:38:27.814-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:27 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T23:38:27.991-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T23:38:28.109-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO ResourceUtils: ==============================================================
[2023-07-14T23:38:28.110-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T23:38:28.110-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO ResourceUtils: ==============================================================
[2023-07-14T23:38:28.111-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T23:38:28.130-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T23:38:28.142-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T23:38:28.142-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T23:38:28.196-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T23:38:28.196-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T23:38:28.197-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T23:38:28.197-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T23:38:28.198-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T23:38:28.501-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO Utils: Successfully started service 'sparkDriver' on port 37537.
[2023-07-14T23:38:28.525-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T23:38:28.558-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T23:38:28.576-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T23:38:28.576-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T23:38:28.582-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T23:38:28.603-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5a74c675-1ba6-441d-8bfb-92bd59ea0f87
[2023-07-14T23:38:28.622-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T23:38:28.642-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T23:38:28.910-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T23:38:28.922-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:28 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T23:38:29.025-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-14T23:38:29.032-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T23:38:29.049-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34611.
[2023-07-14T23:38:29.049-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO NettyBlockTransferService: Server created on 192.168.0.177:34611
[2023-07-14T23:38:29.051-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T23:38:29.056-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 34611, None)
[2023-07-14T23:38:29.060-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:34611 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 34611, None)
[2023-07-14T23:38:29.062-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 34611, None)
[2023-07-14T23:38:29.063-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 34611, None)
[2023-07-14T23:38:29.665-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T23:38:29.689-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:29 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T23:38:31.377-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:31 INFO InMemoryFileIndex: It took 74 ms to list leaf files for 1 paths.
[2023-07-14T23:38:31.495-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:31 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
[2023-07-14T23:38:33.633-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:33 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:38:33.636-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:33 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:38:33.641-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:33 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T23:38:34.041-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T23:38:34.096-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T23:38:34.107-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:34611 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T23:38:34.112-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:34.120-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198808 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:38:34.299-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:34.329-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:38:34.330-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:38:34.332-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:38:34.334-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:38:34.342-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:38:34.459-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T23:38:34.461-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T23:38:34.462-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:34611 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T23:38:34.462-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T23:38:34.476-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:38:34.478-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T23:38:34.519-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T23:38:34.533-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T23:38:34.638-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4504, partition values: [empty row]
[2023-07-14T23:38:34.827-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO CodeGenerator: Code generated in 157.002307 ms
[2023-07-14T23:38:34.898-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-07-14T23:38:34.906-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 395 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:38:34.911-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T23:38:34.921-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,563 s
[2023-07-14T23:38:34.925-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:38:34.927-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T23:38:34.931-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:34 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,631623 s
[2023-07-14T23:38:35.776-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:34611 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T23:38:35.778-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T23:38:35.786-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:35 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T23:38:35.789-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:35 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T23:38:35.812-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:34611 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T23:38:35.974-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:38:35.975-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:38:35.976-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:38:36.347-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO CodeGenerator: Code generated in 169.994019 ms
[2023-07-14T23:38:36.356-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T23:38:36.376-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T23:38:36.378-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:34611 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T23:38:36.380-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:36.385-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198808 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:38:36.471-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:36.473-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:38:36.473-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:38:36.473-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:38:36.473-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:38:36.477-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:38:36.517-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T23:38:36.521-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T23:38:36.522-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:34611 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T23:38:36.523-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T23:38:36.525-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:38:36.525-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T23:38:36.529-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T23:38:36.530-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T23:38:36.579-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:38:36.579-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:38:36.580-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:38:36.609-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4504, partition values: [empty row]
[2023-07-14T23:38:36.636-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO CodeGenerator: Code generated in 23.527007 ms
[2023-07-14T23:38:36.660-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO CodeGenerator: Code generated in 5.890774 ms
[2023-07-14T23:38:36.692-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileOutputCommitter: Saved output of task 'attempt_202307142338368271491627687041674_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-10/_temporary/0/task_202307142338368271491627687041674_0001_m_000000
[2023-07-14T23:38:36.693-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SparkHadoopMapRedUtil: attempt_202307142338368271491627687041674_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T23:38:36.700-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T23:38:36.702-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 176 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:38:36.703-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T23:38:36.705-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,229 s
[2023-07-14T23:38:36.706-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:38:36.706-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T23:38:36.707-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,234853 s
[2023-07-14T23:38:36.709-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileFormatWriter: Start to commit write Job 32a25cb1-2f67-4563-ad15-e44d8f5653c4.
[2023-07-14T23:38:36.742-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileFormatWriter: Write Job 32a25cb1-2f67-4563-ad15-e44d8f5653c4 committed. Elapsed time: 31 ms.
[2023-07-14T23:38:36.746-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileFormatWriter: Finished processing stats for write job 32a25cb1-2f67-4563-ad15-e44d8f5653c4.
[2023-07-14T23:38:36.798-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:38:36.799-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:38:36.799-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T23:38:36.812-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:38:36.813-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:38:36.813-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:38:36.848-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO CodeGenerator: Code generated in 15.624953 ms
[2023-07-14T23:38:36.853-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T23:38:36.866-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T23:38:36.867-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:34611 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T23:38:36.868-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:36.869-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198808 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:38:36.892-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:36.893-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:38:36.893-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:38:36.893-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:38:36.894-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:38:36.895-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:38:36.914-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:34611 in memory (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T23:38:36.917-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:34611 in memory (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T23:38:36.928-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 434.0 MiB)
[2023-07-14T23:38:36.930-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.9 MiB)
[2023-07-14T23:38:36.931-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:34611 (size: 76.9 KiB, free: 434.3 MiB)
[2023-07-14T23:38:36.932-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T23:38:36.933-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:38:36.933-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T23:38:36.935-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T23:38:36.935-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T23:38:36.963-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:38:36.963-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:38:36.964-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:38:36.979-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4504, partition values: [empty row]
[2023-07-14T23:38:36.994-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:36 INFO CodeGenerator: Code generated in 11.786112 ms
[2023-07-14T23:38:37.004-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO FileOutputCommitter: Saved output of task 'attempt_202307142338363640641845709061145_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-10/_temporary/0/task_202307142338363640641845709061145_0002_m_000000
[2023-07-14T23:38:37.005-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO SparkHadoopMapRedUtil: attempt_202307142338363640641845709061145_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-14T23:38:37.006-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T23:38:37.008-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 74 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:38:37.008-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T23:38:37.009-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,114 s
[2023-07-14T23:38:37.009-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:38:37.009-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T23:38:37.010-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,117513 s
[2023-07-14T23:38:37.011-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO FileFormatWriter: Start to commit write Job 51203ae1-9274-44b6-af16-68b9a4b011f1.
[2023-07-14T23:38:37.025-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO FileFormatWriter: Write Job 51203ae1-9274-44b6-af16-68b9a4b011f1 committed. Elapsed time: 13 ms.
[2023-07-14T23:38:37.025-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO FileFormatWriter: Finished processing stats for write job 51203ae1-9274-44b6-af16-68b9a4b011f1.
[2023-07-14T23:38:37.060-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T23:38:37.070-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4041
[2023-07-14T23:38:37.080-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T23:38:37.089-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO MemoryStore: MemoryStore cleared
[2023-07-14T23:38:37.089-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO BlockManager: BlockManager stopped
[2023-07-14T23:38:37.093-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T23:38:37.096-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T23:38:37.099-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T23:38:37.100-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T23:38:37.100-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-145c7fcc-c4a9-4888-bf8e-222af8d95173/pyspark-0e43d329-537d-4a58-8d51-49461c8df334
[2023-07-14T23:38:37.103-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-145c7fcc-c4a9-4888-bf8e-222af8d95173
[2023-07-14T23:38:37.105-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-9240d1ee-5e24-48da-996f-4215f1627a7b
[2023-07-14T23:38:37.164-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230715T023824, end_date=20230715T023837
[2023-07-14T23:38:37.183-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:38:37.205-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:43:55.730-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T23:43:55.736-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-14T23:43:55.736-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:43:55.752-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-14T23:43:55.756-0300] {standard_task_runner.py:57} INFO - Started process 24478 to run task
[2023-07-14T23:43:55.762-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '75', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp0i0bxnir']
[2023-07-14T23:43:55.763-0300] {standard_task_runner.py:85} INFO - Job 75: Subtask transform_twitter_datascience
[2023-07-14T23:43:55.803-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:43:55.878-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-14T23:43:55.884-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T23:43:55.887-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-10
[2023-07-14T23:43:57.536-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:57 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-14T23:43:57.537-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T23:43:57.548-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-14T23:43:57.548-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-14T23:43:57.549-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-14T23:43:57.549-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-14T23:43:57.549-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-14T23:43:57.951-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T23:43:58.583-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-14T23:43:58.602-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO SparkContext: Running Spark version 3.1.3
[2023-07-14T23:43:58.730-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO ResourceUtils: ==============================================================
[2023-07-14T23:43:58.731-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T23:43:58.731-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO ResourceUtils: ==============================================================
[2023-07-14T23:43:58.732-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T23:43:58.790-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T23:43:58.844-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T23:43:58.846-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T23:43:58.895-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T23:43:58.896-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T23:43:58.896-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T23:43:58.896-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T23:43:58.896-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T23:43:59.136-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO Utils: Successfully started service 'sparkDriver' on port 40571.
[2023-07-14T23:43:59.166-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T23:43:59.201-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T23:43:59.234-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T23:43:59.235-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T23:43:59.244-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T23:43:59.273-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0bf9395d-301b-45e3-b325-dee83d5b99f7
[2023-07-14T23:43:59.325-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T23:43:59.365-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T23:43:59.811-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T23:43:59.821-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T23:43:59.910-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4041
[2023-07-14T23:44:00.103-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-14T23:44:00.137-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40013.
[2023-07-14T23:44:00.137-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO NettyBlockTransferService: Server created on 192.168.0.177:40013
[2023-07-14T23:44:00.141-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T23:44:00.150-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 40013, None)
[2023-07-14T23:44:00.154-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:40013 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 40013, None)
[2023-07-14T23:44:00.158-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 40013, None)
[2023-07-14T23:44:00.160-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 40013, None)
[2023-07-14T23:44:00.706-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-14T23:44:00.707-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:00 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T23:44:01.541-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:01 INFO InMemoryFileIndex: It took 74 ms to list leaf files for 1 paths.
[2023-07-14T23:44:01.646-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:01 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
[2023-07-14T23:44:03.611-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:03 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:44:03.613-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:03 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:44:03.619-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T23:44:03.976-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-14T23:44:04.031-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-14T23:44:04.033-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:40013 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-14T23:44:04.038-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:04.045-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198803 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:44:04.211-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:04.228-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:44:04.228-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:44:04.228-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:44:04.230-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:44:04.234-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:44:04.322-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-14T23:44:04.326-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-14T23:44:04.327-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:40013 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:44:04.328-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:44:04.340-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:44:04.341-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T23:44:04.388-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-14T23:44:04.401-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T23:44:04.514-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4499, partition values: [empty row]
[2023-07-14T23:44:04.778-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO CodeGenerator: Code generated in 147.545836 ms
[2023-07-14T23:44:04.824-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2023-07-14T23:44:04.834-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 454 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:44:04.836-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T23:44:04.841-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,593 s
[2023-07-14T23:44:04.846-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:44:04.846-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T23:44:04.849-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,637460 s
[2023-07-14T23:44:05.272-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T23:44:05.273-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-14T23:44:05.274-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T23:44:05.348-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:05.348-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:05.350-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:05.437-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO CodeGenerator: Code generated in 39.430753 ms
[2023-07-14T23:44:05.455-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:40013 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:44:05.460-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:40013 in memory (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-14T23:44:05.493-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO CodeGenerator: Code generated in 37.139968 ms
[2023-07-14T23:44:05.504-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.2 MiB)
[2023-07-14T23:44:05.519-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-14T23:44:05.521-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:40013 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-14T23:44:05.522-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:05.525-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198803 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:44:05.624-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:05.625-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:44:05.626-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:44:05.626-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:44:05.626-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:44:05.629-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:44:05.685-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 434.0 MiB)
[2023-07-14T23:44:05.691-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 434.0 MiB)
[2023-07-14T23:44:05.691-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:40013 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-14T23:44:05.692-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:44:05.694-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:44:05.694-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T23:44:05.699-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:44:05.700-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T23:44:05.757-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:05.757-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:05.758-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:05.857-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO CodeGenerator: Code generated in 42.592667 ms
[2023-07-14T23:44:05.862-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4499, partition values: [empty row]
[2023-07-14T23:44:05.888-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO CodeGenerator: Code generated in 22.315793 ms
[2023-07-14T23:44:05.914-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO CodeGenerator: Code generated in 8.531153 ms
[2023-07-14T23:44:05.945-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileOutputCommitter: Saved output of task 'attempt_20230714234405700726408035588422_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-10/_temporary/0/task_20230714234405700726408035588422_0001_m_000000
[2023-07-14T23:44:05.946-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO SparkHadoopMapRedUtil: attempt_20230714234405700726408035588422_0001_m_000000_1: Committed
[2023-07-14T23:44:05.951-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-14T23:44:05.953-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 256 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:44:05.953-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T23:44:05.954-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,322 s
[2023-07-14T23:44:05.955-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:44:05.955-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T23:44:05.956-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,331821 s
[2023-07-14T23:44:05.971-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileFormatWriter: Write Job 6f52a0dc-3c5a-49a0-8dc0-0564e1d19ec0 committed.
[2023-07-14T23:44:05.974-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:05 INFO FileFormatWriter: Finished processing stats for write job 6f52a0dc-3c5a-49a0-8dc0-0564e1d19ec0.
[2023-07-14T23:44:06.022-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:44:06.022-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:44:06.022-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T23:44:06.034-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:06.034-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:06.035-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:06.066-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO CodeGenerator: Code generated in 14.393174 ms
[2023-07-14T23:44:06.069-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-14T23:44:06.078-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.8 MiB)
[2023-07-14T23:44:06.081-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:40013 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-14T23:44:06.082-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:06.083-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198803 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:44:06.110-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:06.112-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:44:06.113-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:44:06.114-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:44:06.114-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:44:06.115-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:44:06.148-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.6 MiB)
[2023-07-14T23:44:06.151-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.5 KiB, free 433.5 MiB)
[2023-07-14T23:44:06.151-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:40013 (size: 64.5 KiB, free: 434.2 MiB)
[2023-07-14T23:44:06.152-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:44:06.152-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:44:06.153-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T23:44:06.154-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:44:06.155-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T23:44:06.179-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:06.179-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:06.180-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:06.211-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO CodeGenerator: Code generated in 10.476104 ms
[2023-07-14T23:44:06.215-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4499, partition values: [empty row]
[2023-07-14T23:44:06.232-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO CodeGenerator: Code generated in 14.707732 ms
[2023-07-14T23:44:06.240-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileOutputCommitter: Saved output of task 'attempt_202307142344062046726258521268081_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-10/_temporary/0/task_202307142344062046726258521268081_0002_m_000000
[2023-07-14T23:44:06.240-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SparkHadoopMapRedUtil: attempt_202307142344062046726258521268081_0002_m_000000_2: Committed
[2023-07-14T23:44:06.242-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-14T23:44:06.244-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 91 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:44:06.245-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T23:44:06.247-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,130 s
[2023-07-14T23:44:06.248-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:44:06.248-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T23:44:06.248-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,138043 s
[2023-07-14T23:44:06.258-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileFormatWriter: Write Job 92c6a880-b864-413a-8953-b115908d54be committed.
[2023-07-14T23:44:06.259-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO FileFormatWriter: Finished processing stats for write job 92c6a880-b864-413a-8953-b115908d54be.
[2023-07-14T23:44:06.301-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T23:44:06.306-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4041
[2023-07-14T23:44:06.317-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T23:44:06.325-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO MemoryStore: MemoryStore cleared
[2023-07-14T23:44:06.325-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO BlockManager: BlockManager stopped
[2023-07-14T23:44:06.330-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T23:44:06.331-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T23:44:06.335-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T23:44:06.335-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T23:44:06.336-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-f20f6780-4473-48c5-b8b9-0c82b9bedbd1
[2023-07-14T23:44:06.338-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-0684a4bd-a324-4760-b3a2-ff60e11f5786
[2023-07-14T23:44:06.340-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-0684a4bd-a324-4760-b3a2-ff60e11f5786/pyspark-c4cc924b-ecf7-452b-8a3c-74fe0db3732d
[2023-07-14T23:44:06.383-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230715T024355, end_date=20230715T024406
[2023-07-14T23:44:06.424-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:44:06.448-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-15T12:12:24.518-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-15T12:12:24.537-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-15T12:12:24.537-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-15T12:12:24.579-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-10 00:00:00+00:00
[2023-07-15T12:12:24.591-0300] {standard_task_runner.py:57} INFO - Started process 13116 to run task
[2023-07-15T12:12:24.596-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '95', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1jb6ouq8']
[2023-07-15T12:12:24.601-0300] {standard_task_runner.py:85} INFO - Job 95: Subtask transform_twitter_datascience
[2023-07-15T12:12:24.701-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-10T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-15T12:12:24.860-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-15T12:12:24.870-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-15T12:12:24.876-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-10
[2023-07-15T12:12:27.586-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:27 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-15T12:12:27.589-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-15T12:12:27.720-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-15T12:12:27.721-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-15T12:12:27.721-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-15T12:12:27.721-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-15T12:12:27.721-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-15T12:12:28.653-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-15T12:12:29.968-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-15T12:12:30.001-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO SparkContext: Running Spark version 3.1.3
[2023-07-15T12:12:30.152-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO ResourceUtils: ==============================================================
[2023-07-15T12:12:30.154-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-15T12:12:30.155-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO ResourceUtils: ==============================================================
[2023-07-15T12:12:30.156-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-15T12:12:30.212-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-15T12:12:30.253-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO ResourceProfile: Limiting resource is cpu
[2023-07-15T12:12:30.257-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-15T12:12:30.370-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO SecurityManager: Changing view acls to: lucas
[2023-07-15T12:12:30.371-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-15T12:12:30.371-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO SecurityManager: Changing view acls groups to:
[2023-07-15T12:12:30.371-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO SecurityManager: Changing modify acls groups to:
[2023-07-15T12:12:30.373-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-15T12:12:31.078-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO Utils: Successfully started service 'sparkDriver' on port 36411.
[2023-07-15T12:12:31.123-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO SparkEnv: Registering MapOutputTracker
[2023-07-15T12:12:31.172-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-15T12:12:31.218-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-15T12:12:31.223-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-15T12:12:31.234-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-15T12:12:31.289-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6e5cfebb-6971-4981-9172-4b94a704102a
[2023-07-15T12:12:31.397-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-15T12:12:31.468-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:31 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-15T12:12:32.033-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-15T12:12:32.168-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[2023-07-15T12:12:32.576-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-15T12:12:32.633-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36487.
[2023-07-15T12:12:32.633-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO NettyBlockTransferService: Server created on 192.168.0.102:36487
[2023-07-15T12:12:32.640-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-15T12:12:32.651-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 36487, None)
[2023-07-15T12:12:32.661-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:36487 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 36487, None)
[2023-07-15T12:12:32.665-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 36487, None)
[2023-07-15T12:12:32.667-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 36487, None)
[2023-07-15T12:12:33.619-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-15T12:12:33.622-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:33 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-15T12:12:35.445-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:35 INFO InMemoryFileIndex: It took 101 ms to list leaf files for 1 paths.
[2023-07-15T12:12:35.619-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:35 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
[2023-07-15T12:12:39.652-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:39 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:12:39.656-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:39 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:12:39.661-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:39 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-15T12:12:40.278-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-15T12:12:40.401-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-15T12:12:40.412-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:36487 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:12:40.424-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:12:40.446-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203431 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:12:40.828-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:12:40.862-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:12:40.862-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:12:40.863-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:12:40.870-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:12:40.882-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:12:41.089-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-15T12:12:41.095-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-15T12:12:41.096-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:36487 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-15T12:12:41.098-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:12:41.126-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:12:41.127-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-15T12:12:41.243-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-15T12:12:41.279-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-15T12:12:41.525-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:41 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9127, partition values: [empty row]
[2023-07-15T12:12:42.063-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:42 INFO CodeGenerator: Code generated in 335.357551 ms
[2023-07-15T12:12:42.199-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2023-07-15T12:12:42.225-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1013 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:12:42.228-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-15T12:12:42.243-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:42 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,329 s
[2023-07-15T12:12:42.248-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:12:42.249-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-15T12:12:42.253-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:42 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,423609 s
[2023-07-15T12:12:43.229-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-15T12:12:43.232-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-15T12:12:43.232-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-15T12:12:43.427-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:12:43.428-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:12:43.429-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:12:43.618-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO CodeGenerator: Code generated in 77.045165 ms
[2023-07-15T12:12:43.740-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO CodeGenerator: Code generated in 80.993071 ms
[2023-07-15T12:12:43.751-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-15T12:12:43.774-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)
[2023-07-15T12:12:43.776-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:36487 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-15T12:12:43.777-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:12:43.781-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203431 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:12:43.927-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:12:43.928-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:12:43.928-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:12:43.929-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:12:43.929-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:12:43.930-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:43 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:12:44.017-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 433.8 MiB)
[2023-07-15T12:12:44.026-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 433.7 MiB)
[2023-07-15T12:12:44.027-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:36487 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-15T12:12:44.028-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:12:44.029-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:12:44.029-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-15T12:12:44.040-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:12:44.042-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-15T12:12:44.213-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:12:44.213-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:12:44.215-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:12:44.232-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:36487 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2023-07-15T12:12:44.371-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO CodeGenerator: Code generated in 58.964627 ms
[2023-07-15T12:12:44.377-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9127, partition values: [empty row]
[2023-07-15T12:12:44.441-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO CodeGenerator: Code generated in 58.209503 ms
[2023-07-15T12:12:44.498-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO CodeGenerator: Code generated in 18.44883 ms
[2023-07-15T12:12:44.609-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileOutputCommitter: Saved output of task 'attempt_202307151212434250040048674657683_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-10/_temporary/0/task_202307151212434250040048674657683_0001_m_000000
[2023-07-15T12:12:44.612-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO SparkHadoopMapRedUtil: attempt_202307151212434250040048674657683_0001_m_000000_1: Committed
[2023-07-15T12:12:44.641-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-15T12:12:44.653-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 622 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:12:44.654-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-15T12:12:44.668-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,723 s
[2023-07-15T12:12:44.669-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:12:44.669-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-15T12:12:44.669-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,733265 s
[2023-07-15T12:12:44.754-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileFormatWriter: Write Job a98a6a44-e467-4cd9-9d19-b23c0996e57c committed.
[2023-07-15T12:12:44.768-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileFormatWriter: Finished processing stats for write job a98a6a44-e467-4cd9-9d19-b23c0996e57c.
[2023-07-15T12:12:44.905-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:12:44.908-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:12:44.910-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-15T12:12:44.944-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:12:44.945-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:12:44.952-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:12:45.050-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO CodeGenerator: Code generated in 40.524941 ms
[2023-07-15T12:12:45.067-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.6 MiB)
[2023-07-15T12:12:45.092-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.6 MiB)
[2023-07-15T12:12:45.094-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:36487 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-15T12:12:45.096-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:12:45.098-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203431 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:12:45.156-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:12:45.159-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:12:45.160-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:12:45.160-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:12:45.160-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:12:45.161-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:12:45.196-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.4 MiB)
[2023-07-15T12:12:45.199-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.3 MiB)
[2023-07-15T12:12:45.200-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:36487 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-15T12:12:45.202-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:12:45.205-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:12:45.206-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-15T12:12:45.210-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:12:45.211-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-15T12:12:45.246-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:12:45.248-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:12:45.249-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:12:45.311-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO CodeGenerator: Code generated in 29.362697 ms
[2023-07-15T12:12:45.313-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9127, partition values: [empty row]
[2023-07-15T12:12:45.348-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO CodeGenerator: Code generated in 23.097701 ms
[2023-07-15T12:12:45.375-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO FileOutputCommitter: Saved output of task 'attempt_202307151212456394240440762256837_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-10/_temporary/0/task_202307151212456394240440762256837_0002_m_000000
[2023-07-15T12:12:45.376-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO SparkHadoopMapRedUtil: attempt_202307151212456394240440762256837_0002_m_000000_2: Committed
[2023-07-15T12:12:45.378-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-15T12:12:45.382-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 172 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:12:45.383-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-15T12:12:45.385-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,222 s
[2023-07-15T12:12:45.386-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:12:45.387-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-15T12:12:45.387-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,229596 s
[2023-07-15T12:12:45.439-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO FileFormatWriter: Write Job 1bee55b4-b6f3-4eea-b2bb-2fefaf8e0318 committed.
[2023-07-15T12:12:45.443-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO FileFormatWriter: Finished processing stats for write job 1bee55b4-b6f3-4eea-b2bb-2fefaf8e0318.
[2023-07-15T12:12:45.554-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-15T12:12:45.576-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-15T12:12:45.596-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-15T12:12:45.612-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO MemoryStore: MemoryStore cleared
[2023-07-15T12:12:45.613-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO BlockManager: BlockManager stopped
[2023-07-15T12:12:45.618-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-15T12:12:45.626-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-15T12:12:45.631-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO SparkContext: Successfully stopped SparkContext
[2023-07-15T12:12:45.631-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO ShutdownHookManager: Shutdown hook called
[2023-07-15T12:12:45.632-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-90d0f993-7f2e-46a0-b8a2-de764265e0d5/pyspark-a5f13a30-120d-45b0-9aff-28b511f3d9fc
[2023-07-15T12:12:45.640-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-44912afe-869f-4eb5-abcd-708de5cb387e
[2023-07-15T12:12:45.643-0300] {spark_submit.py:492} INFO - 23/07/15 12:12:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-90d0f993-7f2e-46a0-b8a2-de764265e0d5
[2023-07-15T12:12:45.781-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230710T000000, start_date=20230715T151224, end_date=20230715T151245
[2023-07-15T12:12:45.845-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-15T12:12:45.878-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
