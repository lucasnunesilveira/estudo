[2023-07-13T22:25:37.391-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:25:37.394-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:25:37.394-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:25:37.402-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-13T22:25:37.404-0300] {standard_task_runner.py:57} INFO - Started process 36746 to run task
[2023-07-13T22:25:37.406-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '13', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp9iw0mgw4']
[2023-07-13T22:25:37.407-0300] {standard_task_runner.py:85} INFO - Job 13: Subtask transform_twitter_datascience
[2023-07-13T22:25:37.427-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:25:37.477-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-13T22:25:37.482-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:25:37.484-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-12
[2023-07-13T22:25:38.965-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:38 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:25:38.968-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:25:38.982-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:25:38.982-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:25:38.982-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:25:38.982-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:25:38.982-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:25:39.030-0300] {spark_submit.py:492} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2023-07-13T22:25:39.030-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:631)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:271)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:234)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:119)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1022)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1022)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
[2023-07-13T22:25:39.031-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-07-13T22:25:39.060-0300] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 423, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-12. Error code is: 1.
[2023-07-13T22:25:39.066-0300] {taskinstance.py:1345} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T012537, end_date=20230714T012539
[2023-07-13T22:25:39.080-0300] {standard_task_runner.py:104} ERROR - Failed to execute job 13 for task transform_twitter_datascience (Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-12. Error code is: 1.; 36746)
[2023-07-13T22:25:39.102-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-13T22:25:39.109-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:31:41.911-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:31:41.920-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:31:41.920-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:31:41.937-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-13T22:31:41.939-0300] {standard_task_runner.py:57} INFO - Started process 39546 to run task
[2023-07-13T22:31:41.941-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '13', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpr6u5t977']
[2023-07-13T22:31:41.942-0300] {standard_task_runner.py:85} INFO - Job 13: Subtask transform_twitter_datascience
[2023-07-13T22:31:41.965-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:31:42.007-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-13T22:31:42.011-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:31:42.012-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-12
[2023-07-13T22:31:43.349-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:43 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:31:43.350-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:31:43.357-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:31:43.357-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:31:43.358-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:31:43.358-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:31:43.358-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:31:43.811-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:31:44.396-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-13T22:31:44.407-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SparkContext: Running Spark version 3.1.3
[2023-07-13T22:31:44.442-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:44.442-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:31:44.442-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:44.443-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:31:44.462-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:31:44.474-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:31:44.474-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:31:44.515-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:31:44.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:31:44.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:31:44.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:31:44.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:31:44.743-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO Utils: Successfully started service 'sparkDriver' on port 37839.
[2023-07-13T22:31:44.768-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:31:44.794-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:31:44.809-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:31:44.809-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:31:44.813-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:31:44.826-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d22fd98a-2914-45cf-b9fb-e2006bd342c7
[2023-07-13T22:31:44.846-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:31:44.866-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:44 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:31:45.060-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:31:45.115-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4040
[2023-07-13T22:31:45.311-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:31:45.332-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41661.
[2023-07-13T22:31:45.333-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO NettyBlockTransferService: Server created on 192.168.0.177:41661
[2023-07-13T22:31:45.335-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:31:45.342-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 41661, None)
[2023-07-13T22:31:45.345-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:41661 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 41661, None)
[2023-07-13T22:31:45.347-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 41661, None)
[2023-07-13T22:31:45.348-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 41661, None)
[2023-07-13T22:31:45.735-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-13T22:31:45.736-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:45 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:31:46.494-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:46 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
[2023-07-13T22:31:46.610-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:46 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 8 paths.
[2023-07-13T22:31:48.114-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:48.114-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:48.117-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:31:48.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-13T22:31:48.406-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-13T22:31:48.409-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:41661 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-13T22:31:48.412-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:48.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613286 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:48.560-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:48.576-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:48.576-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:48.576-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:48.577-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:48.580-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:48.670-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-13T22:31:48.672-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-13T22:31:48.672-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:41661 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-13T22:31:48.673-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:48.682-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:48.683-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:31:48.731-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6291 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:48.745-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:31:48.844-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [empty row]
[2023-07-13T22:31:49.084-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO CodeGenerator: Code generated in 137.080302 ms
[2023-07-13T22:31:49.122-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:31:49.130-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [empty row]
[2023-07-13T22:31:49.136-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:31:49.141-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [empty row]
[2023-07-13T22:31:49.144-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [empty row]
[2023-07-13T22:31:49.147-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [empty row]
[2023-07-13T22:31:49.150-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [empty row]
[2023-07-13T22:31:49.168-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2023-07-13T22:31:49.184-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 461 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:49.186-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:31:49.194-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,599 s
[2023-07-13T22:31:49.197-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:49.197-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:31:49.199-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,638428 s
[2023-07-13T22:31:49.543-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:31:49.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:31:49.547-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-13T22:31:49.548-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:31:49.612-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:49.612-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:49.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:49.681-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO CodeGenerator: Code generated in 24.683673 ms
[2023-07-13T22:31:49.743-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO CodeGenerator: Code generated in 41.471643 ms
[2023-07-13T22:31:49.747-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-13T22:31:49.757-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)
[2023-07-13T22:31:49.758-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:41661 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:31:49.759-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:49.762-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613286 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:49.816-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:49.817-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:49.817-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:49.818-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:49.818-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:49.818-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:49.868-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:41661 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2023-07-13T22:31:49.870-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 195.4 KiB, free 433.8 MiB)
[2023-07-13T22:31:49.872-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 68.5 KiB, free 433.7 MiB)
[2023-07-13T22:31:49.873-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:41661 (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:31:49.874-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:49.874-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:49.875-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:31:49.878-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:49.878-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:31:49.940-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:41661 in memory (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-13T22:31:49.947-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:49.947-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:49.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:50.007-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO CodeGenerator: Code generated in 19.390153 ms
[2023-07-13T22:31:50.009-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:31:50.031-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO CodeGenerator: Code generated in 18.182302 ms
[2023-07-13T22:31:50.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO CodeGenerator: Code generated in 4.600154 ms
[2023-07-13T22:31:50.075-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:31:50.081-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:31:50.088-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:31:50.094-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:31:50.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:31:50.104-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:31:50.108-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:31:50.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileOutputCommitter: Saved output of task 'attempt_202307132231496881750261510290694_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-12/_temporary/0/task_202307132231496881750261510290694_0001_m_000000
[2023-07-13T22:31:50.122-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SparkHadoopMapRedUtil: attempt_202307132231496881750261510290694_0001_m_000000_1: Committed
[2023-07-13T22:31:50.125-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3119 bytes result sent to driver
[2023-07-13T22:31:50.127-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 251 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:50.127-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:31:50.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,309 s
[2023-07-13T22:31:50.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:50.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:31:50.129-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,312701 s
[2023-07-13T22:31:50.144-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileFormatWriter: Write Job 2548d0ee-2bb6-41d7-b514-c0c7c94a56d4 committed.
[2023-07-13T22:31:50.147-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileFormatWriter: Finished processing stats for write job 2548d0ee-2bb6-41d7-b514-c0c7c94a56d4.
[2023-07-13T22:31:50.193-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:31:50.194-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:50.194-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:50.194-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:31:50.206-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:50.206-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:50.207-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:50.227-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO CodeGenerator: Code generated in 9.62487 ms
[2023-07-13T22:31:50.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO CodeGenerator: Code generated in 11.103935 ms
[2023-07-13T22:31:50.245-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-13T22:31:50.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.7 MiB)
[2023-07-13T22:31:50.258-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:41661 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:31:50.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:50.260-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613286 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:50.296-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:50.297-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:50.297-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:50.297-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:50.297-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:50.298-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:50.317-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 184.3 KiB, free 433.6 MiB)
[2023-07-13T22:31:50.321-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 433.5 MiB)
[2023-07-13T22:31:50.322-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:41661 (size: 65.7 KiB, free: 434.2 MiB)
[2023-07-13T22:31:50.324-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:50.325-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:50.325-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:31:50.326-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:50.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:31:50.340-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:50.341-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:50.341-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:50.362-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO CodeGenerator: Code generated in 6.583554 ms
[2023-07-13T22:31:50.363-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:31:50.377-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:41661 in memory (size: 27.6 KiB, free: 434.2 MiB)
[2023-07-13T22:31:50.380-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:41661 in memory (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:31:50.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO CodeGenerator: Code generated in 16.939388 ms
[2023-07-13T22:31:50.390-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:31:50.393-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:31:50.395-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:31:50.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:31:50.400-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:31:50.403-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:31:50.405-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:31:50.410-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileOutputCommitter: Saved output of task 'attempt_202307132231501696890964316151696_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-12/_temporary/0/task_202307132231501696890964316151696_0002_m_000000
[2023-07-13T22:31:50.410-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SparkHadoopMapRedUtil: attempt_202307132231501696890964316151696_0002_m_000000_2: Committed
[2023-07-13T22:31:50.412-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3014 bytes result sent to driver
[2023-07-13T22:31:50.413-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 87 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:50.413-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:31:50.414-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,114 s
[2023-07-13T22:31:50.414-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:50.414-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:31:50.414-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,118007 s
[2023-07-13T22:31:50.427-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileFormatWriter: Write Job 2a2f5146-3365-4450-babc-e6a7adf35b98 committed.
[2023-07-13T22:31:50.427-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO FileFormatWriter: Finished processing stats for write job 2a2f5146-3365-4450-babc-e6a7adf35b98.
[2023-07-13T22:31:50.462-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:31:50.469-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:31:50.478-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:31:50.486-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:31:50.486-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO BlockManager: BlockManager stopped
[2023-07-13T22:31:50.490-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:31:50.491-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:31:50.495-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:31:50.495-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:31:50.495-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9d00cb1-260b-4b5a-86d1-ced7c9eb2ad5
[2023-07-13T22:31:50.498-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9d00cb1-260b-4b5a-86d1-ced7c9eb2ad5/pyspark-c099a0e2-ab1f-4bf3-ab63-6702b184be1c
[2023-07-13T22:31:50.499-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-108d1876-bddd-4ffc-b0a7-d3be3e5b5dbb
[2023-07-13T22:31:50.544-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T013141, end_date=20230714T013150
[2023-07-13T22:31:50.557-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:31:50.563-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:43:48.546-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:43:48.554-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:43:48.554-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:43:48.571-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-13T22:43:48.574-0300] {standard_task_runner.py:57} INFO - Started process 44691 to run task
[2023-07-13T22:43:48.577-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpylogh16t']
[2023-07-13T22:43:48.578-0300] {standard_task_runner.py:85} INFO - Job 32: Subtask transform_twitter_datascience
[2023-07-13T22:43:48.613-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:43:48.678-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-13T22:43:48.683-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:43:48.685-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-12
[2023-07-13T22:43:50.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:43:50.063-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:43:50.774-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:43:50.817-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:43:50.885-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO ResourceUtils: ==============================================================
[2023-07-13T22:43:50.885-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:43:50.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO ResourceUtils: ==============================================================
[2023-07-13T22:43:50.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:43:50.904-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:43:50.915-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:43:50.916-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:43:50.961-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:43:50.961-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:43:50.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:43:50.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:43:50.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:43:51.155-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO Utils: Successfully started service 'sparkDriver' on port 45567.
[2023-07-13T22:43:51.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:43:51.203-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:43:51.217-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:43:51.218-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:43:51.221-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:43:51.237-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e8cefeb7-28e0-47a4-9bb0-b2d72c5be91b
[2023-07-13T22:43:51.251-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:43:51.267-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:43:51.437-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:43:51.511-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:43:51.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:43:51.532-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36451.
[2023-07-13T22:43:51.532-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO NettyBlockTransferService: Server created on 192.168.0.177:36451
[2023-07-13T22:43:51.533-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:43:51.540-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 36451, None)
[2023-07-13T22:43:51.543-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:36451 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 36451, None)
[2023-07-13T22:43:51.545-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 36451, None)
[2023-07-13T22:43:51.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 36451, None)
[2023-07-13T22:43:51.900-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:43:51.906-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:51 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:43:52.711-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:52 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
[2023-07-13T22:43:52.808-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:52 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:43:54.328-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:43:54.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:43:54.331-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:43:54.543-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:43:54.599-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:54.601-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:36451 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:54.605-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:54.616-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635929 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:54.757-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:54.770-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:54.770-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:54.770-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:54.771-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:54.775-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:54.867-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:43:54.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:54.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:36451 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:54.870-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:54.881-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:54.881-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:43:54.926-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:54.938-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:43:55.035-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:43:55.192-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO CodeGenerator: Code generated in 119.341396 ms
[2023-07-13T22:43:55.237-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [empty row]
[2023-07-13T22:43:55.246-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [empty row]
[2023-07-13T22:43:55.252-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:43:55.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:43:55.262-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [empty row]
[2023-07-13T22:43:55.266-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [empty row]
[2023-07-13T22:43:55.270-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [empty row]
[2023-07-13T22:43:55.290-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:43:55.301-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 384 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:55.307-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:43:55.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,541 s
[2023-07-13T22:43:55.335-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:55.335-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:43:55.341-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,582654 s
[2023-07-13T22:43:55.728-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:43:55.730-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:43:55.731-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:43:55.732-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:43:55.792-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:55.792-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:55.793-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:55.930-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:36451 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:55.935-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:36451 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:55.961-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO CodeGenerator: Code generated in 101.03672 ms
[2023-07-13T22:43:55.965-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:55.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:43:55.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:36451 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:43:55.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:55.980-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635929 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:56.031-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:56.032-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:56.032-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:56.032-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:56.032-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:56.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:56.054-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.5 KiB, free 433.9 MiB)
[2023-07-13T22:43:56.057-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:43:56.058-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:36451 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:43:56.058-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:56.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:56.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:43:56.062-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:56.062-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:43:56.100-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:56.100-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:56.101-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:56.127-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:43:56.151-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO CodeGenerator: Code generated in 19.861416 ms
[2023-07-13T22:43:56.172-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO CodeGenerator: Code generated in 5.941324 ms
[2023-07-13T22:43:56.204-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:43:56.215-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:43:56.223-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:43:56.231-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:43:56.239-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:43:56.244-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:43:56.249-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:43:56.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileOutputCommitter: Saved output of task 'attempt_202307132243557451042887889903697_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-12/_temporary/0/task_202307132243557451042887889903697_0001_m_000000
[2023-07-13T22:43:56.260-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkHadoopMapRedUtil: attempt_202307132243557451042887889903697_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:43:56.265-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:43:56.267-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 207 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:56.267-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:43:56.268-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,234 s
[2023-07-13T22:43:56.268-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:56.268-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:43:56.269-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,237923 s
[2023-07-13T22:43:56.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileFormatWriter: Start to commit write Job e0011ea6-0194-4595-a5fa-e500dbcfed0b.
[2023-07-13T22:43:56.284-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileFormatWriter: Write Job e0011ea6-0194-4595-a5fa-e500dbcfed0b committed. Elapsed time: 12 ms.
[2023-07-13T22:43:56.287-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileFormatWriter: Finished processing stats for write job e0011ea6-0194-4595-a5fa-e500dbcfed0b.
[2023-07-13T22:43:56.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:43:56.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:43:56.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:43:56.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:43:56.323-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:56.323-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:56.323-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:56.359-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO CodeGenerator: Code generated in 18.261226 ms
[2023-07-13T22:43:56.363-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:43:56.373-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:43:56.373-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:36451 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:43:56.374-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:56.375-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635929 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:56.396-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:56.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:56.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:56.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:56.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:56.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:56.413-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.5 KiB, free 433.4 MiB)
[2023-07-13T22:43:56.416-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:43:56.418-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:36451 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:43:56.418-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:56.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:56.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:43:56.420-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:56.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:43:56.435-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:56.435-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:56.436-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:56.447-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:43:56.461-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO CodeGenerator: Code generated in 11.447912 ms
[2023-07-13T22:43:56.469-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:43:56.472-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:43:56.476-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:43:56.480-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:43:56.484-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:43:56.487-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:43:56.490-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:43:56.495-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileOutputCommitter: Saved output of task 'attempt_202307132243565461287631995937580_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-12/_temporary/0/task_202307132243565461287631995937580_0002_m_000000
[2023-07-13T22:43:56.495-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkHadoopMapRedUtil: attempt_202307132243565461287631995937580_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-13T22:43:56.498-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:43:56.500-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 79 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:56.500-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:43:56.501-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,102 s
[2023-07-13T22:43:56.501-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:56.501-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:43:56.502-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,105426 s
[2023-07-13T22:43:56.502-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileFormatWriter: Start to commit write Job bbaa397e-3e13-4634-a020-7fb69b8e1f2b.
[2023-07-13T22:43:56.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileFormatWriter: Write Job bbaa397e-3e13-4634-a020-7fb69b8e1f2b committed. Elapsed time: 9 ms.
[2023-07-13T22:43:56.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO FileFormatWriter: Finished processing stats for write job bbaa397e-3e13-4634-a020-7fb69b8e1f2b.
[2023-07-13T22:43:56.576-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:43:56.592-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:43:56.604-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:43:56.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:43:56.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO BlockManager: BlockManager stopped
[2023-07-13T22:43:56.616-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:43:56.618-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:43:56.622-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:43:56.622-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:43:56.623-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-42f3ac95-7947-42e6-ac1d-1d565d44c978
[2023-07-13T22:43:56.625-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-38b28eb2-5acb-4698-a156-9198e0cffe94/pyspark-aae7cccf-5d5e-4198-9c3c-3c328421ad46
[2023-07-13T22:43:56.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-38b28eb2-5acb-4698-a156-9198e0cffe94
[2023-07-13T22:43:56.680-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T014348, end_date=20230714T014356
[2023-07-13T22:43:56.719-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:43:56.729-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:51:59.448-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:51:59.452-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:51:59.452-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:51:59.460-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-13T22:51:59.462-0300] {standard_task_runner.py:57} INFO - Started process 49515 to run task
[2023-07-13T22:51:59.464-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '31', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpm59f6w42']
[2023-07-13T22:51:59.464-0300] {standard_task_runner.py:85} INFO - Job 31: Subtask transform_twitter_datascience
[2023-07-13T22:51:59.489-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:51:59.545-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-13T22:51:59.547-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:51:59.548-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest FormacaoAirflow/dados_transformation --process-date 2023-07-12
[2023-07-13T22:52:00.777-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:00 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:52:00.779-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:52:01.471-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:52:01.519-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:52:01.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO ResourceUtils: ==============================================================
[2023-07-13T22:52:01.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:52:01.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO ResourceUtils: ==============================================================
[2023-07-13T22:52:01.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:52:01.622-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:52:01.633-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:52:01.633-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:52:01.678-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:52:01.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:52:01.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:52:01.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:52:01.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:52:01.908-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO Utils: Successfully started service 'sparkDriver' on port 38049.
[2023-07-13T22:52:01.933-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:52:01.968-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:52:01.989-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:52:01.990-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:52:01.994-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:52:02.014-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-16cf6d30-f7dc-4704-bab8-c2823d2ae06b
[2023-07-13T22:52:02.030-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:52:02.046-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:52:02.232-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:52:02.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:52:02.334-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:52:02.354-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35839.
[2023-07-13T22:52:02.355-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO NettyBlockTransferService: Server created on 192.168.0.177:35839
[2023-07-13T22:52:02.357-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:52:02.362-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 35839, None)
[2023-07-13T22:52:02.365-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:35839 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 35839, None)
[2023-07-13T22:52:02.367-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 35839, None)
[2023-07-13T22:52:02.368-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 35839, None)
[2023-07-13T22:52:02.795-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:52:02.801-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:02 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:52:03.674-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:03 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
[2023-07-13T22:52:03.797-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:03 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 8 paths.
[2023-07-13T22:52:05.405-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:52:05.406-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:52:05.408-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:52:05.623-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:52:05.663-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:52:05.666-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:35839 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:52:05.669-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:52:05.678-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672133 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:52:05.807-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:52:05.819-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:52:05.819-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:52:05.820-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:52:05.821-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:52:05.824-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:52:05.913-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:52:05.915-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:52:05.915-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:35839 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:52:05.916-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:52:05.928-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:52:05.929-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:52:05.965-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:52:05.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:52:06.058-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:52:06.181-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO CodeGenerator: Code generated in 97.864473 ms
[2023-07-13T22:52:06.236-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [empty row]
[2023-07-13T22:52:06.247-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [empty row]
[2023-07-13T22:52:06.254-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [empty row]
[2023-07-13T22:52:06.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:52:06.263-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:52:06.266-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [empty row]
[2023-07-13T22:52:06.270-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [empty row]
[2023-07-13T22:52:06.287-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:52:06.294-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 336 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:52:06.295-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:52:06.303-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,467 s
[2023-07-13T22:52:06.305-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:52:06.306-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:52:06.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,499844 s
[2023-07-13T22:52:06.536-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:35839 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:52:06.547-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:35839 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:52:06.660-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:52:06.662-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:52:06.663-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:52:06.663-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:52:06.721-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:52:06.721-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:52:06.722-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:52:06.860-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO CodeGenerator: Code generated in 74.023658 ms
[2023-07-13T22:52:06.864-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:52:06.872-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:52:06.873-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:35839 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:52:06.874-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:52:06.878-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672133 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:52:06.947-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:52:06.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:52:06.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:52:06.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:52:06.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:52:06.949-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:52:06.971-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.4 KiB, free 433.9 MiB)
[2023-07-13T22:52:06.974-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:52:06.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:35839 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:52:06.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:52:06.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:52:06.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:52:06.979-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:52:06.980-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:52:07.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:52:07.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:52:07.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:52:07.058-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:52:07.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO CodeGenerator: Code generated in 15.088241 ms
[2023-07-13T22:52:07.093-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO CodeGenerator: Code generated in 4.143125 ms
[2023-07-13T22:52:07.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [19549]
[2023-07-13T22:52:07.130-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [19546]
[2023-07-13T22:52:07.137-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [19548]
[2023-07-13T22:52:07.142-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:52:07.150-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:52:07.155-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [19550]
[2023-07-13T22:52:07.159-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [19547]
[2023-07-13T22:52:07.186-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileOutputCommitter: Saved output of task 'attempt_202307132252067919127512557971676_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-12/_temporary/0/task_202307132252067919127512557971676_0001_m_000000
[2023-07-13T22:52:07.187-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SparkHadoopMapRedUtil: attempt_202307132252067919127512557971676_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:52:07.195-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:52:07.198-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 220 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:52:07.198-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:52:07.200-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,249 s
[2023-07-13T22:52:07.200-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:52:07.200-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:52:07.202-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,254365 s
[2023-07-13T22:52:07.204-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileFormatWriter: Start to commit write Job a65431d3-0a28-450e-9b8b-a7250946624f.
[2023-07-13T22:52:07.215-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileFormatWriter: Write Job a65431d3-0a28-450e-9b8b-a7250946624f committed. Elapsed time: 9 ms.
[2023-07-13T22:52:07.217-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileFormatWriter: Finished processing stats for write job a65431d3-0a28-450e-9b8b-a7250946624f.
[2023-07-13T22:52:07.243-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:52:07.244-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:52:07.244-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:52:07.244-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:52:07.252-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:52:07.252-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:52:07.252-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:52:07.288-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO CodeGenerator: Code generated in 15.617466 ms
[2023-07-13T22:52:07.292-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:52:07.306-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:52:07.307-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:35839 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:52:07.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:52:07.309-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672133 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:52:07.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:52:07.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:52:07.330-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:52:07.330-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:52:07.330-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:52:07.331-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:52:07.349-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:52:07.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:52:07.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:35839 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:52:07.352-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:52:07.352-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:52:07.352-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:52:07.354-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:52:07.354-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:52:07.367-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:52:07.368-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:52:07.368-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:52:07.380-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:52:07.391-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO CodeGenerator: Code generated in 8.752804 ms
[2023-07-13T22:52:07.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [19549]
[2023-07-13T22:52:07.402-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [19546]
[2023-07-13T22:52:07.405-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [19548]
[2023-07-13T22:52:07.408-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:52:07.410-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:52:07.413-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [19550]
[2023-07-13T22:52:07.416-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [19547]
[2023-07-13T22:52:07.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileOutputCommitter: Saved output of task 'attempt_202307132252078957468053581888243_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/user/process_date=2023-07-12/_temporary/0/task_202307132252078957468053581888243_0002_m_000000
[2023-07-13T22:52:07.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SparkHadoopMapRedUtil: attempt_202307132252078957468053581888243_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:52:07.422-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:52:07.423-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 70 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:52:07.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:52:07.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,093 s
[2023-07-13T22:52:07.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:52:07.425-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:52:07.425-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,096085 s
[2023-07-13T22:52:07.425-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileFormatWriter: Start to commit write Job d8aedb8c-2bb8-4019-9a2e-6f06c3af17e4.
[2023-07-13T22:52:07.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileFormatWriter: Write Job d8aedb8c-2bb8-4019-9a2e-6f06c3af17e4 committed. Elapsed time: 8 ms.
[2023-07-13T22:52:07.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO FileFormatWriter: Finished processing stats for write job d8aedb8c-2bb8-4019-9a2e-6f06c3af17e4.
[2023-07-13T22:52:07.462-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:52:07.473-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:52:07.485-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:52:07.492-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:52:07.492-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO BlockManager: BlockManager stopped
[2023-07-13T22:52:07.494-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:52:07.496-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:52:07.499-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:52:07.499-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:52:07.500-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e115682-d01f-4c01-9742-d6d55926e99a/pyspark-ff3a4fdd-1c88-4024-90f4-fc9a67006763
[2023-07-13T22:52:07.501-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-1f63da37-214a-47df-b736-7d5b871fe0dc
[2023-07-13T22:52:07.503-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e115682-d01f-4c01-9742-d6d55926e99a
[2023-07-13T22:52:07.546-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T015159, end_date=20230714T015207
[2023-07-13T22:52:07.571-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:52:07.577-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:53:32.282-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:53:32.289-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-13T22:53:32.289-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:53:32.301-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-13T22:53:32.304-0300] {standard_task_runner.py:57} INFO - Started process 51402 to run task
[2023-07-13T22:53:32.307-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp57t3dmxl']
[2023-07-13T22:53:32.308-0300] {standard_task_runner.py:85} INFO - Job 32: Subtask transform_twitter_datascience
[2023-07-13T22:53:32.330-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:53:32.367-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-13T22:53:32.370-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:53:32.370-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest dados_transformation --process-date 2023-07-12
[2023-07-13T22:53:33.555-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:33 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:53:33.557-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:53:34.230-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:53:34.274-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:53:34.341-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO ResourceUtils: ==============================================================
[2023-07-13T22:53:34.341-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:53:34.341-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO ResourceUtils: ==============================================================
[2023-07-13T22:53:34.342-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:53:34.359-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:53:34.369-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:53:34.369-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:53:34.408-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:53:34.408-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:53:34.409-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:53:34.409-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:53:34.409-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:53:34.603-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO Utils: Successfully started service 'sparkDriver' on port 39439.
[2023-07-13T22:53:34.633-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:53:34.666-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:53:34.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:53:34.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:53:34.685-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:53:34.704-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ed08a0c1-7727-4231-b256-4fb45b81b1fb
[2023-07-13T22:53:34.718-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:53:34.734-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:53:34.906-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:53:35.010-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:53:35.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:53:35.030-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38795.
[2023-07-13T22:53:35.031-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO NettyBlockTransferService: Server created on 192.168.0.177:38795
[2023-07-13T22:53:35.032-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:53:35.037-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 38795, None)
[2023-07-13T22:53:35.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:38795 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 38795, None)
[2023-07-13T22:53:35.042-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 38795, None)
[2023-07-13T22:53:35.043-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 38795, None)
[2023-07-13T22:53:35.412-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:53:35.418-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:35 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:53:36.056-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:36 INFO InMemoryFileIndex: It took 45 ms to list leaf files for 1 paths.
[2023-07-13T22:53:36.165-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:36 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 8 paths.
[2023-07-13T22:53:37.559-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:37.561-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:37.563-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:53:37.768-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:53:37.812-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:37.814-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:38795 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:37.817-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:37.824-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672327 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:37.964-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:37.977-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:37.978-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:37.978-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:37.979-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:37.982-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:38.069-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:53:38.071-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:38.071-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:38795 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:38.072-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:38.081-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:38.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:53:38.124-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:38.136-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:53:38.222-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [empty row]
[2023-07-13T22:53:38.371-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO CodeGenerator: Code generated in 112.541503 ms
[2023-07-13T22:53:38.411-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [empty row]
[2023-07-13T22:53:38.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-18025, partition values: [empty row]
[2023-07-13T22:53:38.428-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [empty row]
[2023-07-13T22:53:38.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [empty row]
[2023-07-13T22:53:38.439-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:53:38.443-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:53:38.446-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4503, partition values: [empty row]
[2023-07-13T22:53:38.463-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:53:38.470-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 353 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:38.472-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:53:38.478-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,482 s
[2023-07-13T22:53:38.481-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:38.481-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:53:38.483-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,519377 s
[2023-07-13T22:53:38.812-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:38.815-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:53:38.816-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:53:38.817-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:53:38.818-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:38795 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:38.823-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:38795 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:38.880-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:38.880-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:38.881-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:39.024-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO CodeGenerator: Code generated in 83.619995 ms
[2023-07-13T22:53:39.028-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:39.037-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:53:39.038-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:38795 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:53:39.039-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:39.042-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672327 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:39.089-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:39.091-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:39.091-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:39.091-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:39.091-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:39.092-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:39.122-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.3 KiB, free 433.9 MiB)
[2023-07-13T22:53:39.127-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:53:39.127-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:38795 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:53:39.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:39.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:39.129-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:53:39.132-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:39.132-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:53:39.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:39.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:39.174-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:39.199-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:39.233-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO CodeGenerator: Code generated in 29.078039 ms
[2023-07-13T22:53:39.257-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO CodeGenerator: Code generated in 5.791267 ms
[2023-07-13T22:53:39.293-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [19549]
[2023-07-13T22:53:39.302-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-18025, partition values: [19550]
[2023-07-13T22:53:39.309-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:39.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:39.322-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:39.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:39.332-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4503, partition values: [19551]
[2023-07-13T22:53:39.343-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileOutputCommitter: Saved output of task 'attempt_20230713225339690460245278992326_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-12/_temporary/0/task_20230713225339690460245278992326_0001_m_000000
[2023-07-13T22:53:39.344-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkHadoopMapRedUtil: attempt_20230713225339690460245278992326_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:53:39.349-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:53:39.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 222 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:39.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:53:39.352-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,259 s
[2023-07-13T22:53:39.353-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:39.353-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:53:39.354-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,263862 s
[2023-07-13T22:53:39.356-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileFormatWriter: Start to commit write Job 91f85b1c-c53a-4a55-b9e1-f718b75d9bf2.
[2023-07-13T22:53:39.369-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileFormatWriter: Write Job 91f85b1c-c53a-4a55-b9e1-f718b75d9bf2 committed. Elapsed time: 12 ms.
[2023-07-13T22:53:39.372-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileFormatWriter: Finished processing stats for write job 91f85b1c-c53a-4a55-b9e1-f718b75d9bf2.
[2023-07-13T22:53:39.408-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:39.408-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:39.408-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:39.409-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:53:39.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:39.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:39.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:39.446-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO CodeGenerator: Code generated in 11.57026 ms
[2023-07-13T22:53:39.450-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:53:39.459-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:53:39.460-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:38795 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:53:39.461-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:39.462-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672327 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:39.483-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:39.484-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:39.484-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:39.484-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:39.484-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:39.485-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:39.500-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:38795 in memory (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:53:39.502-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:38795 in memory (size: 83.4 KiB, free: 434.4 MiB)
[2023-07-13T22:53:39.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 434.0 MiB)
[2023-07-13T22:53:39.518-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.9 MiB)
[2023-07-13T22:53:39.519-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:38795 (size: 77.2 KiB, free: 434.3 MiB)
[2023-07-13T22:53:39.520-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:39.520-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:39.521-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:53:39.522-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:39.523-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:53:39.541-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:39.541-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:39.542-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:39.553-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:39.566-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO CodeGenerator: Code generated in 10.67622 ms
[2023-07-13T22:53:39.572-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [19549]
[2023-07-13T22:53:39.575-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-18025, partition values: [19550]
[2023-07-13T22:53:39.579-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:39.582-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:39.585-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:39.588-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:39.590-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4503, partition values: [19551]
[2023-07-13T22:53:39.594-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253391270232740878527945_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-12/_temporary/0/task_202307132253391270232740878527945_0002_m_000000
[2023-07-13T22:53:39.595-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkHadoopMapRedUtil: attempt_202307132253391270232740878527945_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:53:39.596-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:53:39.597-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 76 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:39.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:53:39.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,112 s
[2023-07-13T22:53:39.599-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:39.599-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:53:39.599-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,116286 s
[2023-07-13T22:53:39.600-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileFormatWriter: Start to commit write Job 4cb21de4-a4ae-49c0-b39c-d1f7d1b33aa2.
[2023-07-13T22:53:39.610-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileFormatWriter: Write Job 4cb21de4-a4ae-49c0-b39c-d1f7d1b33aa2 committed. Elapsed time: 9 ms.
[2023-07-13T22:53:39.610-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO FileFormatWriter: Finished processing stats for write job 4cb21de4-a4ae-49c0-b39c-d1f7d1b33aa2.
[2023-07-13T22:53:39.644-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:53:39.653-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:53:39.663-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:53:39.670-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:53:39.670-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO BlockManager: BlockManager stopped
[2023-07-13T22:53:39.673-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:53:39.675-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:53:39.678-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:53:39.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:53:39.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-17e85305-e737-40b7-8d26-c18bf4553380
[2023-07-13T22:53:39.681-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bea6080-61aa-4ba4-adbd-020ce0859000/pyspark-1bb16396-ba05-41ba-8ec6-8ef7eb4cfb7e
[2023-07-13T22:53:39.683-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bea6080-61aa-4ba4-adbd-020ce0859000
[2023-07-13T22:53:39.731-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T015332, end_date=20230714T015339
[2023-07-13T22:53:39.760-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:53:39.765-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T15:27:23.506-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T15:27:23.533-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T15:27:23.533-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T15:27:23.581-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-14T15:27:23.594-0300] {standard_task_runner.py:57} INFO - Started process 15796 to run task
[2023-07-14T15:27:23.608-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '49', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp6_qbnouk']
[2023-07-14T15:27:23.611-0300] {standard_task_runner.py:85} INFO - Job 49: Subtask transform_twitter_datascience
[2023-07-14T15:27:23.727-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T15:27:23.819-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-14T15:27:23.824-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T15:27:23.826-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12 --dest /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-12 --process-date 2023-07-12
[2023-07-14T15:27:26.905-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:26 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T15:27:26.910-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T15:27:28.728-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:28 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T15:27:28.940-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T15:27:29.096-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO ResourceUtils: ==============================================================
[2023-07-14T15:27:29.097-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T15:27:29.097-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO ResourceUtils: ==============================================================
[2023-07-14T15:27:29.098-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T15:27:29.142-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T15:27:29.164-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T15:27:29.168-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T15:27:29.258-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T15:27:29.259-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T15:27:29.260-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T15:27:29.260-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T15:27:29.261-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T15:27:29.851-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO Utils: Successfully started service 'sparkDriver' on port 37395.
[2023-07-14T15:27:29.924-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:29 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T15:27:30.007-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T15:27:30.042-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T15:27:30.043-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T15:27:30.050-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T15:27:30.095-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-36ec1d7b-03be-427e-a7a3-2732ccf7d0b3
[2023-07-14T15:27:30.128-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T15:27:30.158-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T15:27:30.496-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T15:27:30.674-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T15:27:30.689-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T15:27:30.725-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37951.
[2023-07-14T15:27:30.726-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO NettyBlockTransferService: Server created on 192.168.0.102:37951
[2023-07-14T15:27:30.728-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T15:27:30.743-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 37951, None)
[2023-07-14T15:27:30.751-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:37951 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 37951, None)
[2023-07-14T15:27:30.757-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 37951, None)
[2023-07-14T15:27:30.759-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 37951, None)
[2023-07-14T15:27:31.539-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T15:27:31.547-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:31 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T15:27:33.112-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:33 INFO InMemoryFileIndex: It took 91 ms to list leaf files for 1 paths.
[2023-07-14T15:27:33.226-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:33 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2023-07-14T15:27:36.422-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:36 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:27:36.424-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:36 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:27:36.427-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T15:27:36.924-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T15:27:37.015-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T15:27:37.023-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:37951 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:27:37.028-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:37.044-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203217 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:27:37.307-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:37.338-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:27:37.339-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:27:37.340-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:27:37.342-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:27:37.347-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:27:37.547-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T15:27:37.554-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T15:27:37.556-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:37951 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:27:37.557-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:27:37.576-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:27:37.577-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T15:27:37.662-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5007 bytes) taskResourceAssignments Map()
[2023-07-14T15:27:37.685-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T15:27:37.910-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:37 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8913, partition values: [empty row]
[2023-07-14T15:27:38.171-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO CodeGenerator: Code generated in 204.185014 ms
[2023-07-14T15:27:38.273-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T15:27:38.288-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 642 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:27:38.290-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T15:27:38.298-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,919 s
[2023-07-14T15:27:38.306-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:27:38.307-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T15:27:38.309-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,002347 s
[2023-07-14T15:27:38.630-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:37951 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:27:38.658-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:37951 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:27:39.161-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T15:27:39.164-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T15:27:39.166-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T15:27:39.330-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:27:39.330-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:27:39.335-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:27:39.656-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO CodeGenerator: Code generated in 146.262961 ms
[2023-07-14T15:27:39.662-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T15:27:39.681-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T15:27:39.682-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:37951 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T15:27:39.687-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:39.691-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203217 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:27:39.788-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:39.790-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:27:39.790-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:27:39.790-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:27:39.790-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:27:39.792-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:27:39.840-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T15:27:39.845-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T15:27:39.846-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:37951 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T15:27:39.847-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:27:39.848-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:27:39.848-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T15:27:39.857-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:27:39.858-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T15:27:39.975-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:27:39.976-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:27:39.977-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:27:40.028-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8913, partition values: [empty row]
[2023-07-14T15:27:40.063-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO CodeGenerator: Code generated in 29.307269 ms
[2023-07-14T15:27:40.102-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO CodeGenerator: Code generated in 8.467973 ms
[2023-07-14T15:27:40.157-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_20230714152739150657396377494375_0001_m_000000_1' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-12/tweet/process_date=2023-07-12/_temporary/0/task_20230714152739150657396377494375_0001_m_000000
[2023-07-14T15:27:40.158-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SparkHadoopMapRedUtil: attempt_20230714152739150657396377494375_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T15:27:40.168-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-14T15:27:40.172-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 319 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:27:40.172-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T15:27:40.174-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,380 s
[2023-07-14T15:27:40.174-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:27:40.174-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T15:27:40.175-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,386718 s
[2023-07-14T15:27:40.178-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileFormatWriter: Start to commit write Job e7c8e9fb-0dfc-4c8e-b79b-957be7a10c23.
[2023-07-14T15:27:40.199-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileFormatWriter: Write Job e7c8e9fb-0dfc-4c8e-b79b-957be7a10c23 committed. Elapsed time: 19 ms.
[2023-07-14T15:27:40.206-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileFormatWriter: Finished processing stats for write job e7c8e9fb-0dfc-4c8e-b79b-957be7a10c23.
[2023-07-14T15:27:40.264-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:27:40.265-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:27:40.265-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T15:27:40.282-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:27:40.283-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:27:40.283-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:27:40.331-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO CodeGenerator: Code generated in 19.076779 ms
[2023-07-14T15:27:40.341-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T15:27:40.355-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T15:27:40.356-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:37951 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T15:27:40.357-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:40.358-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203217 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:27:40.390-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:40.391-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:27:40.392-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:27:40.392-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:27:40.392-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:27:40.393-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:27:40.420-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T15:27:40.423-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T15:27:40.424-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:37951 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T15:27:40.425-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:27:40.425-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:27:40.425-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T15:27:40.427-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:27:40.427-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T15:27:40.455-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:27:40.456-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:27:40.456-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:27:40.481-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8913, partition values: [empty row]
[2023-07-14T15:27:40.508-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO CodeGenerator: Code generated in 20.973871 ms
[2023-07-14T15:27:40.524-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_202307141527402530322075241520717_0002_m_000000_2' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-12/user/process_date=2023-07-12/_temporary/0/task_202307141527402530322075241520717_0002_m_000000
[2023-07-14T15:27:40.525-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SparkHadoopMapRedUtil: attempt_202307141527402530322075241520717_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-14T15:27:40.526-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T15:27:40.528-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 102 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:27:40.529-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T15:27:40.531-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,136 s
[2023-07-14T15:27:40.531-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:27:40.531-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T15:27:40.532-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,141183 s
[2023-07-14T15:27:40.533-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileFormatWriter: Start to commit write Job 8198557c-b4e9-4f7e-aa8c-6c6449fb6f6a.
[2023-07-14T15:27:40.549-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileFormatWriter: Write Job 8198557c-b4e9-4f7e-aa8c-6c6449fb6f6a committed. Elapsed time: 15 ms.
[2023-07-14T15:27:40.551-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO FileFormatWriter: Finished processing stats for write job 8198557c-b4e9-4f7e-aa8c-6c6449fb6f6a.
[2023-07-14T15:27:40.612-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T15:27:40.632-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T15:27:40.655-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T15:27:40.665-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO MemoryStore: MemoryStore cleared
[2023-07-14T15:27:40.667-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO BlockManager: BlockManager stopped
[2023-07-14T15:27:40.675-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T15:27:40.677-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T15:27:40.685-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T15:27:40.689-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T15:27:40.691-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-66f625bd-66c6-49bf-a1ab-f6b7d599a387
[2023-07-14T15:27:40.694-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0298416-6365-48e2-9f95-e4e60fe65318
[2023-07-14T15:27:40.699-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0298416-6365-48e2-9f95-e4e60fe65318/pyspark-cc39b489-b535-4782-a1af-b49e701a457f
[2023-07-14T15:27:40.788-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T182723, end_date=20230714T182740
[2023-07-14T15:27:40.830-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T15:27:40.860-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:12:07.287-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T16:12:07.293-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T16:12:07.293-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:12:07.305-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-14T16:12:07.307-0300] {standard_task_runner.py:57} INFO - Started process 24556 to run task
[2023-07-14T16:12:07.309-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '52', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpzixj_98f']
[2023-07-14T16:12:07.310-0300] {standard_task_runner.py:85} INFO - Job 52: Subtask transform_twitter_datascience
[2023-07-14T16:12:07.336-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:12:07.403-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-14T16:12:07.408-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T16:12:07.410-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-12 --process-date 2023-07-12
[2023-07-14T16:12:08.762-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:08 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T16:12:08.765-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T16:12:10.048-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T16:12:10.096-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T16:12:10.168-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO ResourceUtils: ==============================================================
[2023-07-14T16:12:10.168-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T16:12:10.169-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO ResourceUtils: ==============================================================
[2023-07-14T16:12:10.169-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T16:12:10.191-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T16:12:10.204-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T16:12:10.206-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T16:12:10.291-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T16:12:10.293-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T16:12:10.294-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T16:12:10.295-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T16:12:10.297-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T16:12:10.611-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO Utils: Successfully started service 'sparkDriver' on port 36291.
[2023-07-14T16:12:10.646-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T16:12:10.685-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T16:12:10.702-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T16:12:10.703-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T16:12:10.708-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T16:12:10.728-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-afac0df7-9bbd-4fa8-9304-b2b2310bd24b
[2023-07-14T16:12:10.742-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T16:12:10.761-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:10 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T16:12:11.077-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T16:12:11.178-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T16:12:11.193-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T16:12:11.227-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40995.
[2023-07-14T16:12:11.227-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO NettyBlockTransferService: Server created on 192.168.0.102:40995
[2023-07-14T16:12:11.229-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T16:12:11.239-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 40995, None)
[2023-07-14T16:12:11.243-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:40995 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 40995, None)
[2023-07-14T16:12:11.247-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 40995, None)
[2023-07-14T16:12:11.250-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 40995, None)
[2023-07-14T16:12:11.745-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T16:12:11.750-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:11 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T16:12:12.828-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:12 INFO InMemoryFileIndex: It took 104 ms to list leaf files for 1 paths.
[2023-07-14T16:12:12.963-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:12 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-07-14T16:12:16.903-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:16 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:12:16.904-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:16 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:12:16.907-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T16:12:17.231-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T16:12:17.283-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T16:12:17.285-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:40995 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:17.289-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:17.297-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198757 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:12:17.459-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:17.474-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:12:17.475-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:12:17.475-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:12:17.476-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:12:17.479-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:12:17.578-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T16:12:17.580-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T16:12:17.581-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:40995 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:17.582-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:12:17.592-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:12:17.593-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T16:12:17.634-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T16:12:17.646-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T16:12:17.763-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4453, partition values: [empty row]
[2023-07-14T16:12:17.967-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:17 INFO CodeGenerator: Code generated in 167.185266 ms
[2023-07-14T16:12:18.017-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-07-14T16:12:18.024-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 397 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:18.025-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T16:12:18.032-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,535 s
[2023-07-14T16:12:18.035-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:18.035-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T16:12:18.037-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,578420 s
[2023-07-14T16:12:18.420-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T16:12:18.421-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T16:12:18.421-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T16:12:18.482-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:18.482-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:18.483-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:18.528-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:40995 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:18.534-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:40995 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:18.661-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO CodeGenerator: Code generated in 77.032949 ms
[2023-07-14T16:12:18.666-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T16:12:18.675-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T16:12:18.675-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:40995 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:12:18.676-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:18.679-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198757 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:12:18.741-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:18.743-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:12:18.743-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:12:18.743-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:12:18.743-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:12:18.744-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:12:18.767-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T16:12:18.770-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T16:12:18.771-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:40995 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T16:12:18.771-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:12:18.772-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:12:18.772-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T16:12:18.776-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:12:18.777-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T16:12:18.824-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:18.824-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:18.825-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:18.863-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4453, partition values: [empty row]
[2023-07-14T16:12:18.886-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO CodeGenerator: Code generated in 18.038789 ms
[2023-07-14T16:12:18.903-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO CodeGenerator: Code generated in 4.011087 ms
[2023-07-14T16:12:18.930-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileOutputCommitter: Saved output of task 'attempt_202307141612188357239157412328129_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-12/tweet/process_date=2023-07-12/_temporary/0/task_202307141612188357239157412328129_0001_m_000000
[2023-07-14T16:12:18.931-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO SparkHadoopMapRedUtil: attempt_202307141612188357239157412328129_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T16:12:18.940-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T16:12:18.942-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 167 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:18.942-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T16:12:18.943-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,197 s
[2023-07-14T16:12:18.943-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:18.943-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T16:12:18.944-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,202716 s
[2023-07-14T16:12:18.945-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileFormatWriter: Start to commit write Job 08eca195-7cb1-4d54-bb14-199de107fe61.
[2023-07-14T16:12:18.966-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileFormatWriter: Write Job 08eca195-7cb1-4d54-bb14-199de107fe61 committed. Elapsed time: 19 ms.
[2023-07-14T16:12:18.976-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:18 INFO FileFormatWriter: Finished processing stats for write job 08eca195-7cb1-4d54-bb14-199de107fe61.
[2023-07-14T16:12:19.049-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:12:19.051-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:12:19.051-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T16:12:19.069-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:19.069-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:19.071-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:19.127-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO CodeGenerator: Code generated in 25.750087 ms
[2023-07-14T16:12:19.133-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T16:12:19.156-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T16:12:19.157-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:40995 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T16:12:19.158-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:19.159-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198757 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:12:19.190-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:19.192-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:12:19.192-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:12:19.192-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:12:19.192-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:12:19.193-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:12:19.216-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T16:12:19.224-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T16:12:19.225-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:40995 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T16:12:19.226-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:12:19.227-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:12:19.227-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T16:12:19.229-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:12:19.232-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T16:12:19.263-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:19.265-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:19.266-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:19.283-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4453, partition values: [empty row]
[2023-07-14T16:12:19.294-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO CodeGenerator: Code generated in 9.253614 ms
[2023-07-14T16:12:19.301-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileOutputCommitter: Saved output of task 'attempt_202307141612199194638729105429416_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-12/user/process_date=2023-07-12/_temporary/0/task_202307141612199194638729105429416_0002_m_000000
[2023-07-14T16:12:19.301-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SparkHadoopMapRedUtil: attempt_202307141612199194638729105429416_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-14T16:12:19.302-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T16:12:19.304-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 75 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:19.305-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T16:12:19.305-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,110 s
[2023-07-14T16:12:19.305-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:19.305-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T16:12:19.305-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,115762 s
[2023-07-14T16:12:19.306-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileFormatWriter: Start to commit write Job 561e73d0-eb5c-404b-8201-5fd718c77a69.
[2023-07-14T16:12:19.315-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileFormatWriter: Write Job 561e73d0-eb5c-404b-8201-5fd718c77a69 committed. Elapsed time: 8 ms.
[2023-07-14T16:12:19.315-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO FileFormatWriter: Finished processing stats for write job 561e73d0-eb5c-404b-8201-5fd718c77a69.
[2023-07-14T16:12:19.339-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T16:12:19.352-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T16:12:19.361-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T16:12:19.375-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO MemoryStore: MemoryStore cleared
[2023-07-14T16:12:19.375-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO BlockManager: BlockManager stopped
[2023-07-14T16:12:19.378-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T16:12:19.382-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T16:12:19.406-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T16:12:19.406-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T16:12:19.407-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-42f37429-4d8b-4a8f-897c-b59e21beb3bf
[2023-07-14T16:12:19.409-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-42f37429-4d8b-4a8f-897c-b59e21beb3bf/pyspark-e3450d12-afd1-4fd9-b122-65a607f4bbc3
[2023-07-14T16:12:19.410-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-f721e70c-459a-40ea-b105-2f45d6abc1e5
[2023-07-14T16:12:19.461-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T191207, end_date=20230714T191219
[2023-07-14T16:12:19.476-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:12:19.487-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:25:26.863-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T17:25:26.877-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T17:25:26.878-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:25:26.899-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-14T17:25:26.903-0300] {standard_task_runner.py:57} INFO - Started process 33162 to run task
[2023-07-14T17:25:26.913-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '55', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp72oos7k6']
[2023-07-14T17:25:26.914-0300] {standard_task_runner.py:85} INFO - Job 55: Subtask transform_twitter_datascience
[2023-07-14T17:25:26.973-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:25:27.062-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-14T17:25:27.067-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T17:25:27.069-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-12
[2023-07-14T17:25:33.019-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:33 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T17:25:33.026-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T17:25:35.018-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T17:25:35.239-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T17:25:35.500-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO ResourceUtils: ==============================================================
[2023-07-14T17:25:35.500-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T17:25:35.501-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO ResourceUtils: ==============================================================
[2023-07-14T17:25:35.501-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T17:25:35.554-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T17:25:35.592-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T17:25:35.593-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T17:25:35.697-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T17:25:35.698-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T17:25:35.699-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T17:25:35.699-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T17:25:35.700-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T17:25:36.196-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO Utils: Successfully started service 'sparkDriver' on port 36637.
[2023-07-14T17:25:36.250-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T17:25:36.351-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T17:25:36.388-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T17:25:36.390-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T17:25:36.404-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T17:25:36.491-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d0316a26-9d0a-42e3-8d03-40363e8ce8f0
[2023-07-14T17:25:36.537-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T17:25:36.575-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:36 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T17:25:37.101-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T17:25:37.119-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T17:25:37.436-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T17:25:37.482-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T17:25:37.583-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39161.
[2023-07-14T17:25:37.585-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO NettyBlockTransferService: Server created on 192.168.0.102:39161
[2023-07-14T17:25:37.592-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T17:25:37.624-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 39161, None)
[2023-07-14T17:25:37.643-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:39161 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 39161, None)
[2023-07-14T17:25:37.656-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 39161, None)
[2023-07-14T17:25:37.662-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 39161, None)
[2023-07-14T17:25:39.149-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T17:25:39.167-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:39 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T17:25:41.376-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:41 INFO InMemoryFileIndex: It took 89 ms to list leaf files for 1 paths.
[2023-07-14T17:25:41.500-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:41 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2023-07-14T17:25:45.594-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:45 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:25:45.600-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:45 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:25:45.606-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:45 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T17:25:46.269-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T17:25:46.356-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T17:25:46.363-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:39161 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:25:46.368-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:25:46.390-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203392 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:25:46.629-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:25:46.653-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:25:46.654-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:25:46.654-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:25:46.656-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:25:46.664-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:25:46.881-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T17:25:46.885-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T17:25:46.888-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:39161 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:25:46.891-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:25:46.921-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:25:46.926-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T17:25:47.067-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T17:25:47.110-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T17:25:47.388-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9088, partition values: [empty row]
[2023-07-14T17:25:47.629-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO CodeGenerator: Code generated in 192.010799 ms
[2023-07-14T17:25:47.718-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T17:25:47.733-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 700 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:25:47.736-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T17:25:47.758-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,072 s
[2023-07-14T17:25:47.765-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:25:47.768-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T17:25:47.775-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:47 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,145202 s
[2023-07-14T17:25:48.482-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:39161 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:25:48.528-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:39161 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:25:49.017-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T17:25:49.027-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:49 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T17:25:49.034-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:49 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T17:25:49.312-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:25:49.313-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:25:49.322-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:25:50.251-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO CodeGenerator: Code generated in 380.245968 ms
[2023-07-14T17:25:50.268-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T17:25:50.294-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T17:25:50.305-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:39161 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T17:25:50.312-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:25:50.328-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203392 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:25:50.578-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:25:50.593-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:25:50.594-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:25:50.596-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:25:50.597-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:25:50.615-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:25:50.716-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T17:25:50.734-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T17:25:50.739-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:39161 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T17:25:50.739-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:25:50.742-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:25:50.744-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T17:25:50.763-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:25:50.772-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T17:25:50.956-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:25:50.958-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:25:50.960-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:25:51.091-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9088, partition values: [empty row]
[2023-07-14T17:25:51.179-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO CodeGenerator: Code generated in 76.54442 ms
[2023-07-14T17:25:51.277-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO CodeGenerator: Code generated in 22.911967 ms
[2023-07-14T17:25:51.472-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileOutputCommitter: Saved output of task 'attempt_202307141725504644544845133417981_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-12/_temporary/0/task_202307141725504644544845133417981_0001_m_000000
[2023-07-14T17:25:51.480-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO SparkHadoopMapRedUtil: attempt_202307141725504644544845133417981_0001_m_000000_1: Committed. Elapsed time: 3 ms.
[2023-07-14T17:25:51.500-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-14T17:25:51.511-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 753 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:25:51.512-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T17:25:51.515-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,898 s
[2023-07-14T17:25:51.515-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:25:51.516-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T17:25:51.518-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,937609 s
[2023-07-14T17:25:51.523-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileFormatWriter: Start to commit write Job 256cc0a9-56db-44a3-ba7b-2939662bdcfa.
[2023-07-14T17:25:51.568-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileFormatWriter: Write Job 256cc0a9-56db-44a3-ba7b-2939662bdcfa committed. Elapsed time: 43 ms.
[2023-07-14T17:25:51.579-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileFormatWriter: Finished processing stats for write job 256cc0a9-56db-44a3-ba7b-2939662bdcfa.
[2023-07-14T17:25:51.666-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:25:51.667-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:25:51.667-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T17:25:51.697-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:25:51.697-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:25:51.698-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:25:51.750-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO CodeGenerator: Code generated in 19.860956 ms
[2023-07-14T17:25:51.761-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T17:25:51.771-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T17:25:51.772-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:39161 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:25:51.775-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:25:51.780-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203392 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:25:51.810-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:25:51.812-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:25:51.812-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:25:51.812-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:25:51.812-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:25:51.813-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:25:51.832-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.8 KiB, free 433.4 MiB)
[2023-07-14T17:25:51.834-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T17:25:51.836-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:39161 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T17:25:51.837-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:25:51.837-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:25:51.838-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T17:25:51.842-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:25:51.845-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T17:25:51.864-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:25:51.864-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:25:51.865-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:25:51.886-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9088, partition values: [empty row]
[2023-07-14T17:25:51.910-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO CodeGenerator: Code generated in 17.650249 ms
[2023-07-14T17:25:51.928-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileOutputCommitter: Saved output of task 'attempt_202307141725517792194219418350260_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-12/_temporary/0/task_202307141725517792194219418350260_0002_m_000000
[2023-07-14T17:25:51.928-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO SparkHadoopMapRedUtil: attempt_202307141725517792194219418350260_0002_m_000000_2: Committed. Elapsed time: 3 ms.
[2023-07-14T17:25:51.930-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T17:25:51.932-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 93 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:25:51.933-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T17:25:51.933-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,119 s
[2023-07-14T17:25:51.934-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:25:51.934-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T17:25:51.934-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,123416 s
[2023-07-14T17:25:51.936-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileFormatWriter: Start to commit write Job 4fdf7ba1-d779-406a-98d0-721c856158a6.
[2023-07-14T17:25:51.950-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileFormatWriter: Write Job 4fdf7ba1-d779-406a-98d0-721c856158a6 committed. Elapsed time: 14 ms.
[2023-07-14T17:25:51.951-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:51 INFO FileFormatWriter: Finished processing stats for write job 4fdf7ba1-d779-406a-98d0-721c856158a6.
[2023-07-14T17:25:52.017-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T17:25:52.034-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T17:25:52.051-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T17:25:52.064-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO MemoryStore: MemoryStore cleared
[2023-07-14T17:25:52.064-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO BlockManager: BlockManager stopped
[2023-07-14T17:25:52.067-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T17:25:52.071-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T17:25:52.078-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T17:25:52.078-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T17:25:52.079-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-dba91453-2616-4be3-bcde-c4da440da241
[2023-07-14T17:25:52.081-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d2ed7fb-a832-478a-9b0d-b85b0bcfe96d
[2023-07-14T17:25:52.082-0300] {spark_submit.py:492} INFO - 23/07/14 17:25:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-dba91453-2616-4be3-bcde-c4da440da241/pyspark-61799eab-a311-41dd-ad59-c0c0fefe4f76
[2023-07-14T17:25:52.144-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T202526, end_date=20230714T202552
[2023-07-14T17:25:52.174-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:25:52.189-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:30:58.866-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T17:30:58.874-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T17:30:58.874-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:30:58.897-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-14T17:30:58.902-0300] {standard_task_runner.py:57} INFO - Started process 36693 to run task
[2023-07-14T17:30:58.907-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '62', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpys27f359']
[2023-07-14T17:30:58.909-0300] {standard_task_runner.py:85} INFO - Job 62: Subtask transform_twitter_datascience
[2023-07-14T17:30:58.967-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:30:59.046-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-14T17:30:59.052-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T17:30:59.053-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-12
[2023-07-14T17:31:01.493-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:01 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T17:31:01.500-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T17:31:02.955-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:02 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T17:31:03.104-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T17:31:03.307-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO ResourceUtils: ==============================================================
[2023-07-14T17:31:03.307-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T17:31:03.307-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO ResourceUtils: ==============================================================
[2023-07-14T17:31:03.308-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T17:31:03.339-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T17:31:03.355-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T17:31:03.355-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T17:31:03.421-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T17:31:03.421-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T17:31:03.421-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T17:31:03.422-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T17:31:03.422-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T17:31:03.756-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO Utils: Successfully started service 'sparkDriver' on port 39643.
[2023-07-14T17:31:03.806-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T17:31:03.866-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T17:31:03.898-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T17:31:03.899-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T17:31:03.904-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T17:31:03.939-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-97cb6e59-bf25-4124-a6fe-40b8f7f925ff
[2023-07-14T17:31:03.973-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T17:31:04.002-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T17:31:04.317-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T17:31:04.336-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T17:31:04.642-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T17:31:04.654-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T17:31:04.684-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39009.
[2023-07-14T17:31:04.684-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO NettyBlockTransferService: Server created on 192.168.0.102:39009
[2023-07-14T17:31:04.685-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T17:31:04.693-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 39009, None)
[2023-07-14T17:31:04.702-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:39009 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 39009, None)
[2023-07-14T17:31:04.704-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 39009, None)
[2023-07-14T17:31:04.706-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 39009, None)
[2023-07-14T17:31:05.315-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T17:31:05.322-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:05 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T17:31:06.439-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:06 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
[2023-07-14T17:31:06.535-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:06 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
[2023-07-14T17:31:09.052-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:31:09.053-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:31:09.056-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T17:31:09.419-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T17:31:09.486-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T17:31:09.488-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:39009 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:31:09.494-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:09.506-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203308 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:31:09.771-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:09.791-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:31:09.792-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:31:09.793-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:31:09.794-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:31:09.802-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:31:09.968-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T17:31:09.970-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T17:31:09.971-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:39009 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:31:09.971-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:31:09.986-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:31:09.987-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T17:31:10.050-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T17:31:10.068-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T17:31:10.201-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9004, partition values: [empty row]
[2023-07-14T17:31:10.433-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO CodeGenerator: Code generated in 181.587331 ms
[2023-07-14T17:31:10.502-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T17:31:10.511-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 473 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:31:10.517-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T17:31:10.523-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,700 s
[2023-07-14T17:31:10.529-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:31:10.532-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T17:31:10.539-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:10 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,767580 s
[2023-07-14T17:31:11.112-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:39009 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:31:11.128-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:39009 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:31:11.668-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T17:31:11.670-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:11 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T17:31:11.672-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:11 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T17:31:11.838-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:31:11.839-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:31:11.841-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:31:12.075-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO CodeGenerator: Code generated in 129.315566 ms
[2023-07-14T17:31:12.084-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T17:31:12.098-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T17:31:12.100-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:39009 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T17:31:12.101-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:12.104-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203308 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:31:12.207-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:12.209-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:31:12.210-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:31:12.210-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:31:12.210-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:31:12.215-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:31:12.249-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T17:31:12.253-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T17:31:12.254-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:39009 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T17:31:12.254-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:31:12.255-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:31:12.255-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T17:31:12.260-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:31:12.261-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T17:31:12.334-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:31:12.334-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:31:12.335-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:31:12.379-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9004, partition values: [empty row]
[2023-07-14T17:31:12.426-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO CodeGenerator: Code generated in 38.208098 ms
[2023-07-14T17:31:12.465-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO CodeGenerator: Code generated in 9.272791 ms
[2023-07-14T17:31:12.516-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileOutputCommitter: Saved output of task 'attempt_202307141731127104674341189317383_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-12/_temporary/0/task_202307141731127104674341189317383_0001_m_000000
[2023-07-14T17:31:12.517-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SparkHadoopMapRedUtil: attempt_202307141731127104674341189317383_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T17:31:12.522-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T17:31:12.524-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 268 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:31:12.524-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T17:31:12.526-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,308 s
[2023-07-14T17:31:12.526-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:31:12.527-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T17:31:12.528-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,319784 s
[2023-07-14T17:31:12.533-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileFormatWriter: Start to commit write Job 701c31a8-e0c8-47e6-908b-f6923a218421.
[2023-07-14T17:31:12.557-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileFormatWriter: Write Job 701c31a8-e0c8-47e6-908b-f6923a218421 committed. Elapsed time: 19 ms.
[2023-07-14T17:31:12.563-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileFormatWriter: Finished processing stats for write job 701c31a8-e0c8-47e6-908b-f6923a218421.
[2023-07-14T17:31:12.598-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:39009 in memory (size: 82.1 KiB, free: 434.4 MiB)
[2023-07-14T17:31:12.657-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:31:12.658-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:31:12.658-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T17:31:12.676-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:31:12.676-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:31:12.677-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:31:12.724-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO CodeGenerator: Code generated in 21.914742 ms
[2023-07-14T17:31:12.732-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-14T17:31:12.752-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-14T17:31:12.753-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:39009 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:31:12.755-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:12.756-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203308 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:31:12.788-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:12.789-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:31:12.789-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:31:12.789-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:31:12.789-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:31:12.791-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:31:12.824-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.7 MiB)
[2023-07-14T17:31:12.829-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.7 MiB)
[2023-07-14T17:31:12.833-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:39009 (size: 76.9 KiB, free: 434.3 MiB)
[2023-07-14T17:31:12.836-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:31:12.836-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:31:12.836-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T17:31:12.838-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:31:12.838-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T17:31:12.859-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:31:12.860-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:31:12.860-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:31:12.881-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9004, partition values: [empty row]
[2023-07-14T17:31:12.906-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO CodeGenerator: Code generated in 17.232618 ms
[2023-07-14T17:31:12.920-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileOutputCommitter: Saved output of task 'attempt_20230714173112179504676489883841_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-12/_temporary/0/task_20230714173112179504676489883841_0002_m_000000
[2023-07-14T17:31:12.920-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO SparkHadoopMapRedUtil: attempt_20230714173112179504676489883841_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-14T17:31:12.921-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2622 bytes result sent to driver
[2023-07-14T17:31:12.923-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 85 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:31:12.923-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T17:31:12.924-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,130 s
[2023-07-14T17:31:12.924-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:31:12.925-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T17:31:12.926-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,137828 s
[2023-07-14T17:31:12.927-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileFormatWriter: Start to commit write Job 49123f85-3a5f-49b3-8bb7-5b909aa84c87.
[2023-07-14T17:31:12.941-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileFormatWriter: Write Job 49123f85-3a5f-49b3-8bb7-5b909aa84c87 committed. Elapsed time: 13 ms.
[2023-07-14T17:31:12.941-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:12 INFO FileFormatWriter: Finished processing stats for write job 49123f85-3a5f-49b3-8bb7-5b909aa84c87.
[2023-07-14T17:31:13.011-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T17:31:13.026-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T17:31:13.046-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T17:31:13.066-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO MemoryStore: MemoryStore cleared
[2023-07-14T17:31:13.067-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO BlockManager: BlockManager stopped
[2023-07-14T17:31:13.070-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T17:31:13.072-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T17:31:13.077-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T17:31:13.078-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T17:31:13.081-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3ab933f-667a-4eda-a35d-77d9318f5296/pyspark-1b5120a1-06d8-41e1-be1e-1049b7c24a6a
[2023-07-14T17:31:13.085-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3ab933f-667a-4eda-a35d-77d9318f5296
[2023-07-14T17:31:13.087-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-c3ceda53-d7c8-43ad-b542-c9f7be58bb58
[2023-07-14T17:31:13.150-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230714T203058, end_date=20230714T203113
[2023-07-14T17:31:13.205-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:31:13.227-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:44:49.983-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T23:44:49.991-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-14T23:44:49.991-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:44:50.006-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-14T23:44:50.010-0300] {standard_task_runner.py:57} INFO - Started process 25715 to run task
[2023-07-14T23:44:50.016-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '80', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpu33djdfg']
[2023-07-14T23:44:50.018-0300] {standard_task_runner.py:85} INFO - Job 80: Subtask transform_twitter_datascience
[2023-07-14T23:44:50.058-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:44:50.136-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-14T23:44:50.141-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T23:44:50.142-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-12
[2023-07-14T23:44:51.872-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:51 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-14T23:44:51.873-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T23:44:51.885-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-14T23:44:51.885-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-14T23:44:51.885-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-14T23:44:51.885-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-14T23:44:51.886-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-14T23:44:52.263-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T23:44:53.004-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-14T23:44:53.016-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SparkContext: Running Spark version 3.1.3
[2023-07-14T23:44:53.110-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO ResourceUtils: ==============================================================
[2023-07-14T23:44:53.110-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T23:44:53.111-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO ResourceUtils: ==============================================================
[2023-07-14T23:44:53.112-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T23:44:53.162-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T23:44:53.196-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T23:44:53.197-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T23:44:53.272-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T23:44:53.272-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T23:44:53.272-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T23:44:53.272-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T23:44:53.272-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T23:44:53.576-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO Utils: Successfully started service 'sparkDriver' on port 41073.
[2023-07-14T23:44:53.608-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T23:44:53.644-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T23:44:53.666-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T23:44:53.668-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T23:44:53.672-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T23:44:53.685-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c5c5fdee-9b58-41fb-8e3f-d9ae1d03563c
[2023-07-14T23:44:53.707-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T23:44:53.724-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T23:44:53.910-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T23:44:53.929-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T23:44:53.988-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4041
[2023-07-14T23:44:54.174-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-14T23:44:54.199-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45005.
[2023-07-14T23:44:54.200-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO NettyBlockTransferService: Server created on 192.168.0.177:45005
[2023-07-14T23:44:54.203-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T23:44:54.208-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 45005, None)
[2023-07-14T23:44:54.211-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:45005 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 45005, None)
[2023-07-14T23:44:54.213-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 45005, None)
[2023-07-14T23:44:54.215-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 45005, None)
[2023-07-14T23:44:54.646-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-14T23:44:54.647-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:54 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T23:44:55.463-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:55 INFO InMemoryFileIndex: It took 52 ms to list leaf files for 1 paths.
[2023-07-14T23:44:55.539-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:55 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
[2023-07-14T23:44:57.432-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:44:57.434-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:44:57.438-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T23:44:57.686-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-14T23:44:57.733-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-14T23:44:57.737-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:45005 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-14T23:44:57.741-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:57.748-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198798 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:44:57.926-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:57.942-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:44:57.943-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:44:57.943-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:44:57.944-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:44:57.948-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:44:58.039-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-14T23:44:58.042-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-14T23:44:58.042-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:45005 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:44:58.043-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:44:58.055-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:44:58.056-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T23:44:58.104-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-14T23:44:58.116-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T23:44:58.230-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4494, partition values: [empty row]
[2023-07-14T23:44:58.492-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO CodeGenerator: Code generated in 157.446849 ms
[2023-07-14T23:44:58.539-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2023-07-14T23:44:58.548-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 454 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:44:58.552-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T23:44:58.558-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,597 s
[2023-07-14T23:44:58.562-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:44:58.562-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T23:44:58.564-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,638020 s
[2023-07-14T23:44:58.759-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:45005 in memory (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-14T23:44:58.776-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:45005 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:44:59.101-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T23:44:59.103-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-14T23:44:59.104-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T23:44:59.173-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:59.173-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:59.175-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:59.269-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO CodeGenerator: Code generated in 39.253985 ms
[2023-07-14T23:44:59.312-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO CodeGenerator: Code generated in 28.563314 ms
[2023-07-14T23:44:59.319-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.2 MiB)
[2023-07-14T23:44:59.329-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-14T23:44:59.330-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:45005 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-14T23:44:59.331-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:59.334-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198798 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:44:59.405-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:59.407-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:44:59.408-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:44:59.408-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:44:59.408-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:44:59.410-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:44:59.459-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 434.0 MiB)
[2023-07-14T23:44:59.463-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 434.0 MiB)
[2023-07-14T23:44:59.464-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:45005 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-14T23:44:59.464-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:44:59.465-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:44:59.465-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T23:44:59.471-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:44:59.472-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T23:44:59.544-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:59.544-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:59.545-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:59.642-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO CodeGenerator: Code generated in 52.941084 ms
[2023-07-14T23:44:59.646-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4494, partition values: [empty row]
[2023-07-14T23:44:59.690-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO CodeGenerator: Code generated in 38.266311 ms
[2023-07-14T23:44:59.724-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO CodeGenerator: Code generated in 9.021766 ms
[2023-07-14T23:44:59.773-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_202307142344597379923741638007730_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-12/_temporary/0/task_202307142344597379923741638007730_0001_m_000000
[2023-07-14T23:44:59.774-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO SparkHadoopMapRedUtil: attempt_202307142344597379923741638007730_0001_m_000000_1: Committed
[2023-07-14T23:44:59.780-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-14T23:44:59.783-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 317 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:44:59.784-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T23:44:59.786-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,374 s
[2023-07-14T23:44:59.786-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:44:59.786-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T23:44:59.789-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,382661 s
[2023-07-14T23:44:59.816-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileFormatWriter: Write Job d6f0d1f9-4292-46b9-a596-786faadb1aa7 committed.
[2023-07-14T23:44:59.824-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileFormatWriter: Finished processing stats for write job d6f0d1f9-4292-46b9-a596-786faadb1aa7.
[2023-07-14T23:44:59.923-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:44:59.923-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:44:59.923-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T23:44:59.942-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:59.943-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:59.944-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:59.980-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO CodeGenerator: Code generated in 18.00061 ms
[2023-07-14T23:44:59.988-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-14T23:44:59.996-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.8 MiB)
[2023-07-14T23:44:59.997-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:45005 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-14T23:44:59.997-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:59.999-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198798 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:45:00.024-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:45:00.025-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:45:00.025-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:45:00.025-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:45:00.025-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:45:00.026-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:45:00.044-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.6 MiB)
[2023-07-14T23:45:00.046-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.5 MiB)
[2023-07-14T23:45:00.047-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:45005 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-14T23:45:00.047-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:45:00.048-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:45:00.049-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T23:45:00.051-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:45:00.052-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T23:45:00.069-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:45:00.070-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:45:00.070-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:45:00.097-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO CodeGenerator: Code generated in 9.94647 ms
[2023-07-14T23:45:00.100-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4494, partition values: [empty row]
[2023-07-14T23:45:00.117-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO CodeGenerator: Code generated in 13.142603 ms
[2023-07-14T23:45:00.126-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_202307142345002440350529639978341_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-12/_temporary/0/task_202307142345002440350529639978341_0002_m_000000
[2023-07-14T23:45:00.127-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO SparkHadoopMapRedUtil: attempt_202307142345002440350529639978341_0002_m_000000_2: Committed
[2023-07-14T23:45:00.128-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-14T23:45:00.130-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 79 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:45:00.130-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T23:45:00.131-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,104 s
[2023-07-14T23:45:00.131-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:45:00.131-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T23:45:00.132-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,107778 s
[2023-07-14T23:45:00.144-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO FileFormatWriter: Write Job 3b9e39e2-8b21-4e4e-85d9-3bf4aadb336f committed.
[2023-07-14T23:45:00.144-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO FileFormatWriter: Finished processing stats for write job 3b9e39e2-8b21-4e4e-85d9-3bf4aadb336f.
[2023-07-14T23:45:00.180-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T23:45:00.189-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4041
[2023-07-14T23:45:00.197-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T23:45:00.207-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO MemoryStore: MemoryStore cleared
[2023-07-14T23:45:00.207-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO BlockManager: BlockManager stopped
[2023-07-14T23:45:00.210-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T23:45:00.212-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T23:45:00.216-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T23:45:00.217-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T23:45:00.218-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-63a4ea3c-5ddd-4872-84c9-a2d809d43e15
[2023-07-14T23:45:00.222-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3515b20-a48d-40ff-bf8f-2d26c02f8f64
[2023-07-14T23:45:00.224-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-63a4ea3c-5ddd-4872-84c9-a2d809d43e15/pyspark-59b13603-4185-46cc-bf82-2867238365dd
[2023-07-14T23:45:00.279-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230715T024449, end_date=20230715T024500
[2023-07-14T23:45:00.310-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:45:00.328-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-15T12:14:46.915-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-15T12:14:46.931-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [queued]>
[2023-07-15T12:14:46.932-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-15T12:14:46.965-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-12 00:00:00+00:00
[2023-07-15T12:14:46.975-0300] {standard_task_runner.py:57} INFO - Started process 14679 to run task
[2023-07-15T12:14:46.982-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-12T00:00:00+00:00', '--job-id', '101', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp2ihi0b9o']
[2023-07-15T12:14:46.984-0300] {standard_task_runner.py:85} INFO - Job 101: Subtask transform_twitter_datascience
[2023-07-15T12:14:47.063-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-12T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-15T12:14:47.194-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-12T00:00:00+00:00'
[2023-07-15T12:14:47.200-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-15T12:14:47.202-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-12
[2023-07-15T12:14:50.235-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:50 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-15T12:14:50.237-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-15T12:14:50.333-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-15T12:14:50.334-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-15T12:14:50.334-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-15T12:14:50.334-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-15T12:14:50.335-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-15T12:14:51.296-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-15T12:14:52.597-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-15T12:14:52.637-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:52 INFO SparkContext: Running Spark version 3.1.3
[2023-07-15T12:14:52.825-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:52 INFO ResourceUtils: ==============================================================
[2023-07-15T12:14:52.828-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-15T12:14:52.831-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:52 INFO ResourceUtils: ==============================================================
[2023-07-15T12:14:52.832-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:52 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-15T12:14:52.893-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-15T12:14:52.925-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:52 INFO ResourceProfile: Limiting resource is cpu
[2023-07-15T12:14:52.930-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-15T12:14:53.058-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SecurityManager: Changing view acls to: lucas
[2023-07-15T12:14:53.059-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-15T12:14:53.060-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SecurityManager: Changing view acls groups to:
[2023-07-15T12:14:53.062-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SecurityManager: Changing modify acls groups to:
[2023-07-15T12:14:53.064-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-15T12:14:53.662-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO Utils: Successfully started service 'sparkDriver' on port 41639.
[2023-07-15T12:14:53.709-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SparkEnv: Registering MapOutputTracker
[2023-07-15T12:14:53.765-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-15T12:14:53.800-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-15T12:14:53.802-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-15T12:14:53.811-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-15T12:14:53.834-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b38b299a-d68f-401c-8e13-1b7f29e054fd
[2023-07-15T12:14:53.874-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-15T12:14:53.902-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-15T12:14:54.301-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-15T12:14:54.479-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[2023-07-15T12:14:54.988-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:54 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-15T12:14:55.054-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41095.
[2023-07-15T12:14:55.055-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:55 INFO NettyBlockTransferService: Server created on 192.168.0.102:41095
[2023-07-15T12:14:55.064-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-15T12:14:55.076-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 41095, None)
[2023-07-15T12:14:55.085-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:41095 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 41095, None)
[2023-07-15T12:14:55.092-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 41095, None)
[2023-07-15T12:14:55.098-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 41095, None)
[2023-07-15T12:14:56.126-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-15T12:14:56.127-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:56 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-15T12:14:57.986-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:57 INFO InMemoryFileIndex: It took 136 ms to list leaf files for 1 paths.
[2023-07-15T12:14:58.169-0300] {spark_submit.py:492} INFO - 23/07/15 12:14:58 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
[2023-07-15T12:15:02.370-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:02 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:15:02.375-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:15:02.380-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-15T12:15:02.953-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-15T12:15:03.068-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-15T12:15:03.076-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:41095 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:15:03.081-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:15:03.102-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203363 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:15:03.431-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:15:03.465-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:15:03.465-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:15:03.465-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:15:03.468-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:15:03.480-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:15:03.703-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-15T12:15:03.713-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-15T12:15:03.714-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:41095 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-15T12:15:03.715-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:15:03.749-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:15:03.754-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-15T12:15:03.871-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-15T12:15:03.904-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-15T12:15:04.132-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9059, partition values: [empty row]
[2023-07-15T12:15:04.669-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO CodeGenerator: Code generated in 335.194238 ms
[2023-07-15T12:15:04.765-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2023-07-15T12:15:04.781-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 929 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:15:04.787-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-15T12:15:04.799-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,294 s
[2023-07-15T12:15:04.805-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:15:04.807-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-15T12:15:04.811-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,379678 s
[2023-07-15T12:15:05.716-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-15T12:15:05.721-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:05 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-15T12:15:05.721-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:05 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-15T12:15:05.882-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:15:05.882-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:15:05.885-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:15:06.068-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO CodeGenerator: Code generated in 78.376251 ms
[2023-07-15T12:15:06.177-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO CodeGenerator: Code generated in 64.53364 ms
[2023-07-15T12:15:06.187-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-15T12:15:06.209-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.0 MiB)
[2023-07-15T12:15:06.211-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:41095 (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-15T12:15:06.213-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:15:06.217-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203363 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:15:06.377-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:15:06.379-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:15:06.380-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:15:06.380-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:15:06.380-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:15:06.382-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:15:06.413-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:41095 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2023-07-15T12:15:06.427-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:41095 in memory (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:15:06.488-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 434.0 MiB)
[2023-07-15T12:15:06.497-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 434.0 MiB)
[2023-07-15T12:15:06.498-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:41095 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-15T12:15:06.500-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:15:06.502-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:15:06.502-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-15T12:15:06.514-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:15:06.515-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-15T12:15:06.746-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:15:06.747-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:15:06.748-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:15:06.898-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO CodeGenerator: Code generated in 51.030352 ms
[2023-07-15T12:15:06.905-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9059, partition values: [empty row]
[2023-07-15T12:15:06.971-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:06 INFO CodeGenerator: Code generated in 55.909437 ms
[2023-07-15T12:15:07.028-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO CodeGenerator: Code generated in 12.508517 ms
[2023-07-15T12:15:07.120-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileOutputCommitter: Saved output of task 'attempt_20230715121506727505851847160759_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-12/_temporary/0/task_20230715121506727505851847160759_0001_m_000000
[2023-07-15T12:15:07.121-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SparkHadoopMapRedUtil: attempt_20230715121506727505851847160759_0001_m_000000_1: Committed
[2023-07-15T12:15:07.138-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-15T12:15:07.146-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 642 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:15:07.146-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-15T12:15:07.148-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,762 s
[2023-07-15T12:15:07.151-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:15:07.152-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-15T12:15:07.153-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,775372 s
[2023-07-15T12:15:07.204-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileFormatWriter: Write Job 4928e93e-8cf4-4468-8bb8-aef26ed02a2f committed.
[2023-07-15T12:15:07.213-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileFormatWriter: Finished processing stats for write job 4928e93e-8cf4-4468-8bb8-aef26ed02a2f.
[2023-07-15T12:15:07.322-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:15:07.322-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:15:07.323-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-15T12:15:07.348-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:15:07.348-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:15:07.349-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:15:07.413-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO CodeGenerator: Code generated in 26.905848 ms
[2023-07-15T12:15:07.422-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-15T12:15:07.443-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 433.8 MiB)
[2023-07-15T12:15:07.445-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:41095 (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-15T12:15:07.446-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:15:07.448-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203363 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:15:07.493-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:15:07.496-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:15:07.496-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:15:07.496-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:15:07.496-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:15:07.497-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:15:07.530-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.6 MiB)
[2023-07-15T12:15:07.535-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.5 MiB)
[2023-07-15T12:15:07.537-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:41095 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-15T12:15:07.541-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:15:07.542-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:15:07.543-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-15T12:15:07.544-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:15:07.545-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-15T12:15:07.577-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:15:07.577-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:15:07.578-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:15:07.631-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO CodeGenerator: Code generated in 19.35218 ms
[2023-07-15T12:15:07.635-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9059, partition values: [empty row]
[2023-07-15T12:15:07.670-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO CodeGenerator: Code generated in 28.126737 ms
[2023-07-15T12:15:07.693-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileOutputCommitter: Saved output of task 'attempt_202307151215078579836799881883132_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-12/_temporary/0/task_202307151215078579836799881883132_0002_m_000000
[2023-07-15T12:15:07.694-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SparkHadoopMapRedUtil: attempt_202307151215078579836799881883132_0002_m_000000_2: Committed
[2023-07-15T12:15:07.699-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-15T12:15:07.702-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 158 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:15:07.703-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-15T12:15:07.705-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,206 s
[2023-07-15T12:15:07.707-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:15:07.709-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-15T12:15:07.712-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,215958 s
[2023-07-15T12:15:07.736-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileFormatWriter: Write Job 8738b94e-8444-41cb-8704-7e0d7c645945 committed.
[2023-07-15T12:15:07.737-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO FileFormatWriter: Finished processing stats for write job 8738b94e-8444-41cb-8704-7e0d7c645945.
[2023-07-15T12:15:07.846-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-15T12:15:07.864-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-15T12:15:07.890-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-15T12:15:07.911-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO MemoryStore: MemoryStore cleared
[2023-07-15T12:15:07.912-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO BlockManager: BlockManager stopped
[2023-07-15T12:15:07.917-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-15T12:15:07.922-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-15T12:15:07.934-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO SparkContext: Successfully stopped SparkContext
[2023-07-15T12:15:07.935-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO ShutdownHookManager: Shutdown hook called
[2023-07-15T12:15:07.936-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-0f401cd8-4e05-4f45-b414-f55efd8fabf6
[2023-07-15T12:15:07.943-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-27329e14-1a4e-4048-a2d4-2b9ccc94a64d/pyspark-d9ae25e6-08d4-452a-89af-767ff4e3408a
[2023-07-15T12:15:07.947-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-27329e14-1a4e-4048-a2d4-2b9ccc94a64d
[2023-07-15T12:15:08.022-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230712T000000, start_date=20230715T151446, end_date=20230715T151508
[2023-07-15T12:15:08.055-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-15T12:15:08.082-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
