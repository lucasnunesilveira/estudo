[2023-07-13T22:25:30.794-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:25:30.798-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:25:30.798-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:25:30.807-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:25:30.809-0300] {standard_task_runner.py:57} INFO - Started process 36604 to run task
[2023-07-13T22:25:30.811-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp67wdq_u9']
[2023-07-13T22:25:30.812-0300] {standard_task_runner.py:85} INFO - Job 11: Subtask transform_twitter_datascience
[2023-07-13T22:25:30.836-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:25:30.893-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:25:30.896-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:25:30.897-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-11
[2023-07-13T22:25:32.614-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:32 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:25:32.617-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:25:32.639-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:25:32.640-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:25:32.640-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:25:32.640-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:25:32.640-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:25:32.721-0300] {spark_submit.py:492} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2023-07-13T22:25:32.725-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:631)
[2023-07-13T22:25:32.725-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:271)
[2023-07-13T22:25:32.725-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:234)
[2023-07-13T22:25:32.726-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:119)
[2023-07-13T22:25:32.726-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1022)
[2023-07-13T22:25:32.726-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1022)
[2023-07-13T22:25:32.726-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)
[2023-07-13T22:25:32.726-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
[2023-07-13T22:25:32.726-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
[2023-07-13T22:25:32.726-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-07-13T22:25:32.765-0300] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 423, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-11. Error code is: 1.
[2023-07-13T22:25:32.770-0300] {taskinstance.py:1345} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T012530, end_date=20230714T012532
[2023-07-13T22:25:32.780-0300] {standard_task_runner.py:104} ERROR - Failed to execute job 11 for task transform_twitter_datascience (Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-11. Error code is: 1.; 36604)
[2023-07-13T22:25:32.798-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-13T22:25:32.806-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:31:24.872-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:31:24.878-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:31:24.878-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:31:24.887-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:31:24.889-0300] {standard_task_runner.py:57} INFO - Started process 39213 to run task
[2023-07-13T22:31:24.891-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpx6tv66bk']
[2023-07-13T22:31:24.892-0300] {standard_task_runner.py:85} INFO - Job 11: Subtask transform_twitter_datascience
[2023-07-13T22:31:24.917-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:31:24.977-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:31:24.983-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:31:24.985-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-11
[2023-07-13T22:31:26.215-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:26 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:31:26.215-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:31:26.224-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:31:26.225-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:31:26.225-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:31:26.225-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:31:26.225-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:31:26.648-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:31:27.189-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-13T22:31:27.198-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SparkContext: Running Spark version 3.1.3
[2023-07-13T22:31:27.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:27.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:31:27.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:27.242-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:31:27.267-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:31:27.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:31:27.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:31:27.322-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:31:27.323-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:31:27.323-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:31:27.323-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:31:27.323-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:31:27.577-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO Utils: Successfully started service 'sparkDriver' on port 35371.
[2023-07-13T22:31:27.606-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:31:27.648-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:31:27.666-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:31:27.667-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:31:27.671-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:31:27.687-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-52a9e8c0-afb5-474d-8747-61c9373ef06a
[2023-07-13T22:31:27.711-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:31:27.728-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:31:27.940-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:31:28.006-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4040
[2023-07-13T22:31:28.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:31:28.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38175.
[2023-07-13T22:31:28.241-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO NettyBlockTransferService: Server created on 192.168.0.177:38175
[2023-07-13T22:31:28.244-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:31:28.251-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 38175, None)
[2023-07-13T22:31:28.255-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:38175 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 38175, None)
[2023-07-13T22:31:28.258-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 38175, None)
[2023-07-13T22:31:28.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 38175, None)
[2023-07-13T22:31:28.710-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-13T22:31:28.710-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:28 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:31:29.541-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:29 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
[2023-07-13T22:31:29.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:29 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 8 paths.
[2023-07-13T22:31:31.275-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:31.276-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:31.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:31:31.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-13T22:31:31.600-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-13T22:31:31.602-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:38175 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:31:31.605-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:31.616-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33604254 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:31.813-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:31.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:31.830-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:31.830-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:31.832-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:31.836-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:31.987-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-13T22:31:32.000-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-13T22:31:32.001-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:38175 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-13T22:31:32.002-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:32.035-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:32.036-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:31:32.106-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6291 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:32.134-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:31:32.275-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:31:32.651-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO CodeGenerator: Code generated in 187.368517 ms
[2023-07-13T22:31:32.696-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [empty row]
[2023-07-13T22:31:32.703-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:31:32.707-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [empty row]
[2023-07-13T22:31:32.712-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [empty row]
[2023-07-13T22:31:32.715-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [empty row]
[2023-07-13T22:31:32.717-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [empty row]
[2023-07-13T22:31:32.721-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [empty row]
[2023-07-13T22:31:32.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2023-07-13T22:31:32.756-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 660 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:32.757-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:31:32.766-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,900 s
[2023-07-13T22:31:32.769-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:32.769-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:31:32.772-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:32 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,958247 s
[2023-07-13T22:31:33.428-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:31:33.441-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:31:33.448-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-13T22:31:33.449-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:31:33.672-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:33.673-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:33.673-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:33.866-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO CodeGenerator: Code generated in 51.375148 ms
[2023-07-13T22:31:33.965-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:38175 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-13T22:31:33.974-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:38175 in memory (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:31:34.018-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO CodeGenerator: Code generated in 80.870399 ms
[2023-07-13T22:31:34.037-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.2 MiB)
[2023-07-13T22:31:34.051-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-13T22:31:34.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:38175 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:31:34.054-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:34.060-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33604254 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:34.250-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:34.255-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:34.256-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:34.256-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:34.256-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:34.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:34.415-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 195.4 KiB, free 434.0 MiB)
[2023-07-13T22:31:34.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 68.5 KiB, free 433.9 MiB)
[2023-07-13T22:31:34.420-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:38175 (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:31:34.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:34.425-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:34.426-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:31:34.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:34.436-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:31:34.545-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:34.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:34.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:34.710-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO CodeGenerator: Code generated in 41.365075 ms
[2023-07-13T22:31:34.716-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:31:34.764-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO CodeGenerator: Code generated in 42.285403 ms
[2023-07-13T22:31:34.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO CodeGenerator: Code generated in 7.671802 ms
[2023-07-13T22:31:34.903-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:31:34.919-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:31:34.936-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:31:34.950-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:31:34.965-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:31:34.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:31:34.986-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [19551]
[2023-07-13T22:31:35.023-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileOutputCommitter: Saved output of task 'attempt_202307132231343379995032226230783_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-11/_temporary/0/task_202307132231343379995032226230783_0001_m_000000
[2023-07-13T22:31:35.025-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SparkHadoopMapRedUtil: attempt_202307132231343379995032226230783_0001_m_000000_1: Committed
[2023-07-13T22:31:35.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3076 bytes result sent to driver
[2023-07-13T22:31:35.045-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 617 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:35.045-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:31:35.046-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,785 s
[2023-07-13T22:31:35.047-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:35.047-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:31:35.048-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,797750 s
[2023-07-13T22:31:35.087-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileFormatWriter: Write Job 0792c8f7-5ebb-4411-bef4-f15b00c2f970 committed.
[2023-07-13T22:31:35.093-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileFormatWriter: Finished processing stats for write job 0792c8f7-5ebb-4411-bef4-f15b00c2f970.
[2023-07-13T22:31:35.160-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:31:35.161-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:35.161-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:35.161-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:31:35.194-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:35.196-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:35.197-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:35.268-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO CodeGenerator: Code generated in 19.331261 ms
[2023-07-13T22:31:35.295-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO CodeGenerator: Code generated in 22.931685 ms
[2023-07-13T22:31:35.300-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-13T22:31:35.333-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.7 MiB)
[2023-07-13T22:31:35.334-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:38175 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:31:35.334-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:35.335-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33604254 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:35.384-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:35.385-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:35.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:35.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:35.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:35.387-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:35.412-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 184.3 KiB, free 433.6 MiB)
[2023-07-13T22:31:35.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 433.5 MiB)
[2023-07-13T22:31:35.425-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:38175 (size: 65.7 KiB, free: 434.2 MiB)
[2023-07-13T22:31:35.425-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:35.426-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:35.426-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:31:35.427-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:35.444-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:31:35.468-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:35.468-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:35.469-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:35.513-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO CodeGenerator: Code generated in 12.352562 ms
[2023-07-13T22:31:35.515-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:31:35.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:38175 in memory (size: 27.6 KiB, free: 434.2 MiB)
[2023-07-13T22:31:35.553-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:38175 in memory (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:31:35.566-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO CodeGenerator: Code generated in 22.693031 ms
[2023-07-13T22:31:35.579-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:31:35.583-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:31:35.589-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:31:35.597-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:31:35.601-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:31:35.604-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:31:35.608-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [19551]
[2023-07-13T22:31:35.626-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileOutputCommitter: Saved output of task 'attempt_202307132231358807679991211764677_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-11/_temporary/0/task_202307132231358807679991211764677_0002_m_000000
[2023-07-13T22:31:35.626-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SparkHadoopMapRedUtil: attempt_202307132231358807679991211764677_0002_m_000000_2: Committed
[2023-07-13T22:31:35.632-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3014 bytes result sent to driver
[2023-07-13T22:31:35.638-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 212 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:35.639-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:31:35.641-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,252 s
[2023-07-13T22:31:35.641-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:35.641-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:31:35.642-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,258598 s
[2023-07-13T22:31:35.704-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileFormatWriter: Write Job ee763f0d-c0fd-42f9-aa6b-57e20497266b committed.
[2023-07-13T22:31:35.704-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO FileFormatWriter: Finished processing stats for write job ee763f0d-c0fd-42f9-aa6b-57e20497266b.
[2023-07-13T22:31:35.825-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:31:35.843-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:31:35.860-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:31:35.877-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:31:35.877-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO BlockManager: BlockManager stopped
[2023-07-13T22:31:35.892-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:31:35.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:31:35.906-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:31:35.906-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:31:35.907-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-cdff7895-a0bb-42e9-9d31-9f10676056a2/pyspark-54f000ec-d1d2-4fa1-a5cb-667176b98e09
[2023-07-13T22:31:35.911-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-cdff7895-a0bb-42e9-9d31-9f10676056a2
[2023-07-13T22:31:35.914-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-03320da9-dce2-493d-a222-c5190edd1b6b
[2023-07-13T22:31:36.035-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T013124, end_date=20230714T013136
[2023-07-13T22:31:36.067-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:31:36.079-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:43:31.320-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:43:31.324-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:43:31.325-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:43:31.333-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:43:31.335-0300] {standard_task_runner.py:57} INFO - Started process 44278 to run task
[2023-07-13T22:43:31.337-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmphzng3yd_']
[2023-07-13T22:43:31.338-0300] {standard_task_runner.py:85} INFO - Job 29: Subtask transform_twitter_datascience
[2023-07-13T22:43:31.358-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:43:31.401-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:43:31.406-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:43:31.408-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-11
[2023-07-13T22:43:32.737-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:32 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:43:32.739-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:43:33.417-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:43:33.471-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:43:33.542-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO ResourceUtils: ==============================================================
[2023-07-13T22:43:33.542-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:43:33.543-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO ResourceUtils: ==============================================================
[2023-07-13T22:43:33.543-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:43:33.560-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:43:33.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:43:33.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:43:33.616-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:43:33.616-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:43:33.617-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:43:33.617-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:43:33.617-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:43:33.815-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO Utils: Successfully started service 'sparkDriver' on port 41453.
[2023-07-13T22:43:33.842-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:43:33.880-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:43:33.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:43:33.900-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:43:33.905-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:43:33.932-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4872c3a3-48f2-4af3-bf73-262ea1bb8ad2
[2023-07-13T22:43:33.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:43:33.986-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:43:34.223-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:43:34.306-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:43:34.313-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:43:34.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44301.
[2023-07-13T22:43:34.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO NettyBlockTransferService: Server created on 192.168.0.177:44301
[2023-07-13T22:43:34.331-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:43:34.336-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 44301, None)
[2023-07-13T22:43:34.340-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:44301 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 44301, None)
[2023-07-13T22:43:34.342-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 44301, None)
[2023-07-13T22:43:34.343-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 44301, None)
[2023-07-13T22:43:34.674-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:43:34.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:34 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:43:35.356-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:35 INFO InMemoryFileIndex: It took 50 ms to list leaf files for 1 paths.
[2023-07-13T22:43:35.477-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:35 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 8 paths.
[2023-07-13T22:43:36.978-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:36 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:43:36.979-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:36 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:43:36.981-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:43:37.256-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:43:37.294-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:37.297-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:44301 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:37.301-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:37.310-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33622074 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:37.431-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:37.445-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:37.445-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:37.446-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:37.447-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:37.449-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:37.535-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:43:37.537-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:37.538-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:44301 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:37.539-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:37.550-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:37.551-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:43:37.589-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:37.601-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:43:37.691-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [empty row]
[2023-07-13T22:43:37.834-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO CodeGenerator: Code generated in 113.592907 ms
[2023-07-13T22:43:37.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [empty row]
[2023-07-13T22:43:37.877-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [empty row]
[2023-07-13T22:43:37.880-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:43:37.884-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:43:37.887-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [empty row]
[2023-07-13T22:43:37.889-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [empty row]
[2023-07-13T22:43:37.892-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [empty row]
[2023-07-13T22:43:37.907-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:43:37.914-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 331 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:37.915-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:43:37.919-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,450 s
[2023-07-13T22:43:37.922-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:37.922-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:43:37.924-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:37 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,492287 s
[2023-07-13T22:43:38.088-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:44301 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:38.094-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:44301 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:38.251-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:43:38.253-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:43:38.254-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:43:38.254-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:43:38.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:38.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:38.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:38.456-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO CodeGenerator: Code generated in 79.709858 ms
[2023-07-13T22:43:38.463-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:38.473-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:43:38.474-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:44301 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:43:38.475-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:38.479-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33622074 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:38.526-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:38.527-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:38.528-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:38.528-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:38.528-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:38.529-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:38.549-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.5 KiB, free 433.9 MiB)
[2023-07-13T22:43:38.552-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:43:38.553-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:44301 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:43:38.554-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:38.554-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:38.554-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:43:38.557-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:38.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:43:38.596-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:38.596-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:38.597-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:38.619-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:43:38.640-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO CodeGenerator: Code generated in 17.510507 ms
[2023-07-13T22:43:38.670-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO CodeGenerator: Code generated in 4.127398 ms
[2023-07-13T22:43:38.692-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:43:38.700-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:43:38.706-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:43:38.712-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:43:38.718-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:43:38.722-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:43:38.727-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:43:38.738-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: Saved output of task 'attempt_202307132243382717814171125745088_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-11/_temporary/0/task_202307132243382717814171125745088_0001_m_000000
[2023-07-13T22:43:38.739-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SparkHadoopMapRedUtil: attempt_202307132243382717814171125745088_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:43:38.746-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:43:38.747-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 192 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:38.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:43:38.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,219 s
[2023-07-13T22:43:38.749-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:38.749-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:43:38.750-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,222973 s
[2023-07-13T22:43:38.752-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileFormatWriter: Start to commit write Job 5a9b201c-98cb-4ef0-8e4a-9ee426af806f.
[2023-07-13T22:43:38.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileFormatWriter: Write Job 5a9b201c-98cb-4ef0-8e4a-9ee426af806f committed. Elapsed time: 10 ms.
[2023-07-13T22:43:38.766-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileFormatWriter: Finished processing stats for write job 5a9b201c-98cb-4ef0-8e4a-9ee426af806f.
[2023-07-13T22:43:38.793-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:43:38.793-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:43:38.793-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:43:38.794-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:43:38.802-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:38.802-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:38.802-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:38.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO CodeGenerator: Code generated in 10.732435 ms
[2023-07-13T22:43:38.832-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:43:38.841-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:43:38.842-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:44301 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:43:38.843-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:38.844-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33622074 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:38.865-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:38.866-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:38.866-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:38.866-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:38.867-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:38.868-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:38.895-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.5 KiB, free 433.4 MiB)
[2023-07-13T22:43:38.897-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:43:38.897-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:44301 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:43:38.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:38.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:38.899-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:43:38.900-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:38.900-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:43:38.915-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:38.915-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:38.916-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:38.927-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:43:38.938-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO CodeGenerator: Code generated in 8.632504 ms
[2023-07-13T22:43:38.942-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:43:38.945-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:43:38.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:43:38.950-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:43:38.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:43:38.955-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:43:38.957-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:43:38.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileOutputCommitter: Saved output of task 'attempt_202307132243384067660381168745399_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-11/_temporary/0/task_202307132243384067660381168745399_0002_m_000000
[2023-07-13T22:43:38.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO SparkHadoopMapRedUtil: attempt_202307132243384067660381168745399_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-13T22:43:38.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:43:38.965-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 65 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:38.965-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:43:38.966-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,097 s
[2023-07-13T22:43:38.966-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:38.966-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:43:38.966-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,100610 s
[2023-07-13T22:43:38.966-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileFormatWriter: Start to commit write Job cc54a6bf-50dd-4527-acf5-9f8b9502141d.
[2023-07-13T22:43:38.977-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileFormatWriter: Write Job cc54a6bf-50dd-4527-acf5-9f8b9502141d committed. Elapsed time: 10 ms.
[2023-07-13T22:43:38.978-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:38 INFO FileFormatWriter: Finished processing stats for write job cc54a6bf-50dd-4527-acf5-9f8b9502141d.
[2023-07-13T22:43:39.001-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:43:39.010-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:43:39.018-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:43:39.025-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:43:39.026-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO BlockManager: BlockManager stopped
[2023-07-13T22:43:39.028-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:43:39.030-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:43:39.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:43:39.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:43:39.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0be0566-bfb4-4ef1-aca5-bcd724ffc539
[2023-07-13T22:43:39.036-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-bf658e06-e016-4484-b0fb-d284d0e477eb/pyspark-46d455b5-5f3c-43c9-93ce-1ac24f256489
[2023-07-13T22:43:39.037-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-bf658e06-e016-4484-b0fb-d284d0e477eb
[2023-07-13T22:43:39.087-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T014331, end_date=20230714T014339
[2023-07-13T22:43:39.114-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:43:39.123-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:49:52.890-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:49:52.895-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:49:52.895-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:49:52.905-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:49:52.907-0300] {standard_task_runner.py:57} INFO - Started process 47460 to run task
[2023-07-13T22:49:52.909-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp51yrki__']
[2023-07-13T22:49:52.910-0300] {standard_task_runner.py:85} INFO - Job 29: Subtask transform_twitter_datascience
[2023-07-13T22:49:52.930-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:49:52.988-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:49:52.992-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:49:52.993-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest FormacaoAirflow/dados_transformation --process-date 2023-07-11
[2023-07-13T22:49:54.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:49:54.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:49:54.847-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:49:54.896-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:49:54.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 INFO ResourceUtils: ==============================================================
[2023-07-13T22:49:54.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:49:54.964-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 INFO ResourceUtils: ==============================================================
[2023-07-13T22:49:54.964-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:49:54.983-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:49:54.992-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:49:54.993-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:49:55.028-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:49:55.029-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:49:55.029-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:49:55.029-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:49:55.029-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:49:55.258-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO Utils: Successfully started service 'sparkDriver' on port 43543.
[2023-07-13T22:49:55.287-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:49:55.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:49:55.346-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:49:55.346-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:49:55.350-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:49:55.366-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7e6daf50-0136-4d9e-a5ca-4d7c4167a87f
[2023-07-13T22:49:55.385-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:49:55.401-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:49:55.611-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:49:55.719-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:49:55.726-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:49:55.742-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40591.
[2023-07-13T22:49:55.742-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO NettyBlockTransferService: Server created on 192.168.0.177:40591
[2023-07-13T22:49:55.744-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:49:55.750-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 40591, None)
[2023-07-13T22:49:55.754-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:40591 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 40591, None)
[2023-07-13T22:49:55.756-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 40591, None)
[2023-07-13T22:49:55.757-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 40591, None)
[2023-07-13T22:49:56.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:49:56.103-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:56 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:49:56.879-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:56 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
[2023-07-13T22:49:57.005-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:57 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:49:58.682-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:58 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:49:58.683-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:58 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:49:58.686-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:58 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:49:58.910-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:49:58.949-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:49:58.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:40591 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:58.955-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:58 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:58.971-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33640787 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:49:59.089-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:59.102-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:49:59.102-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:49:59.102-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:49:59.103-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:49:59.107-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:49:59.184-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:49:59.186-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:49:59.186-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:40591 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:59.187-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:49:59.197-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:49:59.198-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:49:59.235-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:49:59.246-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:49:59.332-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:49:59.480-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO CodeGenerator: Code generated in 121.125782 ms
[2023-07-13T22:49:59.520-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [empty row]
[2023-07-13T22:49:59.526-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [empty row]
[2023-07-13T22:49:59.529-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:49:59.532-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:49:59.535-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [empty row]
[2023-07-13T22:49:59.537-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [empty row]
[2023-07-13T22:49:59.540-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [empty row]
[2023-07-13T22:49:59.553-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:49:59.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 330 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:49:59.560-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:49:59.564-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,439 s
[2023-07-13T22:49:59.567-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:49:59.567-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:49:59.569-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,479346 s
[2023-07-13T22:49:59.806-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:40591 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:59.810-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:40591 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:59.967-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:49:59.972-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:49:59.974-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:49:59.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:59 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:50:00.055-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:50:00.056-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:50:00.058-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:50:00.283-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO CodeGenerator: Code generated in 114.797267 ms
[2023-07-13T22:50:00.291-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:50:00.305-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:50:00.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:40591 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:50:00.310-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:50:00.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33640787 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:50:00.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:50:00.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:50:00.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:50:00.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:50:00.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:50:00.399-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:50:00.433-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.4 KiB, free 433.9 MiB)
[2023-07-13T22:50:00.436-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:50:00.437-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:40591 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:50:00.437-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:50:00.438-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:50:00.438-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:50:00.442-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:50:00.443-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:50:00.482-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:50:00.482-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:50:00.483-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:50:00.510-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:50:00.536-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO CodeGenerator: Code generated in 20.772121 ms
[2023-07-13T22:50:00.553-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO CodeGenerator: Code generated in 3.482642 ms
[2023-07-13T22:50:00.578-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [19546]
[2023-07-13T22:50:00.585-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [19548]
[2023-07-13T22:50:00.591-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:50:00.596-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:50:00.601-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [19549]
[2023-07-13T22:50:00.606-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:50:00.610-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [19547]
[2023-07-13T22:50:00.622-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: Saved output of task 'attempt_202307132250006359555475686169817_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-11/_temporary/0/task_202307132250006359555475686169817_0001_m_000000
[2023-07-13T22:50:00.622-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkHadoopMapRedUtil: attempt_202307132250006359555475686169817_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:50:00.629-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:50:00.630-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 191 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:50:00.631-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:50:00.631-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,231 s
[2023-07-13T22:50:00.632-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:50:00.632-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:50:00.632-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,235594 s
[2023-07-13T22:50:00.634-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileFormatWriter: Start to commit write Job 95a39c5d-54ef-4c63-8257-e3ed6e4f26f7.
[2023-07-13T22:50:00.645-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileFormatWriter: Write Job 95a39c5d-54ef-4c63-8257-e3ed6e4f26f7 committed. Elapsed time: 9 ms.
[2023-07-13T22:50:00.647-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileFormatWriter: Finished processing stats for write job 95a39c5d-54ef-4c63-8257-e3ed6e4f26f7.
[2023-07-13T22:50:00.669-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:40591 in memory (size: 83.4 KiB, free: 434.4 MiB)
[2023-07-13T22:50:00.690-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:50:00.691-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:50:00.691-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:50:00.691-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:50:00.701-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:50:00.701-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:50:00.702-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:50:00.725-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO CodeGenerator: Code generated in 9.043397 ms
[2023-07-13T22:50:00.728-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-13T22:50:00.737-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-13T22:50:00.738-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:40591 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:50:00.739-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:50:00.740-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33640787 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:50:00.762-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:50:00.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:50:00.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:50:00.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:50:00.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:50:00.764-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:50:00.782-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.7 MiB)
[2023-07-13T22:50:00.784-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.7 MiB)
[2023-07-13T22:50:00.785-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:40591 (size: 77.2 KiB, free: 434.3 MiB)
[2023-07-13T22:50:00.785-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:50:00.785-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:50:00.786-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:50:00.787-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:50:00.787-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:50:00.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:50:00.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:50:00.801-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:50:00.813-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:50:00.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO CodeGenerator: Code generated in 12.650627 ms
[2023-07-13T22:50:00.837-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [19546]
[2023-07-13T22:50:00.841-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [19548]
[2023-07-13T22:50:00.844-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:50:00.848-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:50:00.852-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [19549]
[2023-07-13T22:50:00.855-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:50:00.858-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [19547]
[2023-07-13T22:50:00.864-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileOutputCommitter: Saved output of task 'attempt_202307132250002719554592311310896_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/user/process_date=2023-07-11/_temporary/0/task_202307132250002719554592311310896_0002_m_000000
[2023-07-13T22:50:00.864-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkHadoopMapRedUtil: attempt_202307132250002719554592311310896_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:50:00.866-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:50:00.868-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 82 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:50:00.868-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:50:00.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,103 s
[2023-07-13T22:50:00.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:50:00.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:50:00.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,107289 s
[2023-07-13T22:50:00.870-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileFormatWriter: Start to commit write Job d9577a71-92bf-4eb8-8a70-e1cf6254cab0.
[2023-07-13T22:50:00.878-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileFormatWriter: Write Job d9577a71-92bf-4eb8-8a70-e1cf6254cab0 committed. Elapsed time: 8 ms.
[2023-07-13T22:50:00.879-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO FileFormatWriter: Finished processing stats for write job d9577a71-92bf-4eb8-8a70-e1cf6254cab0.
[2023-07-13T22:50:00.903-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:50:00.911-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:50:00.920-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:50:00.928-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:50:00.928-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO BlockManager: BlockManager stopped
[2023-07-13T22:50:00.932-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:50:00.934-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:50:00.937-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:50:00.937-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:50:00.938-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-9833a61b-1611-4578-8a0c-54594f5c1004
[2023-07-13T22:50:00.940-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-9833a61b-1611-4578-8a0c-54594f5c1004/pyspark-52fbdcb7-2ee5-40b1-b00b-74aa85f518a1
[2023-07-13T22:50:00.941-0300] {spark_submit.py:492} INFO - 23/07/13 22:50:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-25e6d0e0-3959-4d62-ab9f-bc50293c0f28
[2023-07-13T22:50:00.986-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T014952, end_date=20230714T015000
[2023-07-13T22:50:01.010-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:50:01.017-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:51:48.173-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:51:48.180-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:51:48.180-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:51:48.191-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:51:48.195-0300] {standard_task_runner.py:57} INFO - Started process 49214 to run task
[2023-07-13T22:51:48.197-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpf94rka7v']
[2023-07-13T22:51:48.198-0300] {standard_task_runner.py:85} INFO - Job 29: Subtask transform_twitter_datascience
[2023-07-13T22:51:48.227-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:51:48.265-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:51:48.268-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:51:48.269-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest FormacaoAirflow/dados_transformation --process-date 2023-07-11
[2023-07-13T22:51:49.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:49 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:51:49.514-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:51:50.147-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:51:50.207-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:51:50.272-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO ResourceUtils: ==============================================================
[2023-07-13T22:51:50.273-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:51:50.273-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO ResourceUtils: ==============================================================
[2023-07-13T22:51:50.273-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:51:50.289-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:51:50.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:51:50.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:51:50.336-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:51:50.336-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:51:50.337-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:51:50.337-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:51:50.337-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:51:50.506-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO Utils: Successfully started service 'sparkDriver' on port 33239.
[2023-07-13T22:51:50.525-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:51:50.548-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:51:50.562-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:51:50.562-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:51:50.565-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:51:50.580-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-25b9fb03-74f4-47f2-b76f-3e1d75dbd4ae
[2023-07-13T22:51:50.592-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:51:50.605-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:51:50.773-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:51:50.859-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:51:50.865-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:51:50.882-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37797.
[2023-07-13T22:51:50.882-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO NettyBlockTransferService: Server created on 192.168.0.177:37797
[2023-07-13T22:51:50.884-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:51:50.889-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 37797, None)
[2023-07-13T22:51:50.891-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:37797 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 37797, None)
[2023-07-13T22:51:50.893-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 37797, None)
[2023-07-13T22:51:50.894-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 37797, None)
[2023-07-13T22:51:51.293-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:51:51.298-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:51 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:51:51.960-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:51 INFO InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.
[2023-07-13T22:51:52.058-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:52 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:51:53.481-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:51:53.482-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:51:53.485-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:51:53.695-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:51:53.742-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:51:53.745-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:37797 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:53.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:53.756-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672098 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:53.875-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:53.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:53.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:53.891-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:53.892-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:53.896-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:53.982-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:51:53.984-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:51:53.984-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:37797 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:53.985-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:53.994-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:53.994-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:51:54.029-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:54.039-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:51:54.132-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:51:54.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO CodeGenerator: Code generated in 131.802308 ms
[2023-07-13T22:51:54.335-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [empty row]
[2023-07-13T22:51:54.342-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [empty row]
[2023-07-13T22:51:54.346-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [empty row]
[2023-07-13T22:51:54.350-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:51:54.353-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:51:54.355-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [empty row]
[2023-07-13T22:51:54.358-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [empty row]
[2023-07-13T22:51:54.371-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:51:54.377-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 355 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:54.378-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:51:54.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,464 s
[2023-07-13T22:51:54.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:54.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:51:54.389-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,512751 s
[2023-07-13T22:51:54.501-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:37797 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:54.513-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:37797 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:54.712-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:51:54.715-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:51:54.716-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:51:54.716-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:51:54.771-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:54.771-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:54.772-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:54.905-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO CodeGenerator: Code generated in 73.35086 ms
[2023-07-13T22:51:54.909-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:51:54.918-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:51:54.919-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:37797 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:51:54.920-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:54.923-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672098 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:54.970-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:54.971-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:54.971-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:54.971-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:54.971-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:54.972-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:54 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:55.005-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.4 KiB, free 433.9 MiB)
[2023-07-13T22:51:55.010-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.5 KiB, free 433.9 MiB)
[2023-07-13T22:51:55.011-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:37797 (size: 83.5 KiB, free: 434.3 MiB)
[2023-07-13T22:51:55.012-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:55.013-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:55.013-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:51:55.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:55.017-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:51:55.073-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:55.073-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:55.073-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:55.093-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:51:55.112-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO CodeGenerator: Code generated in 15.637054 ms
[2023-07-13T22:51:55.130-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO CodeGenerator: Code generated in 3.888508 ms
[2023-07-13T22:51:55.155-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [19549]
[2023-07-13T22:51:55.165-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [19546]
[2023-07-13T22:51:55.172-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [19548]
[2023-07-13T22:51:55.179-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:51:55.187-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:51:55.192-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [19550]
[2023-07-13T22:51:55.197-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [19547]
[2023-07-13T22:51:55.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileOutputCommitter: Saved output of task 'attempt_202307132251547418820361142315670_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-11/_temporary/0/task_202307132251547418820361142315670_0001_m_000000
[2023-07-13T22:51:55.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkHadoopMapRedUtil: attempt_202307132251547418820361142315670_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:51:55.215-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:51:55.216-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 202 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:55.217-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:51:55.217-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,244 s
[2023-07-13T22:51:55.218-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:55.218-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:51:55.219-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,248323 s
[2023-07-13T22:51:55.220-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileFormatWriter: Start to commit write Job a2cd3518-2710-4af7-b325-4b9814eb8be7.
[2023-07-13T22:51:55.231-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileFormatWriter: Write Job a2cd3518-2710-4af7-b325-4b9814eb8be7 committed. Elapsed time: 9 ms.
[2023-07-13T22:51:55.233-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileFormatWriter: Finished processing stats for write job a2cd3518-2710-4af7-b325-4b9814eb8be7.
[2023-07-13T22:51:55.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:51:55.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:51:55.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:51:55.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:51:55.268-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:55.269-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:55.269-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:55.300-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO CodeGenerator: Code generated in 13.707342 ms
[2023-07-13T22:51:55.303-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:51:55.312-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:51:55.313-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:37797 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:51:55.313-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:55.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672098 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:55.335-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:55.336-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:55.336-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:55.336-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:55.337-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:55.337-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:55.355-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:51:55.356-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:51:55.357-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:37797 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:51:55.357-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:55.358-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:55.358-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:51:55.359-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:55.360-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:51:55.372-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:55.372-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:55.373-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:55.384-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:51:55.396-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO CodeGenerator: Code generated in 9.524471 ms
[2023-07-13T22:51:55.401-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [19549]
[2023-07-13T22:51:55.405-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [19546]
[2023-07-13T22:51:55.409-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [19548]
[2023-07-13T22:51:55.412-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:51:55.415-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:51:55.417-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [19550]
[2023-07-13T22:51:55.420-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [19547]
[2023-07-13T22:51:55.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileOutputCommitter: Saved output of task 'attempt_202307132251555944163542660190265_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/user/process_date=2023-07-11/_temporary/0/task_202307132251555944163542660190265_0002_m_000000
[2023-07-13T22:51:55.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkHadoopMapRedUtil: attempt_202307132251555944163542660190265_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:51:55.426-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:51:55.427-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 68 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:55.427-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:51:55.428-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,089 s
[2023-07-13T22:51:55.428-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:55.428-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:51:55.428-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,092910 s
[2023-07-13T22:51:55.429-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileFormatWriter: Start to commit write Job 8ed3ff41-91dc-4eda-be32-1645fb5b99eb.
[2023-07-13T22:51:55.436-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileFormatWriter: Write Job 8ed3ff41-91dc-4eda-be32-1645fb5b99eb committed. Elapsed time: 7 ms.
[2023-07-13T22:51:55.437-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO FileFormatWriter: Finished processing stats for write job 8ed3ff41-91dc-4eda-be32-1645fb5b99eb.
[2023-07-13T22:51:55.460-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:51:55.469-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:51:55.479-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:51:55.487-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:51:55.487-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO BlockManager: BlockManager stopped
[2023-07-13T22:51:55.490-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:51:55.492-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:51:55.495-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:51:55.496-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:51:55.496-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-6744b390-1255-4f07-8ef6-a3297a069f53
[2023-07-13T22:51:55.498-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-d4e4b52e-0936-44fb-8173-bf7efe1f98ff
[2023-07-13T22:51:55.500-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-d4e4b52e-0936-44fb-8173-bf7efe1f98ff/pyspark-6c5e645e-3163-49e3-8b6c-99b0deca6310
[2023-07-13T22:51:55.545-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T015148, end_date=20230714T015155
[2023-07-13T22:51:55.571-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:51:55.577-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:53:17.172-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:53:17.176-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:53:17.176-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:53:17.183-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:53:17.186-0300] {standard_task_runner.py:57} INFO - Started process 51030 to run task
[2023-07-13T22:53:17.187-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpiyfkbmu7']
[2023-07-13T22:53:17.188-0300] {standard_task_runner.py:85} INFO - Job 29: Subtask transform_twitter_datascience
[2023-07-13T22:53:17.209-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:53:17.254-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:53:17.258-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:53:17.260-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest dados_transformation --process-date 2023-07-11
[2023-07-13T22:53:18.541-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:18 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:53:18.543-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:53:19.206-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:53:19.251-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:53:19.319-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO ResourceUtils: ==============================================================
[2023-07-13T22:53:19.319-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:53:19.319-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO ResourceUtils: ==============================================================
[2023-07-13T22:53:19.320-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:53:19.339-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:53:19.350-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:53:19.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:53:19.392-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:53:19.392-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:53:19.392-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:53:19.393-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:53:19.393-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:53:19.595-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO Utils: Successfully started service 'sparkDriver' on port 35707.
[2023-07-13T22:53:19.618-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:53:19.646-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:53:19.660-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:53:19.661-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:53:19.664-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:53:19.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6ac94761-fd1f-4b2f-82d8-6458a15bb1e2
[2023-07-13T22:53:19.696-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:53:19.711-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:53:19.888-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:53:19.970-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:53:19.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:53:19.989-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46285.
[2023-07-13T22:53:19.989-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO NettyBlockTransferService: Server created on 192.168.0.177:46285
[2023-07-13T22:53:19.991-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:53:19.996-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 46285, None)
[2023-07-13T22:53:19.999-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:46285 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 46285, None)
[2023-07-13T22:53:20.002-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 46285, None)
[2023-07-13T22:53:20.002-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 46285, None)
[2023-07-13T22:53:20.346-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:53:20.352-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:20 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:53:21.036-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:21 INFO InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.
[2023-07-13T22:53:21.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:21 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 8 paths.
[2023-07-13T22:53:22.561-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:22.562-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:22.564-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:53:22.761-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:53:22.809-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:22.811-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:46285 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:22.814-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:22.822-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33686118 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:22.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:22.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:22.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:22.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:22.964-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:22.967-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:23.028-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:53:23.029-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:23.030-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:46285 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:23.030-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:23.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:23.041-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:53:23.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:23.094-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:53:23.180-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [empty row]
[2023-07-13T22:53:23.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO CodeGenerator: Code generated in 102.110128 ms
[2023-07-13T22:53:23.353-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:53:23.363-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [empty row]
[2023-07-13T22:53:23.370-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [empty row]
[2023-07-13T22:53:23.375-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [empty row]
[2023-07-13T22:53:23.380-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:53:23.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:53:23.387-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [empty row]
[2023-07-13T22:53:23.405-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2849 bytes result sent to driver
[2023-07-13T22:53:23.412-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 340 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:23.414-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:53:23.417-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,439 s
[2023-07-13T22:53:23.420-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:23.420-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:53:23.422-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,470361 s
[2023-07-13T22:53:23.736-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:23.738-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:53:23.740-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:53:23.740-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:53:23.794-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:23.794-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:23.795-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:23.944-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO CodeGenerator: Code generated in 86.787581 ms
[2023-07-13T22:53:23.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-13T22:53:23.957-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-13T22:53:23.957-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:46285 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:53:23.958-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:23.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33686118 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:24.015-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:24.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:24.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:24.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:24.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:24.017-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:24.037-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.3 KiB, free 433.7 MiB)
[2023-07-13T22:53:24.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.6 MiB)
[2023-07-13T22:53:24.041-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:46285 (size: 83.4 KiB, free: 434.2 MiB)
[2023-07-13T22:53:24.041-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:24.042-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:24.042-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:53:24.045-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:24.045-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:53:24.085-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:24.085-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:24.086-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:24.118-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:24.121-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:46285 in memory (size: 34.0 KiB, free: 434.3 MiB)
[2023-07-13T22:53:24.126-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:46285 in memory (size: 7.0 KiB, free: 434.3 MiB)
[2023-07-13T22:53:24.144-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO CodeGenerator: Code generated in 20.781548 ms
[2023-07-13T22:53:24.160-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO CodeGenerator: Code generated in 3.564988 ms
[2023-07-13T22:53:24.182-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:53:24.190-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [19549]
[2023-07-13T22:53:24.197-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:24.202-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:24.208-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:24.212-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:24.216-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [19550]
[2023-07-13T22:53:24.227-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253233450661753688983704_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-11/_temporary/0/task_202307132253233450661753688983704_0001_m_000000
[2023-07-13T22:53:24.228-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkHadoopMapRedUtil: attempt_202307132253233450661753688983704_0001_m_000000_1: Committed. Elapsed time: 2 ms.
[2023-07-13T22:53:24.232-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:53:24.234-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 191 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:24.234-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:53:24.235-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,217 s
[2023-07-13T22:53:24.236-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:24.236-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:53:24.237-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,221689 s
[2023-07-13T22:53:24.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileFormatWriter: Start to commit write Job f6974019-049a-4503-87a6-a010ee077483.
[2023-07-13T22:53:24.250-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileFormatWriter: Write Job f6974019-049a-4503-87a6-a010ee077483 committed. Elapsed time: 10 ms.
[2023-07-13T22:53:24.253-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileFormatWriter: Finished processing stats for write job f6974019-049a-4503-87a6-a010ee077483.
[2023-07-13T22:53:24.282-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:24.282-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:24.282-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:24.283-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:53:24.291-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:24.292-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:24.292-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:24.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO CodeGenerator: Code generated in 9.614538 ms
[2023-07-13T22:53:24.318-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:53:24.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:53:24.328-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:46285 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:53:24.328-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:24.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33686118 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:24.360-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:24.361-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:24.361-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:24.361-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:24.361-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:24.362-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:24.380-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:53:24.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:53:24.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:46285 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:53:24.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:24.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:24.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:53:24.385-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:24.385-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:53:24.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:24.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:24.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:24.409-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:24.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO CodeGenerator: Code generated in 9.024203 ms
[2023-07-13T22:53:24.426-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:53:24.430-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [19549]
[2023-07-13T22:53:24.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:24.437-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:24.439-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:24.442-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:24.444-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [19550]
[2023-07-13T22:53:24.448-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253243951813710327638598_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-11/_temporary/0/task_202307132253243951813710327638598_0002_m_000000
[2023-07-13T22:53:24.448-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkHadoopMapRedUtil: attempt_202307132253243951813710327638598_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:53:24.449-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:53:24.451-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 67 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:24.451-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:53:24.452-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,088 s
[2023-07-13T22:53:24.452-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:24.452-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:53:24.452-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,091736 s
[2023-07-13T22:53:24.453-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileFormatWriter: Start to commit write Job f5057508-3082-43bf-aef7-271b9cfbf0b3.
[2023-07-13T22:53:24.460-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileFormatWriter: Write Job f5057508-3082-43bf-aef7-271b9cfbf0b3 committed. Elapsed time: 6 ms.
[2023-07-13T22:53:24.460-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO FileFormatWriter: Finished processing stats for write job f5057508-3082-43bf-aef7-271b9cfbf0b3.
[2023-07-13T22:53:24.481-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:53:24.489-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:53:24.496-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:53:24.503-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:53:24.503-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO BlockManager: BlockManager stopped
[2023-07-13T22:53:24.506-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:53:24.508-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:53:24.511-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:53:24.511-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:53:24.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed4e67b6-bca0-4fe9-b84b-b3711765105f
[2023-07-13T22:53:24.514-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-f77616b4-081f-460d-80a3-ada6ac11af29
[2023-07-13T22:53:24.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-f77616b4-081f-460d-80a3-ada6ac11af29/pyspark-dd4337a1-49cf-4fd0-82d6-9a2f0d606e97
[2023-07-13T22:53:24.556-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T015317, end_date=20230714T015324
[2023-07-13T22:53:24.603-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:53:24.608-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T15:27:01.918-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T15:27:01.929-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T15:27:01.930-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T15:27:01.947-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T15:27:01.955-0300] {standard_task_runner.py:57} INFO - Started process 15335 to run task
[2023-07-14T15:27:01.958-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '48', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmprfny1lvo']
[2023-07-14T15:27:01.959-0300] {standard_task_runner.py:85} INFO - Job 48: Subtask transform_twitter_datascience
[2023-07-14T15:27:02.036-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T15:27:02.180-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T15:27:02.191-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T15:27:02.194-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11 --dest /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-11 --process-date 2023-07-11
[2023-07-14T15:27:05.454-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:05 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T15:27:05.457-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T15:27:07.108-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T15:27:07.207-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T15:27:07.378-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO ResourceUtils: ==============================================================
[2023-07-14T15:27:07.379-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T15:27:07.380-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO ResourceUtils: ==============================================================
[2023-07-14T15:27:07.381-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T15:27:07.428-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T15:27:07.448-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T15:27:07.449-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T15:27:07.532-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T15:27:07.534-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T15:27:07.537-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T15:27:07.538-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T15:27:07.539-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T15:27:08.051-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO Utils: Successfully started service 'sparkDriver' on port 35611.
[2023-07-14T15:27:08.133-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T15:27:08.247-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T15:27:08.309-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T15:27:08.313-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T15:27:08.337-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T15:27:08.411-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2aa84a24-802f-4d66-8169-33074ddc1c27
[2023-07-14T15:27:08.449-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T15:27:08.490-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T15:27:08.909-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T15:27:09.316-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T15:27:09.355-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T15:27:09.450-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35417.
[2023-07-14T15:27:09.452-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO NettyBlockTransferService: Server created on 192.168.0.102:35417
[2023-07-14T15:27:09.462-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T15:27:09.499-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 35417, None)
[2023-07-14T15:27:09.511-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:35417 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 35417, None)
[2023-07-14T15:27:09.523-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 35417, None)
[2023-07-14T15:27:09.529-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 35417, None)
[2023-07-14T15:27:10.638-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T15:27:10.647-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:10 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T15:27:12.205-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:12 INFO InMemoryFileIndex: It took 75 ms to list leaf files for 1 paths.
[2023-07-14T15:27:12.322-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:12 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
[2023-07-14T15:27:15.806-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:15 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:27:15.808-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:27:15.811-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T15:27:16.321-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T15:27:16.399-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T15:27:16.405-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:35417 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:27:16.410-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:16.425-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198888 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:27:16.680-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:16.708-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:27:16.708-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:27:16.708-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:27:16.710-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:27:16.716-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:27:16.900-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T15:27:16.903-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T15:27:16.904-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:35417 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:27:16.905-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:27:16.920-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:27:16.921-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T15:27:16.973-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5007 bytes) taskResourceAssignments Map()
[2023-07-14T15:27:16.992-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T15:27:17.231-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4584, partition values: [empty row]
[2023-07-14T15:27:17.721-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO CodeGenerator: Code generated in 403.486493 ms
[2023-07-14T15:27:17.828-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-07-14T15:27:17.858-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 893 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:27:17.861-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T15:27:17.875-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,134 s
[2023-07-14T15:27:17.882-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:27:17.884-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T15:27:17.890-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:17 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,209620 s
[2023-07-14T15:27:18.548-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:35417 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:27:18.561-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:35417 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:27:18.607-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T15:27:18.609-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:18 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T15:27:18.610-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:18 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T15:27:18.737-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:27:18.737-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:27:18.738-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:27:19.058-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO CodeGenerator: Code generated in 152.336331 ms
[2023-07-14T15:27:19.069-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T15:27:19.087-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T15:27:19.088-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:35417 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T15:27:19.089-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:19.092-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198888 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:27:19.198-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:19.204-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:27:19.204-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:27:19.204-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:27:19.205-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:27:19.206-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:27:19.239-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T15:27:19.243-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T15:27:19.244-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:35417 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T15:27:19.245-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:27:19.246-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:27:19.246-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T15:27:19.255-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:27:19.256-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T15:27:19.361-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:27:19.362-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:27:19.364-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:27:19.414-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4584, partition values: [empty row]
[2023-07-14T15:27:19.455-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO CodeGenerator: Code generated in 32.334864 ms
[2023-07-14T15:27:19.487-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO CodeGenerator: Code generated in 9.765043 ms
[2023-07-14T15:27:19.536-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_202307141527197617434881386333201_0001_m_000000_1' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-11/tweet/process_date=2023-07-11/_temporary/0/task_202307141527197617434881386333201_0001_m_000000
[2023-07-14T15:27:19.538-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SparkHadoopMapRedUtil: attempt_202307141527197617434881386333201_0001_m_000000_1: Committed. Elapsed time: 4 ms.
[2023-07-14T15:27:19.547-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T15:27:19.553-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 304 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:27:19.554-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T15:27:19.556-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,348 s
[2023-07-14T15:27:19.556-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:27:19.556-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T15:27:19.557-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,358717 s
[2023-07-14T15:27:19.560-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileFormatWriter: Start to commit write Job 29220abd-bf2e-4186-8633-fdbd0b93a1a8.
[2023-07-14T15:27:19.580-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileFormatWriter: Write Job 29220abd-bf2e-4186-8633-fdbd0b93a1a8 committed. Elapsed time: 19 ms.
[2023-07-14T15:27:19.588-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileFormatWriter: Finished processing stats for write job 29220abd-bf2e-4186-8633-fdbd0b93a1a8.
[2023-07-14T15:27:19.640-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:27:19.641-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:27:19.641-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T15:27:19.657-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:27:19.657-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:27:19.658-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:27:19.727-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO CodeGenerator: Code generated in 32.279479 ms
[2023-07-14T15:27:19.738-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T15:27:19.759-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T15:27:19.760-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:35417 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T15:27:19.761-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:19.764-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198888 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:27:19.809-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:35417 in memory (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T15:27:19.815-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:35417 in memory (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T15:27:19.819-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:27:19.823-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:27:19.823-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:27:19.823-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:27:19.823-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:27:19.825-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:27:19.852-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 434.0 MiB)
[2023-07-14T15:27:19.859-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.9 MiB)
[2023-07-14T15:27:19.860-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:35417 (size: 76.9 KiB, free: 434.3 MiB)
[2023-07-14T15:27:19.860-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:27:19.861-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:27:19.861-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T15:27:19.865-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:27:19.866-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T15:27:19.901-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:27:19.902-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:27:19.905-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:27:19.932-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4584, partition values: [empty row]
[2023-07-14T15:27:19.959-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO CodeGenerator: Code generated in 19.709281 ms
[2023-07-14T15:27:19.975-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_202307141527192082443933684643572_0002_m_000000_2' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-11/user/process_date=2023-07-11/_temporary/0/task_202307141527192082443933684643572_0002_m_000000
[2023-07-14T15:27:19.975-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO SparkHadoopMapRedUtil: attempt_202307141527192082443933684643572_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-14T15:27:19.977-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T15:27:19.979-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 115 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:27:19.980-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T15:27:19.981-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,154 s
[2023-07-14T15:27:19.981-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:27:19.982-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T15:27:19.983-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,162553 s
[2023-07-14T15:27:19.985-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:19 INFO FileFormatWriter: Start to commit write Job 608c7c82-89af-473c-afff-b16f1b230fff.
[2023-07-14T15:27:20.004-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO FileFormatWriter: Write Job 608c7c82-89af-473c-afff-b16f1b230fff committed. Elapsed time: 19 ms.
[2023-07-14T15:27:20.005-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO FileFormatWriter: Finished processing stats for write job 608c7c82-89af-473c-afff-b16f1b230fff.
[2023-07-14T15:27:20.065-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T15:27:20.090-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T15:27:20.108-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T15:27:20.122-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO MemoryStore: MemoryStore cleared
[2023-07-14T15:27:20.122-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO BlockManager: BlockManager stopped
[2023-07-14T15:27:20.125-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T15:27:20.131-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T15:27:20.139-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T15:27:20.140-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T15:27:20.140-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2ec60bd-6a84-469f-b9c4-87390648ee51
[2023-07-14T15:27:20.143-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-1eccf857-a112-47c4-a48d-50bf69fb189f
[2023-07-14T15:27:20.149-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-1eccf857-a112-47c4-a48d-50bf69fb189f/pyspark-ff21964d-0010-4a2b-8495-d3cdda6ed9fa
[2023-07-14T15:27:20.238-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T182701, end_date=20230714T182720
[2023-07-14T15:27:20.276-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T15:27:20.306-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:11:49.955-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T16:11:49.962-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T16:11:49.962-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:11:49.976-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T16:11:49.979-0300] {standard_task_runner.py:57} INFO - Started process 23484 to run task
[2023-07-14T16:11:49.982-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp5y_0romq']
[2023-07-14T16:11:49.983-0300] {standard_task_runner.py:85} INFO - Job 50: Subtask transform_twitter_datascience
[2023-07-14T16:11:50.021-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:11:50.097-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T16:11:50.102-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T16:11:50.104-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-11 --process-date 2023-07-11
[2023-07-14T16:11:51.955-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:51 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T16:11:51.958-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T16:11:53.224-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T16:11:53.323-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T16:11:53.472-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO ResourceUtils: ==============================================================
[2023-07-14T16:11:53.473-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T16:11:53.474-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO ResourceUtils: ==============================================================
[2023-07-14T16:11:53.475-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T16:11:53.522-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T16:11:53.546-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T16:11:53.548-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T16:11:53.626-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T16:11:53.627-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T16:11:53.627-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T16:11:53.627-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T16:11:53.627-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T16:11:53.923-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO Utils: Successfully started service 'sparkDriver' on port 41367.
[2023-07-14T16:11:53.958-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:53 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T16:11:54.003-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T16:11:54.026-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T16:11:54.026-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T16:11:54.029-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T16:11:54.057-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-59a3e968-6def-4491-be73-d9c3c40a993b
[2023-07-14T16:11:54.078-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T16:11:54.103-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T16:11:54.390-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T16:11:54.522-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T16:11:54.531-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T16:11:54.559-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37437.
[2023-07-14T16:11:54.560-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO NettyBlockTransferService: Server created on 192.168.0.102:37437
[2023-07-14T16:11:54.561-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T16:11:54.574-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 37437, None)
[2023-07-14T16:11:54.579-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:37437 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 37437, None)
[2023-07-14T16:11:54.582-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 37437, None)
[2023-07-14T16:11:54.585-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 37437, None)
[2023-07-14T16:11:55.148-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T16:11:55.159-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:55 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T16:11:56.314-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:56 INFO InMemoryFileIndex: It took 58 ms to list leaf files for 1 paths.
[2023-07-14T16:11:56.396-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:56 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2023-07-14T16:11:58.876-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:58 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:11:58.877-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:58 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:11:58.883-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:58 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T16:11:59.243-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T16:11:59.304-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T16:11:59.309-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:37437 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:59.314-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:59.328-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207800 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:11:59.558-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:59.578-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:11:59.578-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:11:59.578-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:11:59.579-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:11:59.586-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:11:59.745-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T16:11:59.748-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T16:11:59.749-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:37437 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:59.751-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:11:59.769-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:11:59.772-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T16:11:59.834-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T16:11:59.855-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T16:12:00.004-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13496, partition values: [empty row]
[2023-07-14T16:12:00.241-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO CodeGenerator: Code generated in 183.674393 ms
[2023-07-14T16:12:00.316-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T16:12:00.328-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 506 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:00.331-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T16:12:00.342-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,734 s
[2023-07-14T16:12:00.346-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:00.348-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T16:12:00.351-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,792269 s
[2023-07-14T16:12:00.839-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T16:12:00.840-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T16:12:00.841-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T16:12:00.923-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:00.923-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:00.926-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:01.065-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO CodeGenerator: Code generated in 74.056983 ms
[2023-07-14T16:12:01.070-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-14T16:12:01.079-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-14T16:12:01.079-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:37437 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T16:12:01.080-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:01.082-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207800 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:12:01.140-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:37437 in memory (size: 7.0 KiB, free: 434.3 MiB)
[2023-07-14T16:12:01.144-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:37437 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:01.163-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:01.165-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:12:01.165-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:12:01.165-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:12:01.165-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:12:01.166-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:12:01.186-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T16:12:01.188-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T16:12:01.189-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:37437 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T16:12:01.190-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:12:01.191-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:12:01.191-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T16:12:01.195-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:12:01.196-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T16:12:01.243-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:01.243-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:01.245-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:01.285-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13496, partition values: [empty row]
[2023-07-14T16:12:01.316-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO CodeGenerator: Code generated in 27.058352 ms
[2023-07-14T16:12:01.340-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO CodeGenerator: Code generated in 6.394561 ms
[2023-07-14T16:12:01.381-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileOutputCommitter: Saved output of task 'attempt_202307141612017614078473091384676_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-11/tweet/process_date=2023-07-11/_temporary/0/task_202307141612017614078473091384676_0001_m_000000
[2023-07-14T16:12:01.382-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkHadoopMapRedUtil: attempt_202307141612017614078473091384676_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T16:12:01.390-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T16:12:01.392-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 200 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:01.392-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T16:12:01.393-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,226 s
[2023-07-14T16:12:01.394-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:01.394-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T16:12:01.395-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,230991 s
[2023-07-14T16:12:01.397-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileFormatWriter: Start to commit write Job 3ebd1e41-bac0-4a76-9eac-754d7909ee1d.
[2023-07-14T16:12:01.409-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileFormatWriter: Write Job 3ebd1e41-bac0-4a76-9eac-754d7909ee1d committed. Elapsed time: 11 ms.
[2023-07-14T16:12:01.412-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileFormatWriter: Finished processing stats for write job 3ebd1e41-bac0-4a76-9eac-754d7909ee1d.
[2023-07-14T16:12:01.444-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:12:01.444-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:12:01.445-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T16:12:01.452-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:01.452-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:01.452-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:01.489-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO CodeGenerator: Code generated in 15.14942 ms
[2023-07-14T16:12:01.495-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T16:12:01.507-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T16:12:01.508-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:37437 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T16:12:01.509-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:01.511-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207800 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:12:01.542-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:01.546-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:12:01.547-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:12:01.547-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:12:01.547-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:12:01.547-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:12:01.570-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T16:12:01.573-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T16:12:01.574-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:37437 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T16:12:01.575-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:12:01.575-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:12:01.575-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T16:12:01.577-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:12:01.577-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T16:12:01.594-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:01.594-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:01.595-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:01.608-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13496, partition values: [empty row]
[2023-07-14T16:12:01.620-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO CodeGenerator: Code generated in 9.835465 ms
[2023-07-14T16:12:01.628-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileOutputCommitter: Saved output of task 'attempt_202307141612018488080476595759532_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-11/user/process_date=2023-07-11/_temporary/0/task_202307141612018488080476595759532_0002_m_000000
[2023-07-14T16:12:01.629-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkHadoopMapRedUtil: attempt_202307141612018488080476595759532_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-14T16:12:01.630-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T16:12:01.631-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 55 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:01.631-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T16:12:01.632-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,084 s
[2023-07-14T16:12:01.632-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:01.632-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T16:12:01.633-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,090323 s
[2023-07-14T16:12:01.633-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileFormatWriter: Start to commit write Job 3fc2a707-7601-4d32-99d9-8a4e03437acd.
[2023-07-14T16:12:01.643-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileFormatWriter: Write Job 3fc2a707-7601-4d32-99d9-8a4e03437acd committed. Elapsed time: 9 ms.
[2023-07-14T16:12:01.644-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO FileFormatWriter: Finished processing stats for write job 3fc2a707-7601-4d32-99d9-8a4e03437acd.
[2023-07-14T16:12:01.672-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T16:12:01.680-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T16:12:01.688-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T16:12:01.695-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO MemoryStore: MemoryStore cleared
[2023-07-14T16:12:01.695-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO BlockManager: BlockManager stopped
[2023-07-14T16:12:01.698-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T16:12:01.700-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T16:12:01.703-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T16:12:01.703-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T16:12:01.704-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-4f2fddb3-1085-4e9f-9440-60541d74d5c6
[2023-07-14T16:12:01.706-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b469004-423a-487f-b9ee-fa7ee6c1c188/pyspark-bdd0fa40-9ebc-42ba-9b12-73e11cd33688
[2023-07-14T16:12:01.708-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b469004-423a-487f-b9ee-fa7ee6c1c188
[2023-07-14T16:12:01.775-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T191149, end_date=20230714T191201
[2023-07-14T16:12:01.792-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:12:01.799-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:30:39.008-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T17:30:39.017-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T17:30:39.017-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:30:39.028-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T17:30:39.035-0300] {standard_task_runner.py:57} INFO - Started process 36384 to run task
[2023-07-14T17:30:39.038-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '60', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp6pi62at3']
[2023-07-14T17:30:39.039-0300] {standard_task_runner.py:85} INFO - Job 60: Subtask transform_twitter_datascience
[2023-07-14T17:30:39.086-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:30:39.167-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T17:30:39.177-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T17:30:39.180-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-11
[2023-07-14T17:30:42.103-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:42 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T17:30:42.106-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T17:30:43.367-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T17:30:43.485-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T17:30:43.653-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO ResourceUtils: ==============================================================
[2023-07-14T17:30:43.653-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T17:30:43.653-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO ResourceUtils: ==============================================================
[2023-07-14T17:30:43.654-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T17:30:43.686-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T17:30:43.703-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T17:30:43.704-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T17:30:43.770-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T17:30:43.771-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T17:30:43.771-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T17:30:43.771-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T17:30:43.772-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T17:30:44.081-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO Utils: Successfully started service 'sparkDriver' on port 44139.
[2023-07-14T17:30:44.121-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T17:30:44.176-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T17:30:44.201-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T17:30:44.202-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T17:30:44.205-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T17:30:44.233-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-afc82d7c-cf69-4b6d-b189-65f14d2a0a8f
[2023-07-14T17:30:44.253-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T17:30:44.274-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T17:30:44.572-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T17:30:44.586-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T17:30:44.727-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T17:30:44.738-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T17:30:44.771-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41033.
[2023-07-14T17:30:44.771-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO NettyBlockTransferService: Server created on 192.168.0.102:41033
[2023-07-14T17:30:44.773-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T17:30:44.785-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 41033, None)
[2023-07-14T17:30:44.790-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:41033 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 41033, None)
[2023-07-14T17:30:44.794-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 41033, None)
[2023-07-14T17:30:44.799-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 41033, None)
[2023-07-14T17:30:45.353-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T17:30:45.364-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:45 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T17:30:46.421-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:46 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.
[2023-07-14T17:30:46.520-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-07-14T17:30:49.222-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:30:49.223-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:30:49.226-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T17:30:49.599-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T17:30:49.656-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T17:30:49.660-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:41033 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:49.668-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:49.680-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203417 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:49.888-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:49.919-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:49.920-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:49.920-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:49.922-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:49.927-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:50.089-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T17:30:50.094-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T17:30:50.098-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:41033 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:50.099-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:50.121-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:50.122-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T17:30:50.186-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:50.203-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T17:30:50.355-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9113, partition values: [empty row]
[2023-07-14T17:30:50.593-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO CodeGenerator: Code generated in 182.359452 ms
[2023-07-14T17:30:50.667-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T17:30:50.673-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 501 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:50.677-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T17:30:50.686-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,733 s
[2023-07-14T17:30:50.689-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:50.690-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T17:30:50.697-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:50 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,806773 s
[2023-07-14T17:30:51.157-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:41033 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:51.171-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:41033 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:51.306-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T17:30:51.309-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T17:30:51.310-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T17:30:51.412-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:51.413-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:51.415-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:51.613-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO CodeGenerator: Code generated in 106.675742 ms
[2023-07-14T17:30:51.620-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T17:30:51.633-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T17:30:51.634-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:41033 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T17:30:51.635-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:51.639-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203417 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:51.739-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:51.741-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:51.741-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:51.741-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:51.742-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:51.744-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:51.777-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T17:30:51.785-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T17:30:51.785-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:41033 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T17:30:51.786-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:51.786-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:51.787-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T17:30:51.791-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:51.792-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T17:30:51.862-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:51.864-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:51.866-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:51.904-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9113, partition values: [empty row]
[2023-07-14T17:30:51.937-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO CodeGenerator: Code generated in 29.540817 ms
[2023-07-14T17:30:51.969-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:51 INFO CodeGenerator: Code generated in 8.454359 ms
[2023-07-14T17:30:52.017-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileOutputCommitter: Saved output of task 'attempt_20230714173051128554817108709791_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-11/_temporary/0/task_20230714173051128554817108709791_0001_m_000000
[2023-07-14T17:30:52.018-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SparkHadoopMapRedUtil: attempt_20230714173051128554817108709791_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T17:30:52.027-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T17:30:52.033-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 244 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:52.034-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T17:30:52.035-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,288 s
[2023-07-14T17:30:52.035-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:52.035-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T17:30:52.036-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,296536 s
[2023-07-14T17:30:52.038-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileFormatWriter: Start to commit write Job 65fc278b-521b-48b2-bcd5-6b5016800da6.
[2023-07-14T17:30:52.057-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileFormatWriter: Write Job 65fc278b-521b-48b2-bcd5-6b5016800da6 committed. Elapsed time: 18 ms.
[2023-07-14T17:30:52.062-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileFormatWriter: Finished processing stats for write job 65fc278b-521b-48b2-bcd5-6b5016800da6.
[2023-07-14T17:30:52.109-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:30:52.109-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:30:52.110-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T17:30:52.126-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:52.126-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:52.127-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:52.179-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:41033 in memory (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:30:52.186-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:41033 in memory (size: 82.1 KiB, free: 434.4 MiB)
[2023-07-14T17:30:52.204-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO CodeGenerator: Code generated in 25.273816 ms
[2023-07-14T17:30:52.207-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T17:30:52.220-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T17:30:52.221-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:41033 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T17:30:52.222-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:52.224-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203417 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:52.252-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:52.253-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:52.253-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:52.254-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:52.254-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:52.255-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:52.285-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 434.0 MiB)
[2023-07-14T17:30:52.288-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.9 MiB)
[2023-07-14T17:30:52.288-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:41033 (size: 76.9 KiB, free: 434.3 MiB)
[2023-07-14T17:30:52.289-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:52.289-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:52.289-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T17:30:52.292-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:52.293-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T17:30:52.317-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:52.317-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:52.318-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:52.341-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9113, partition values: [empty row]
[2023-07-14T17:30:52.361-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO CodeGenerator: Code generated in 15.240585 ms
[2023-07-14T17:30:52.376-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileOutputCommitter: Saved output of task 'attempt_202307141730522048501707537242272_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-11/_temporary/0/task_202307141730522048501707537242272_0002_m_000000
[2023-07-14T17:30:52.376-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SparkHadoopMapRedUtil: attempt_202307141730522048501707537242272_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-07-14T17:30:52.378-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T17:30:52.384-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 92 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:52.384-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T17:30:52.385-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,130 s
[2023-07-14T17:30:52.386-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:52.386-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T17:30:52.386-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,133830 s
[2023-07-14T17:30:52.387-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileFormatWriter: Start to commit write Job fa18ca3e-f92e-4b97-909e-c776d0468a0c.
[2023-07-14T17:30:52.402-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileFormatWriter: Write Job fa18ca3e-f92e-4b97-909e-c776d0468a0c committed. Elapsed time: 15 ms.
[2023-07-14T17:30:52.403-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO FileFormatWriter: Finished processing stats for write job fa18ca3e-f92e-4b97-909e-c776d0468a0c.
[2023-07-14T17:30:52.465-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T17:30:52.482-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T17:30:52.495-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T17:30:52.506-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO MemoryStore: MemoryStore cleared
[2023-07-14T17:30:52.506-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO BlockManager: BlockManager stopped
[2023-07-14T17:30:52.511-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T17:30:52.519-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T17:30:52.522-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T17:30:52.522-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T17:30:52.522-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-08fac7d6-a552-40c1-9856-3fb94c3e541d
[2023-07-14T17:30:52.525-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0c6969d-2cdd-4f66-b42d-a9213ad98fe0/pyspark-5951301b-a40e-44f2-807c-8d01e949a290
[2023-07-14T17:30:52.529-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0c6969d-2cdd-4f66-b42d-a9213ad98fe0
[2023-07-14T17:30:52.600-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230714T203039, end_date=20230714T203052
[2023-07-14T17:30:52.651-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:30:52.676-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:44:20.929-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T23:44:20.938-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T23:44:20.938-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:44:20.955-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T23:44:20.959-0300] {standard_task_runner.py:57} INFO - Started process 25076 to run task
[2023-07-14T23:44:20.963-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '77', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpj04_3jub']
[2023-07-14T23:44:20.964-0300] {standard_task_runner.py:85} INFO - Job 77: Subtask transform_twitter_datascience
[2023-07-14T23:44:21.011-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:44:21.088-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T23:44:21.093-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T23:44:21.095-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-11
[2023-07-14T23:44:22.748-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:22 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-14T23:44:22.749-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T23:44:22.762-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-14T23:44:22.763-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-14T23:44:22.763-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-14T23:44:22.763-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-14T23:44:22.763-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-14T23:44:23.122-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T23:44:23.842-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-14T23:44:23.863-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:23 INFO SparkContext: Running Spark version 3.1.3
[2023-07-14T23:44:23.960-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:23 INFO ResourceUtils: ==============================================================
[2023-07-14T23:44:23.961-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:23 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T23:44:23.962-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:23 INFO ResourceUtils: ==============================================================
[2023-07-14T23:44:23.963-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:23 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T23:44:24.016-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T23:44:24.049-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T23:44:24.051-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T23:44:24.124-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T23:44:24.125-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T23:44:24.125-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T23:44:24.125-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T23:44:24.125-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T23:44:24.372-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO Utils: Successfully started service 'sparkDriver' on port 46097.
[2023-07-14T23:44:24.394-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T23:44:24.423-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T23:44:24.440-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T23:44:24.440-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T23:44:24.444-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T23:44:24.457-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8847da76-21eb-4816-8d24-3a1267ce589b
[2023-07-14T23:44:24.478-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T23:44:24.494-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T23:44:24.824-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T23:44:24.832-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T23:44:24.897-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4041
[2023-07-14T23:44:25.116-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-14T23:44:25.152-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36565.
[2023-07-14T23:44:25.152-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO NettyBlockTransferService: Server created on 192.168.0.177:36565
[2023-07-14T23:44:25.158-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T23:44:25.172-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 36565, None)
[2023-07-14T23:44:25.177-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:36565 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 36565, None)
[2023-07-14T23:44:25.184-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 36565, None)
[2023-07-14T23:44:25.188-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 36565, None)
[2023-07-14T23:44:25.635-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-14T23:44:25.636-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:25 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T23:44:26.576-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:26 INFO InMemoryFileIndex: It took 121 ms to list leaf files for 1 paths.
[2023-07-14T23:44:26.725-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:26 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
[2023-07-14T23:44:28.640-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:28 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:44:28.641-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:44:28.644-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T23:44:28.914-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-14T23:44:28.962-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-14T23:44:28.964-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:36565 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-14T23:44:28.968-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:28 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:28.977-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198774 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:44:29.118-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:29.137-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:44:29.138-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:44:29.138-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:44:29.141-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:44:29.146-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:44:29.264-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-14T23:44:29.267-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-14T23:44:29.269-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:36565 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:44:29.270-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:44:29.292-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:44:29.294-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T23:44:29.352-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-14T23:44:29.366-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T23:44:29.494-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4470, partition values: [empty row]
[2023-07-14T23:44:29.728-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO CodeGenerator: Code generated in 141.150419 ms
[2023-07-14T23:44:29.783-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2023-07-14T23:44:29.792-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 447 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:44:29.796-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T23:44:29.802-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,640 s
[2023-07-14T23:44:29.806-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:44:29.807-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T23:44:29.812-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:29 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,692763 s
[2023-07-14T23:44:30.234-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T23:44:30.235-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-14T23:44:30.235-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T23:44:30.343-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:30.343-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:30.344-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:30.429-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO CodeGenerator: Code generated in 36.644011 ms
[2023-07-14T23:44:30.494-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO CodeGenerator: Code generated in 44.866269 ms
[2023-07-14T23:44:30.502-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-14T23:44:30.514-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)
[2023-07-14T23:44:30.515-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:36565 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-14T23:44:30.516-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:30.518-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198774 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:44:30.610-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:36565 in memory (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-14T23:44:30.613-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:30.615-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:44:30.616-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:44:30.616-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:44:30.616-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:36565 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:44:30.616-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:44:30.617-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:44:30.664-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 434.0 MiB)
[2023-07-14T23:44:30.666-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 434.0 MiB)
[2023-07-14T23:44:30.667-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:36565 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-14T23:44:30.668-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:44:30.669-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:44:30.669-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T23:44:30.673-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:44:30.675-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T23:44:30.734-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:30.734-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:30.735-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:30.811-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO CodeGenerator: Code generated in 26.696181 ms
[2023-07-14T23:44:30.815-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4470, partition values: [empty row]
[2023-07-14T23:44:30.852-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO CodeGenerator: Code generated in 33.212758 ms
[2023-07-14T23:44:30.878-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO CodeGenerator: Code generated in 7.518013 ms
[2023-07-14T23:44:30.916-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileOutputCommitter: Saved output of task 'attempt_202307142344302180610670097462230_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-11/_temporary/0/task_202307142344302180610670097462230_0001_m_000000
[2023-07-14T23:44:30.917-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO SparkHadoopMapRedUtil: attempt_202307142344302180610670097462230_0001_m_000000_1: Committed
[2023-07-14T23:44:30.922-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2023-07-14T23:44:30.926-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 256 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:44:30.926-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T23:44:30.927-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,309 s
[2023-07-14T23:44:30.927-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:44:30.927-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T23:44:30.928-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,314204 s
[2023-07-14T23:44:30.943-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileFormatWriter: Write Job 37b160f2-2a85-4f50-85d1-0946d150e9ba committed.
[2023-07-14T23:44:30.946-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileFormatWriter: Finished processing stats for write job 37b160f2-2a85-4f50-85d1-0946d150e9ba.
[2023-07-14T23:44:30.988-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:44:30.989-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:44:30.989-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:30 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T23:44:31.004-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:31.005-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:31.005-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:31.031-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO CodeGenerator: Code generated in 10.926404 ms
[2023-07-14T23:44:31.035-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-14T23:44:31.047-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.8 MiB)
[2023-07-14T23:44:31.048-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:36565 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-14T23:44:31.049-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:31.050-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198774 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:44:31.076-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:44:31.078-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:44:31.078-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:44:31.078-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:44:31.078-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:44:31.080-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:44:31.107-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.6 MiB)
[2023-07-14T23:44:31.111-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.5 MiB)
[2023-07-14T23:44:31.113-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:36565 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-14T23:44:31.113-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:44:31.114-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:44:31.115-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T23:44:31.116-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:44:31.117-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T23:44:31.146-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:44:31.149-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:44:31.150-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:44:31.189-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO CodeGenerator: Code generated in 16.450593 ms
[2023-07-14T23:44:31.195-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4470, partition values: [empty row]
[2023-07-14T23:44:31.217-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO CodeGenerator: Code generated in 17.798571 ms
[2023-07-14T23:44:31.238-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileOutputCommitter: Saved output of task 'attempt_202307142344313920093969456366492_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-11/_temporary/0/task_202307142344313920093969456366492_0002_m_000000
[2023-07-14T23:44:31.239-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SparkHadoopMapRedUtil: attempt_202307142344313920093969456366492_0002_m_000000_2: Committed
[2023-07-14T23:44:31.243-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-14T23:44:31.247-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 130 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:44:31.247-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T23:44:31.252-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,168 s
[2023-07-14T23:44:31.252-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:44:31.252-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T23:44:31.252-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,174540 s
[2023-07-14T23:44:31.272-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileFormatWriter: Write Job 5ffb5528-e09e-4d63-8310-926d92f9f7a4 committed.
[2023-07-14T23:44:31.272-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO FileFormatWriter: Finished processing stats for write job 5ffb5528-e09e-4d63-8310-926d92f9f7a4.
[2023-07-14T23:44:31.355-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T23:44:31.370-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4041
[2023-07-14T23:44:31.389-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T23:44:31.400-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO MemoryStore: MemoryStore cleared
[2023-07-14T23:44:31.400-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO BlockManager: BlockManager stopped
[2023-07-14T23:44:31.403-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T23:44:31.407-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T23:44:31.412-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T23:44:31.412-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T23:44:31.413-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f898e57-a03d-4eee-882f-b2e11baca603/pyspark-733fa00f-8153-4814-9aa8-53577a7cb188
[2023-07-14T23:44:31.415-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f898e57-a03d-4eee-882f-b2e11baca603
[2023-07-14T23:44:31.417-0300] {spark_submit.py:492} INFO - 23/07/14 23:44:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-79fd8069-373a-4970-9390-83d421c277b0
[2023-07-14T23:44:31.469-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230715T024420, end_date=20230715T024431
[2023-07-14T23:44:31.517-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:44:31.540-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-15T12:13:32.476-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-15T12:13:32.496-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-15T12:13:32.497-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-15T12:13:32.527-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-15T12:13:32.534-0300] {standard_task_runner.py:57} INFO - Started process 13929 to run task
[2023-07-15T12:13:32.541-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '98', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpysot_ip2']
[2023-07-15T12:13:32.543-0300] {standard_task_runner.py:85} INFO - Job 98: Subtask transform_twitter_datascience
[2023-07-15T12:13:32.628-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-15T12:13:32.757-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-15T12:13:32.763-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-15T12:13:32.766-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-11
[2023-07-15T12:13:35.660-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:35 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-15T12:13:35.660-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-15T12:13:35.759-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-15T12:13:35.759-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-15T12:13:35.760-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-15T12:13:35.760-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-15T12:13:35.760-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-15T12:13:36.762-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-15T12:13:37.996-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-15T12:13:38.035-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO SparkContext: Running Spark version 3.1.3
[2023-07-15T12:13:38.197-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO ResourceUtils: ==============================================================
[2023-07-15T12:13:38.198-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-15T12:13:38.202-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO ResourceUtils: ==============================================================
[2023-07-15T12:13:38.203-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-15T12:13:38.269-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-15T12:13:38.312-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO ResourceProfile: Limiting resource is cpu
[2023-07-15T12:13:38.313-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-15T12:13:38.419-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO SecurityManager: Changing view acls to: lucas
[2023-07-15T12:13:38.420-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-15T12:13:38.420-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO SecurityManager: Changing view acls groups to:
[2023-07-15T12:13:38.420-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO SecurityManager: Changing modify acls groups to:
[2023-07-15T12:13:38.420-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-15T12:13:39.044-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO Utils: Successfully started service 'sparkDriver' on port 40325.
[2023-07-15T12:13:39.091-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO SparkEnv: Registering MapOutputTracker
[2023-07-15T12:13:39.155-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-15T12:13:39.183-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-15T12:13:39.184-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-15T12:13:39.192-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-15T12:13:39.213-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3484232d-3e56-4908-892c-219cb3857146
[2023-07-15T12:13:39.257-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-15T12:13:39.291-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-15T12:13:39.869-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-15T12:13:40.043-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[2023-07-15T12:13:40.500-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-15T12:13:40.567-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34171.
[2023-07-15T12:13:40.567-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO NettyBlockTransferService: Server created on 192.168.0.102:34171
[2023-07-15T12:13:40.577-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-15T12:13:40.589-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 34171, None)
[2023-07-15T12:13:40.605-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:34171 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 34171, None)
[2023-07-15T12:13:40.614-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 34171, None)
[2023-07-15T12:13:40.616-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 34171, None)
[2023-07-15T12:13:41.568-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-15T12:13:41.569-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:41 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-15T12:13:43.378-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:43 INFO InMemoryFileIndex: It took 155 ms to list leaf files for 1 paths.
[2023-07-15T12:13:43.533-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:43 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2023-07-15T12:13:47.812-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:47 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:13:47.814-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:47 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:13:47.824-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-15T12:13:48.411-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-15T12:13:48.507-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-15T12:13:48.511-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:34171 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:13:48.517-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:13:48.538-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4216862 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:13:48.834-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:13:48.868-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:13:48.869-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:13:48.871-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:13:48.878-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:13:48.890-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:13:49.096-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-15T12:13:49.106-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-15T12:13:49.109-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:34171 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-15T12:13:49.112-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:13:49.142-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:13:49.143-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-15T12:13:49.264-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-15T12:13:49.294-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-15T12:13:49.530-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:49 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22558, partition values: [empty row]
[2023-07-15T12:13:50.115-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:50 INFO CodeGenerator: Code generated in 385.514374 ms
[2023-07-15T12:13:50.224-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2023-07-15T12:13:50.242-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 997 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:13:50.246-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-15T12:13:50.259-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:50 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,332 s
[2023-07-15T12:13:50.267-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:13:50.268-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-15T12:13:50.273-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:50 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,437857 s
[2023-07-15T12:13:51.160-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-15T12:13:51.163-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-15T12:13:51.165-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-15T12:13:51.346-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:13:51.347-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:13:51.350-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:13:51.537-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO CodeGenerator: Code generated in 59.081015 ms
[2023-07-15T12:13:51.641-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO CodeGenerator: Code generated in 65.336504 ms
[2023-07-15T12:13:51.651-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-15T12:13:51.696-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.0 MiB)
[2023-07-15T12:13:51.697-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:34171 (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-15T12:13:51.700-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:13:51.712-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4216862 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:13:51.747-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:34171 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2023-07-15T12:13:51.762-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:34171 in memory (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:13:51.860-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:13:51.861-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:13:51.862-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:13:51.862-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:13:51.862-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:13:51.863-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:13:51.963-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 434.0 MiB)
[2023-07-15T12:13:51.971-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 434.0 MiB)
[2023-07-15T12:13:51.974-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:34171 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-15T12:13:51.977-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:13:51.978-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:13:51.979-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-15T12:13:51.986-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:13:51.987-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-15T12:13:52.213-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:13:52.214-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:13:52.214-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:13:52.348-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO CodeGenerator: Code generated in 49.829763 ms
[2023-07-15T12:13:52.354-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22558, partition values: [empty row]
[2023-07-15T12:13:52.427-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO CodeGenerator: Code generated in 63.031542 ms
[2023-07-15T12:13:52.483-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO CodeGenerator: Code generated in 18.990947 ms
[2023-07-15T12:13:52.592-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileOutputCommitter: Saved output of task 'attempt_202307151213511325932886785753231_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-11/_temporary/0/task_202307151213511325932886785753231_0001_m_000000
[2023-07-15T12:13:52.594-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO SparkHadoopMapRedUtil: attempt_202307151213511325932886785753231_0001_m_000000_1: Committed
[2023-07-15T12:13:52.611-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-15T12:13:52.614-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 634 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:13:52.614-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-15T12:13:52.618-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,751 s
[2023-07-15T12:13:52.618-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:13:52.619-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-15T12:13:52.619-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,758965 s
[2023-07-15T12:13:52.663-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileFormatWriter: Write Job 6c540fc9-dc41-4ce6-8504-00f2e353b79c committed.
[2023-07-15T12:13:52.673-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileFormatWriter: Finished processing stats for write job 6c540fc9-dc41-4ce6-8504-00f2e353b79c.
[2023-07-15T12:13:52.794-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:13:52.794-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:13:52.794-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-15T12:13:52.819-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:13:52.820-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:13:52.820-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:13:52.887-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO CodeGenerator: Code generated in 25.391244 ms
[2023-07-15T12:13:52.896-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-15T12:13:52.914-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 433.8 MiB)
[2023-07-15T12:13:52.916-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:34171 (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-15T12:13:52.919-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:13:52.923-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4216862 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:13:52.970-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:13:52.972-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:13:52.973-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:13:52.973-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:13:52.973-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:13:52.976-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:52 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:13:53.026-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.6 MiB)
[2023-07-15T12:13:53.029-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.5 MiB)
[2023-07-15T12:13:53.030-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:34171 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-15T12:13:53.033-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:13:53.036-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:13:53.036-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-15T12:13:53.039-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:13:53.040-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-15T12:13:53.078-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:13:53.078-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:13:53.079-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:13:53.141-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO CodeGenerator: Code generated in 25.420587 ms
[2023-07-15T12:13:53.146-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22558, partition values: [empty row]
[2023-07-15T12:13:53.192-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO CodeGenerator: Code generated in 38.552781 ms
[2023-07-15T12:13:53.228-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO FileOutputCommitter: Saved output of task 'attempt_202307151213525847975362628383727_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-11/_temporary/0/task_202307151213525847975362628383727_0002_m_000000
[2023-07-15T12:13:53.228-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO SparkHadoopMapRedUtil: attempt_202307151213525847975362628383727_0002_m_000000_2: Committed
[2023-07-15T12:13:53.230-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-15T12:13:53.235-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 196 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:13:53.237-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-15T12:13:53.239-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,259 s
[2023-07-15T12:13:53.241-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:13:53.242-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-15T12:13:53.243-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,271804 s
[2023-07-15T12:13:53.273-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO FileFormatWriter: Write Job c9739dc9-cdb1-40df-b081-22cd773d6f27 committed.
[2023-07-15T12:13:53.276-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO FileFormatWriter: Finished processing stats for write job c9739dc9-cdb1-40df-b081-22cd773d6f27.
[2023-07-15T12:13:53.361-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-15T12:13:53.379-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-15T12:13:53.400-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-15T12:13:53.426-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO MemoryStore: MemoryStore cleared
[2023-07-15T12:13:53.427-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO BlockManager: BlockManager stopped
[2023-07-15T12:13:53.430-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-15T12:13:53.435-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-15T12:13:53.446-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO SparkContext: Successfully stopped SparkContext
[2023-07-15T12:13:53.446-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO ShutdownHookManager: Shutdown hook called
[2023-07-15T12:13:53.447-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-65f087a0-3272-4012-b3b8-0476163eef26
[2023-07-15T12:13:53.452-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-65f087a0-3272-4012-b3b8-0476163eef26/pyspark-d64944b5-7954-4c0d-90bb-3499c27717ee
[2023-07-15T12:13:53.459-0300] {spark_submit.py:492} INFO - 23/07/15 12:13:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-23ad85ca-5383-4583-a12a-c8d1b262b761
[2023-07-15T12:13:53.560-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230711T000000, start_date=20230715T151332, end_date=20230715T151353
[2023-07-15T12:13:53.595-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-15T12:13:53.617-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
