[2023-07-12 12:36:43,996] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-12 12:36:44,004] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-12 12:36:44,004] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-07-12 12:36:44,005] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-07-12 12:36:44,005] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-07-12 12:36:44,021] {taskinstance.py:1377} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-12 12:36:44,024] {standard_task_runner.py:52} INFO - Started process 15822 to run task
[2023-07-12 12:36:44,034] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '10', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpnv6b2o7p', '--error-file', '/tmp/tmplbgc63n8']
[2023-07-12 12:36:44,035] {standard_task_runner.py:80} INFO - Job 10: Subtask twitter_datascience
[2023-07-12 12:36:44,078] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-12 12:36:44,137] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-07-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-07-11T00:00:00+00:00
[2023-07-12 12:36:44,145] {base.py:68} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-12 12:36:44,146] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-12 12:36:44,834] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-12 12:36:44,954] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230712T153643, end_date=20230712T153644
[2023-07-12 12:36:45,002] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-07-12 12:36:45,039] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-12 13:50:43,895] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-12 13:50:43,901] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-12 13:50:43,901] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-07-12 13:50:43,902] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-07-12 13:50:43,902] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-07-12 13:50:43,913] {taskinstance.py:1377} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-12 13:50:43,916] {standard_task_runner.py:52} INFO - Started process 20910 to run task
[2023-07-12 13:50:43,918] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '10', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmph2e7kwba', '--error-file', '/tmp/tmpmtb1gzl3']
[2023-07-12 13:50:43,918] {standard_task_runner.py:80} INFO - Job 10: Subtask twitter_datascience
[2023-07-12 13:50:43,939] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-12 13:50:43,969] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-07-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-07-11T00:00:00+00:00
[2023-07-12 13:50:43,973] {base.py:68} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-12 13:50:43,974] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-12 13:50:44,664] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230712T165043, end_date=20230712T165044
[2023-07-12 13:50:44,693] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-07-12 13:50:44,705] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:25:24.810-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:25:24.818-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:25:24.818-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:25:24.831-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:25:24.835-0300] {standard_task_runner.py:57} INFO - Started process 36500 to run task
[2023-07-13T22:25:24.837-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmplr0z7y7h']
[2023-07-13T22:25:24.838-0300] {standard_task_runner.py:85} INFO - Job 9: Subtask twitter_datascience
[2023-07-13T22:25:24.875-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:25:24.950-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:25:24.958-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:25:24.960-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-13T22:25:25.563-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:25:25.662-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T012524, end_date=20230714T012525
[2023-07-13T22:25:25.691-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:25:25.701-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:31:20.277-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:31:20.282-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:31:20.283-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:31:20.292-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:31:20.295-0300] {standard_task_runner.py:57} INFO - Started process 39146 to run task
[2023-07-13T22:31:20.297-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpir_7tnys']
[2023-07-13T22:31:20.298-0300] {standard_task_runner.py:85} INFO - Job 9: Subtask twitter_datascience
[2023-07-13T22:31:20.322-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:31:20.396-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:31:20.401-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:31:20.402-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-13T22:31:20.783-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T013120, end_date=20230714T013120
[2023-07-13T22:31:20.830-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:31:20.840-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:43:18.723-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:43:18.727-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:43:18.727-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:43:18.736-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:43:18.738-0300] {standard_task_runner.py:57} INFO - Started process 43958 to run task
[2023-07-13T22:43:18.739-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpszeahiy9']
[2023-07-13T22:43:18.740-0300] {standard_task_runner.py:85} INFO - Job 27: Subtask twitter_datascience
[2023-07-13T22:43:18.761-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:43:18.822-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:43:18.828-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:43:18.829-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-13T22:43:19.404-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:43:19.499-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:43:19.597-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T014318, end_date=20230714T014319
[2023-07-13T22:43:19.634-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:43:19.644-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:49:40.181-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:49:40.185-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:49:40.186-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:49:40.196-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:49:40.200-0300] {standard_task_runner.py:57} INFO - Started process 47075 to run task
[2023-07-13T22:49:40.206-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpnjey_r0i']
[2023-07-13T22:49:40.207-0300] {standard_task_runner.py:85} INFO - Job 27: Subtask twitter_datascience
[2023-07-13T22:49:40.246-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:49:40.292-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:49:40.298-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:49:40.299-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-13T22:49:40.671-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T014940, end_date=20230714T014940
[2023-07-13T22:49:40.696-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:49:40.706-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:51:44.796-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:51:44.804-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:51:44.805-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:51:44.818-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:51:44.822-0300] {standard_task_runner.py:57} INFO - Started process 49140 to run task
[2023-07-13T22:51:44.823-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '28', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp84an6q77']
[2023-07-13T22:51:44.824-0300] {standard_task_runner.py:85} INFO - Job 28: Subtask twitter_datascience
[2023-07-13T22:51:44.848-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:51:44.892-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:51:44.898-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:51:44.899-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-13T22:51:46.092-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:51:46.186-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:51:46.280-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:51:46.374-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:51:46.468-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:51:46.567-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T015144, end_date=20230714T015146
[2023-07-13T22:51:46.602-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:51:46.612-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:53:05.999-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:53:06.003-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-13T22:53:06.003-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:53:06.011-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-13T22:53:06.014-0300] {standard_task_runner.py:57} INFO - Started process 50733 to run task
[2023-07-13T22:53:06.017-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp5r_2xeqp']
[2023-07-13T22:53:06.018-0300] {standard_task_runner.py:85} INFO - Job 27: Subtask twitter_datascience
[2023-07-13T22:53:06.051-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:53:06.098-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-13T22:53:06.101-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:53:06.102-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-13T22:53:06.470-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:53:06.564-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:53:06.659-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:53:06.752-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:53:06.850-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T015305, end_date=20230714T015306
[2023-07-13T22:53:06.872-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:53:06.882-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T15:26:27.705-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T15:26:27.712-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T15:26:27.712-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T15:26:27.729-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T15:26:27.740-0300] {standard_task_runner.py:57} INFO - Started process 14695 to run task
[2023-07-14T15:26:27.743-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '45', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp457hhwsl']
[2023-07-14T15:26:27.745-0300] {standard_task_runner.py:85} INFO - Job 45: Subtask twitter_datascience
[2023-07-14T15:26:27.813-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T15:26:27.914-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T15:26:27.922-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T15:26:27.924-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-14T15:26:28.739-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T182627, end_date=20230714T182628
[2023-07-14T15:26:28.802-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T15:26:28.857-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:11:43.126-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T16:11:43.132-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T16:11:43.133-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:11:43.145-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T16:11:43.148-0300] {standard_task_runner.py:57} INFO - Started process 22247 to run task
[2023-07-14T16:11:43.153-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '48', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpxst3uiao']
[2023-07-14T16:11:43.154-0300] {standard_task_runner.py:85} INFO - Job 48: Subtask twitter_datascience
[2023-07-14T16:11:43.185-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:11:43.245-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T16:11:43.249-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T16:11:43.250-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-14T16:11:43.924-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-14T16:11:44.034-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-14T16:11:44.348-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T191143, end_date=20230714T191144
[2023-07-14T16:11:44.407-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:11:44.464-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:29:32.059-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T16:29:32.063-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T16:29:32.063-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:29:32.077-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T16:29:32.079-0300] {standard_task_runner.py:57} INFO - Started process 29161 to run task
[2023-07-14T16:29:32.081-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpu3368jdg']
[2023-07-14T16:29:32.082-0300] {standard_task_runner.py:85} INFO - Job 50: Subtask twitter_datascience
[2023-07-14T16:29:32.115-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:29:32.230-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T16:29:32.241-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T16:29:32.243-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-14T16:31:21.844-0300] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 29161. PIDs of all processes in the group: [29161]
[2023-07-14T16:31:21.844-0300] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 29161
[2023-07-14T16:31:21.845-0300] {taskinstance.py:1517} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-07-14T16:31:21.850-0300] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/airflow_pipeline/operators/twitter_operator.py", line 33, in execute
    for pg in TwitterHook(end_time, start_time, query).run():
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/airflow_pipeline/hook/twitter_hook.py", line 61, in run
    return self.paginate(url_raw, session)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/airflow_pipeline/hook/twitter_hook.py", line 41, in paginate
    response = self.connect_to_endpoint(url_raw, session)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/airflow_pipeline/hook/twitter_hook.py", line 35, in connect_to_endpoint
    return self.run_and_check(session, prep, {})
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/http/hooks/http.py", line 180, in run_and_check
    response = session.send(prepped_request, **send_kwargs)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/requests/sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/requests/adapters.py", line 440, in send
    resp = conn.urlopen(
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._validate_conn(conn)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 1040, in _validate_conn
    conn.connect()
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/urllib3/connection.py", line 358, in connect
    self.sock = conn = self._new_conn()
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1519, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-07-14T16:31:21.862-0300] {taskinstance.py:1345} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T192932, end_date=20230714T193121
[2023-07-14T16:31:21.880-0300] {standard_task_runner.py:104} ERROR - Failed to execute job 50 for task twitter_datascience (Task received SIGTERM signal; 29161)
[2023-07-14T16:31:21.900-0300] {process_utils.py:79} INFO - Process psutil.Process(pid=29161, status='terminated', exitcode=1, started='16:29:31') (29161) terminated with exit code 1
[2023-07-14T17:30:34.224-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T17:30:34.239-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T17:30:34.239-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:30:34.272-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T17:30:34.283-0300] {standard_task_runner.py:57} INFO - Started process 36312 to run task
[2023-07-14T17:30:34.293-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '59', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpozmg3nvm']
[2023-07-14T17:30:34.295-0300] {standard_task_runner.py:85} INFO - Job 59: Subtask twitter_datascience
[2023-07-14T17:30:34.354-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:30:34.449-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T17:30:34.453-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T17:30:34.454-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-14T17:30:35.303-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-14T17:30:35.446-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230714T203034, end_date=20230714T203035
[2023-07-14T17:30:35.512-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:30:35.585-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:38:21.219-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T23:38:21.231-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T23:38:21.232-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:38:21.250-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T23:38:21.256-0300] {standard_task_runner.py:57} INFO - Started process 21785 to run task
[2023-07-14T23:38:21.260-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '72', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpk1z_3d79']
[2023-07-14T23:38:21.263-0300] {standard_task_runner.py:85} INFO - Job 72: Subtask twitter_datascience
[2023-07-14T23:38:21.311-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:38:21.390-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T23:38:21.397-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T23:38:21.399-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-14T23:38:22.278-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230715T023821, end_date=20230715T023822
[2023-07-14T23:38:22.316-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:38:22.365-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:43:52.320-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T23:43:52.327-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-14T23:43:52.327-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:43:52.342-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-14T23:43:52.347-0300] {standard_task_runner.py:57} INFO - Started process 24451 to run task
[2023-07-14T23:43:52.351-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '74', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpunviuk6d']
[2023-07-14T23:43:52.352-0300] {standard_task_runner.py:85} INFO - Job 74: Subtask twitter_datascience
[2023-07-14T23:43:52.398-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:43:52.476-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-14T23:43:52.482-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T23:43:52.484-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-14T23:43:53.015-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230715T024352, end_date=20230715T024353
[2023-07-14T23:43:53.045-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:43:53.084-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-15T12:12:18.108-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-15T12:12:18.144-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [queued]>
[2023-07-15T12:12:18.145-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-15T12:12:18.198-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-11 00:00:00+00:00
[2023-07-15T12:12:18.211-0300] {standard_task_runner.py:57} INFO - Started process 13019 to run task
[2023-07-15T12:12:18.222-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-11T00:00:00+00:00', '--job-id', '94', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp5mqf5hex']
[2023-07-15T12:12:18.227-0300] {standard_task_runner.py:85} INFO - Job 94: Subtask twitter_datascience
[2023-07-15T12:12:18.326-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-11T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-15T12:12:18.478-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-11T00:00:00+00:00'
[2023-07-15T12:12:18.490-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-15T12:12:18.492-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z
[2023-07-15T12:12:19.577-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-15T12:12:19.768-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-15T12:12:19.887-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-15T12:12:20.075-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-11T00:00:00.00Z&end_time=2023-07-12T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-15T12:12:20.289-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230711T000000, start_date=20230715T151218, end_date=20230715T151220
[2023-07-15T12:12:20.359-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-15T12:12:20.436-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
