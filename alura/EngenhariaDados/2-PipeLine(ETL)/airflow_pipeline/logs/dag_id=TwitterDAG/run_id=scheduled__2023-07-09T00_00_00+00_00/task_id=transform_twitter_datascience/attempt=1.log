[2023-07-13T22:25:16.095-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:25:16.104-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:25:16.104-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:25:16.118-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-13T22:25:16.122-0300] {standard_task_runner.py:57} INFO - Started process 36351 to run task
[2023-07-13T22:25:16.125-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpfnua5mjd']
[2023-07-13T22:25:16.126-0300] {standard_task_runner.py:85} INFO - Job 7: Subtask transform_twitter_datascience
[2023-07-13T22:25:16.170-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:25:16.219-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-13T22:25:16.223-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:25:16.224-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-09
[2023-07-13T22:25:17.590-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:17 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:25:17.591-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:25:17.601-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:25:17.602-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:25:17.602-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:25:17.602-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:25:17.602-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:631)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:271)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:234)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:119)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1022)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1022)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
[2023-07-13T22:25:17.636-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-07-13T22:25:17.660-0300] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 423, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-09. Error code is: 1.
[2023-07-13T22:25:17.662-0300] {taskinstance.py:1345} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T012516, end_date=20230714T012517
[2023-07-13T22:25:17.670-0300] {standard_task_runner.py:104} ERROR - Failed to execute job 7 for task transform_twitter_datascience (Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-09. Error code is: 1.; 36351)
[2023-07-13T22:25:17.701-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-13T22:25:17.708-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:31:00.241-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:31:00.248-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:31:00.249-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:31:00.263-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-13T22:31:00.267-0300] {standard_task_runner.py:57} INFO - Started process 38547 to run task
[2023-07-13T22:31:00.271-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpon3x4ozy']
[2023-07-13T22:31:00.272-0300] {standard_task_runner.py:85} INFO - Job 7: Subtask transform_twitter_datascience
[2023-07-13T22:31:00.299-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:31:00.344-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-13T22:31:00.347-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:31:00.348-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-09
[2023-07-13T22:31:01.593-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:01 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:31:01.594-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:31:01.605-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:31:01.605-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:31:01.605-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:31:01.605-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:31:01.605-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:31:02.012-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:31:02.690-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-13T22:31:02.698-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO SparkContext: Running Spark version 3.1.3
[2023-07-13T22:31:02.735-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:02.736-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:31:02.736-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:02.736-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:31:02.755-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:31:02.766-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:31:02.766-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:31:02.808-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:31:02.808-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:31:02.808-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:31:02.809-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:31:02.809-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:31:03.020-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO Utils: Successfully started service 'sparkDriver' on port 37895.
[2023-07-13T22:31:03.042-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:31:03.067-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:31:03.083-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:31:03.083-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:31:03.087-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:31:03.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-483002c4-c2ac-47c3-9ff7-97c0ecd73d41
[2023-07-13T22:31:03.116-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:31:03.129-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:31:03.283-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:31:03.336-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4040
[2023-07-13T22:31:03.494-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:31:03.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37147.
[2023-07-13T22:31:03.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO NettyBlockTransferService: Server created on 192.168.0.177:37147
[2023-07-13T22:31:03.517-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:31:03.523-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 37147, None)
[2023-07-13T22:31:03.526-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:37147 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 37147, None)
[2023-07-13T22:31:03.528-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 37147, None)
[2023-07-13T22:31:03.529-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 37147, None)
[2023-07-13T22:31:03.867-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-13T22:31:03.868-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:03 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:31:04.515-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:04 INFO InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.
[2023-07-13T22:31:04.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:04 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 8 paths.
[2023-07-13T22:31:05.895-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:05 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:05.895-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:05 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:05.897-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:05 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:31:06.099-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-13T22:31:06.138-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-13T22:31:06.139-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:37147 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-13T22:31:06.143-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:06.150-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33608727 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:06.282-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:06.295-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:06.296-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:06.297-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:06.298-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:06.302-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:06.379-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-13T22:31:06.381-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-13T22:31:06.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:37147 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-13T22:31:06.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:06.392-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:06.393-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:31:06.439-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6291 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:06.450-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:31:06.552-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:31:06.807-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO CodeGenerator: Code generated in 161.028844 ms
[2023-07-13T22:31:06.836-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [empty row]
[2023-07-13T22:31:06.841-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:31:06.845-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9012, partition values: [empty row]
[2023-07-13T22:31:06.857-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [empty row]
[2023-07-13T22:31:06.859-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [empty row]
[2023-07-13T22:31:06.861-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4506, partition values: [empty row]
[2023-07-13T22:31:06.864-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [empty row]
[2023-07-13T22:31:06.884-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2023-07-13T22:31:06.892-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 462 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:06.894-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:31:06.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,586 s
[2023-07-13T22:31:06.900-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:06.901-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:31:06.902-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:06 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,620246 s
[2023-07-13T22:31:07.218-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:31:07.222-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:31:07.223-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-13T22:31:07.223-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:31:07.278-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:07.278-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:07.278-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:07.350-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO CodeGenerator: Code generated in 23.671765 ms
[2023-07-13T22:31:07.433-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO CodeGenerator: Code generated in 62.800835 ms
[2023-07-13T22:31:07.438-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-13T22:31:07.448-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)
[2023-07-13T22:31:07.449-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:37147 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:31:07.450-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:07.453-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33608727 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:07.506-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:07.507-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:07.507-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:07.507-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:07.515-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:07.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:07.534-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:37147 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2023-07-13T22:31:07.549-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 195.4 KiB, free 433.8 MiB)
[2023-07-13T22:31:07.551-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 68.5 KiB, free 433.7 MiB)
[2023-07-13T22:31:07.551-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:37147 (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:31:07.552-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:07.553-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:07.553-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:31:07.556-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:07.557-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:31:07.615-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:37147 in memory (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-13T22:31:07.629-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:07.629-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:07.629-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:07.689-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO CodeGenerator: Code generated in 28.848484 ms
[2023-07-13T22:31:07.691-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:31:07.726-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO CodeGenerator: Code generated in 30.228019 ms
[2023-07-13T22:31:07.747-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO CodeGenerator: Code generated in 5.898674 ms
[2023-07-13T22:31:07.770-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:31:07.779-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:31:07.788-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9012, partition values: [19549]
[2023-07-13T22:31:07.796-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:31:07.802-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:31:07.807-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4506, partition values: [19550]
[2023-07-13T22:31:07.812-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [19551]
[2023-07-13T22:31:07.825-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: Saved output of task 'attempt_20230713223107820822255592213466_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-09/_temporary/0/task_20230713223107820822255592213466_0001_m_000000
[2023-07-13T22:31:07.825-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SparkHadoopMapRedUtil: attempt_20230713223107820822255592213466_0001_m_000000_1: Committed
[2023-07-13T22:31:07.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3119 bytes result sent to driver
[2023-07-13T22:31:07.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 277 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:07.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:31:07.832-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,315 s
[2023-07-13T22:31:07.832-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:07.832-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:31:07.833-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,327116 s
[2023-07-13T22:31:07.845-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileFormatWriter: Write Job 6ea6e84c-8a6d-447b-8d18-68d69b928f6e committed.
[2023-07-13T22:31:07.847-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileFormatWriter: Finished processing stats for write job 6ea6e84c-8a6d-447b-8d18-68d69b928f6e.
[2023-07-13T22:31:07.877-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:31:07.877-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:07.877-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:07.878-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:31:07.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:07.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:07.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:07.909-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO CodeGenerator: Code generated in 10.982173 ms
[2023-07-13T22:31:07.922-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO CodeGenerator: Code generated in 10.376699 ms
[2023-07-13T22:31:07.924-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-13T22:31:07.932-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.7 MiB)
[2023-07-13T22:31:07.932-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:37147 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:31:07.933-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:07.933-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33608727 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:07.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:07.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:07.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:07.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:07.955-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:07.955-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:07.971-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 184.3 KiB, free 433.6 MiB)
[2023-07-13T22:31:07.974-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 433.5 MiB)
[2023-07-13T22:31:07.974-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:37147 (size: 65.7 KiB, free: 434.2 MiB)
[2023-07-13T22:31:07.974-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:07.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:07.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:31:07.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:07.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:31:07.990-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:31:07.990-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:31:07.990-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:31:08.011-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO CodeGenerator: Code generated in 7.137296 ms
[2023-07-13T22:31:08.013-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:31:08.026-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO CodeGenerator: Code generated in 10.803437 ms
[2023-07-13T22:31:08.036-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:31:08.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:31:08.051-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:37147 in memory (size: 27.6 KiB, free: 434.2 MiB)
[2023-07-13T22:31:08.053-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-9012, partition values: [19549]
[2023-07-13T22:31:08.058-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:37147 in memory (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:31:08.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:31:08.060-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:31:08.063-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4506, partition values: [19550]
[2023-07-13T22:31:08.065-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4479, partition values: [19551]
[2023-07-13T22:31:08.069-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileOutputCommitter: Saved output of task 'attempt_202307132231079004463190335823516_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-09/_temporary/0/task_202307132231079004463190335823516_0002_m_000000
[2023-07-13T22:31:08.069-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO SparkHadoopMapRedUtil: attempt_202307132231079004463190335823516_0002_m_000000_2: Committed
[2023-07-13T22:31:08.070-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3014 bytes result sent to driver
[2023-07-13T22:31:08.071-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 95 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:08.071-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:31:08.072-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,116 s
[2023-07-13T22:31:08.072-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:08.072-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:31:08.073-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,119169 s
[2023-07-13T22:31:08.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileFormatWriter: Write Job e79fa15e-05d5-41ef-a8e3-ad5f6bbc80bd committed.
[2023-07-13T22:31:08.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO FileFormatWriter: Finished processing stats for write job e79fa15e-05d5-41ef-a8e3-ad5f6bbc80bd.
[2023-07-13T22:31:08.121-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:31:08.129-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:31:08.138-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:31:08.145-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:31:08.145-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO BlockManager: BlockManager stopped
[2023-07-13T22:31:08.147-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:31:08.149-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:31:08.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:31:08.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:31:08.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-f1fc883d-da47-44f7-888d-5a90503f8d06
[2023-07-13T22:31:08.155-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-054d3012-28e9-4a64-91e4-cc7c6380c1dd
[2023-07-13T22:31:08.156-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-f1fc883d-da47-44f7-888d-5a90503f8d06/pyspark-38ccb85e-0325-4123-a028-28f6bcc7309b
[2023-07-13T22:31:08.203-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T013100, end_date=20230714T013108
[2023-07-13T22:31:08.248-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:31:08.255-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:42:20.812-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:42:20.818-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:42:20.819-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:42:20.828-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-13T22:42:20.830-0300] {standard_task_runner.py:57} INFO - Started process 42653 to run task
[2023-07-13T22:42:20.836-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '24', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp9pr835y8']
[2023-07-13T22:42:20.837-0300] {standard_task_runner.py:85} INFO - Job 24: Subtask transform_twitter_datascience
[2023-07-13T22:42:20.876-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:42:20.946-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-13T22:42:20.955-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:42:20.957-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-09
[2023-07-13T22:42:22.775-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:22 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:42:22.778-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:42:23.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:42:23.628-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:42:23.713-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO ResourceUtils: ==============================================================
[2023-07-13T22:42:23.714-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:42:23.714-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO ResourceUtils: ==============================================================
[2023-07-13T22:42:23.714-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:42:23.741-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:42:23.753-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:42:23.755-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:42:23.810-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:42:23.810-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:42:23.810-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:42:23.811-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:42:23.811-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:42:24.071-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO Utils: Successfully started service 'sparkDriver' on port 43563.
[2023-07-13T22:42:24.106-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:42:24.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:42:24.175-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:42:24.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:42:24.181-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:42:24.218-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-481ff514-c2e2-41c9-8430-6f55a456ad5e
[2023-07-13T22:42:24.248-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:42:24.291-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:42:24.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:42:25.088-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:42:25.112-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:42:25.160-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40949.
[2023-07-13T22:42:25.160-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO NettyBlockTransferService: Server created on 192.168.0.177:40949
[2023-07-13T22:42:25.165-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:42:25.179-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 40949, None)
[2023-07-13T22:42:25.187-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:40949 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 40949, None)
[2023-07-13T22:42:25.193-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 40949, None)
[2023-07-13T22:42:25.196-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 40949, None)
[2023-07-13T22:42:25.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:42:25.896-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:25 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:42:26.775-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:26 INFO InMemoryFileIndex: It took 52 ms to list leaf files for 1 paths.
[2023-07-13T22:42:26.908-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:26 INFO InMemoryFileIndex: It took 10 ms to list leaf files for 8 paths.
[2023-07-13T22:42:28.901-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:28 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:42:28.903-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:42:28.905-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:42:29.135-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:42:29.179-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:42:29.182-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:40949 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:42:29.185-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:42:29.194-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635805 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:42:29.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:42:29.345-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:42:29.345-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:42:29.345-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:42:29.347-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:42:29.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:42:29.438-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:42:29.440-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:42:29.441-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:40949 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:42:29.441-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:42:29.452-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:42:29.453-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:42:29.494-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:42:29.504-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:42:29.604-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-18077, partition values: [empty row]
[2023-07-13T22:42:29.778-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO CodeGenerator: Code generated in 134.578678 ms
[2023-07-13T22:42:29.815-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13567, partition values: [empty row]
[2023-07-13T22:42:29.820-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [empty row]
[2023-07-13T22:42:29.824-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:42:29.827-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9053, partition values: [empty row]
[2023-07-13T22:42:29.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:42:29.834-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [empty row]
[2023-07-13T22:42:29.837-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [empty row]
[2023-07-13T22:42:29.863-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:42:29.877-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 392 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:42:29.879-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:42:29.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,513 s
[2023-07-13T22:42:29.892-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:42:29.893-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:42:29.895-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:29 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,565119 s
[2023-07-13T22:42:30.400-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:42:30.402-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:42:30.406-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:42:30.407-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:42:30.468-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:42:30.469-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:42:30.470-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:42:30.549-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:40949 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:42:30.554-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:40949 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:42:30.667-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO CodeGenerator: Code generated in 99.048744 ms
[2023-07-13T22:42:30.671-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:42:30.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:42:30.681-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:40949 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:42:30.682-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:42:30.687-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635805 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:42:30.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:42:30.749-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:42:30.749-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:42:30.750-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:42:30.750-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:42:30.750-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:42:30.773-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.5 KiB, free 433.9 MiB)
[2023-07-13T22:42:30.776-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:42:30.777-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:40949 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:42:30.777-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:42:30.778-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:42:30.778-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:42:30.781-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:42:30.782-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:42:30.827-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:42:30.827-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:42:30.828-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:42:30.855-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-18077, partition values: [19547]
[2023-07-13T22:42:30.878-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO CodeGenerator: Code generated in 19.362802 ms
[2023-07-13T22:42:30.897-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO CodeGenerator: Code generated in 4.340525 ms
[2023-07-13T22:42:30.925-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13567, partition values: [19546]
[2023-07-13T22:42:30.936-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:42:30.946-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:42:30.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9053, partition values: [19548]
[2023-07-13T22:42:30.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:42:30.970-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:42:30.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:42:30.989-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO FileOutputCommitter: Saved output of task 'attempt_202307132242308026536349397854689_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-09/_temporary/0/task_202307132242308026536349397854689_0001_m_000000
[2023-07-13T22:42:30.990-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO SparkHadoopMapRedUtil: attempt_202307132242308026536349397854689_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:42:30.995-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:42:30.997-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 218 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:42:30.997-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:42:30.998-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,247 s
[2023-07-13T22:42:30.998-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:42:30.998-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:42:30.999-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:30 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,250506 s
[2023-07-13T22:42:31.001-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileFormatWriter: Start to commit write Job 467e1996-f96b-4bb4-9657-bff28ee7bdbd.
[2023-07-13T22:42:31.013-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileFormatWriter: Write Job 467e1996-f96b-4bb4-9657-bff28ee7bdbd committed. Elapsed time: 11 ms.
[2023-07-13T22:42:31.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileFormatWriter: Finished processing stats for write job 467e1996-f96b-4bb4-9657-bff28ee7bdbd.
[2023-07-13T22:42:31.043-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:42:31.043-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:42:31.043-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:42:31.043-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:42:31.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:42:31.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:42:31.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:42:31.087-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO CodeGenerator: Code generated in 18.552065 ms
[2023-07-13T22:42:31.090-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:42:31.101-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:42:31.102-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:40949 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:42:31.102-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:42:31.103-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635805 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:42:31.127-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:42:31.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:42:31.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:42:31.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:42:31.128-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:42:31.129-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:42:31.156-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.5 KiB, free 433.4 MiB)
[2023-07-13T22:42:31.160-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:42:31.161-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:40949 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:42:31.161-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:42:31.162-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:42:31.162-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:42:31.163-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:42:31.164-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:42:31.180-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:42:31.180-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:42:31.181-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:42:31.193-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-18077, partition values: [19547]
[2023-07-13T22:42:31.206-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO CodeGenerator: Code generated in 11.21621 ms
[2023-07-13T22:42:31.211-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13567, partition values: [19546]
[2023-07-13T22:42:31.215-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:42:31.218-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:42:31.221-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9053, partition values: [19548]
[2023-07-13T22:42:31.224-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:42:31.226-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:42:31.229-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:42:31.233-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileOutputCommitter: Saved output of task 'attempt_202307132242317603836683708233973_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-09/_temporary/0/task_202307132242317603836683708233973_0002_m_000000
[2023-07-13T22:42:31.234-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SparkHadoopMapRedUtil: attempt_202307132242317603836683708233973_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-13T22:42:31.235-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:42:31.236-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 73 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:42:31.237-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:42:31.237-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,107 s
[2023-07-13T22:42:31.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:42:31.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:42:31.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,110838 s
[2023-07-13T22:42:31.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileFormatWriter: Start to commit write Job 8f84ef02-b6f9-4510-8418-1250c2541561.
[2023-07-13T22:42:31.247-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileFormatWriter: Write Job 8f84ef02-b6f9-4510-8418-1250c2541561 committed. Elapsed time: 8 ms.
[2023-07-13T22:42:31.248-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO FileFormatWriter: Finished processing stats for write job 8f84ef02-b6f9-4510-8418-1250c2541561.
[2023-07-13T22:42:31.272-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:42:31.281-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:42:31.300-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:42:31.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:42:31.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO BlockManager: BlockManager stopped
[2023-07-13T22:42:31.311-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:42:31.313-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:42:31.317-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:42:31.317-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:42:31.317-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b28f15e-0daf-4263-9658-8f910e394f73/pyspark-e2eca325-13e1-4a63-8796-ce209cf0fec7
[2023-07-13T22:42:31.319-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-a414e656-8dfa-4cb1-bcc7-dd85b26a4ff4
[2023-07-13T22:42:31.321-0300] {spark_submit.py:492} INFO - 23/07/13 22:42:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b28f15e-0daf-4263-9658-8f910e394f73
[2023-07-13T22:42:31.368-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T014220, end_date=20230714T014231
[2023-07-13T22:42:31.388-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:42:31.394-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:43:08.355-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:43:08.360-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:43:08.360-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:43:08.371-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-13T22:43:08.374-0300] {standard_task_runner.py:57} INFO - Started process 43647 to run task
[2023-07-13T22:43:08.379-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpn_fm4d1n']
[2023-07-13T22:43:08.380-0300] {standard_task_runner.py:85} INFO - Job 26: Subtask transform_twitter_datascience
[2023-07-13T22:43:08.411-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:43:08.477-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-13T22:43:08.484-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:43:08.485-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-09
[2023-07-13T22:43:09.769-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:09 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:43:09.772-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:43:10.511-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:43:10.577-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:43:10.646-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO ResourceUtils: ==============================================================
[2023-07-13T22:43:10.646-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:43:10.646-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO ResourceUtils: ==============================================================
[2023-07-13T22:43:10.646-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:43:10.664-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:43:10.674-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:43:10.675-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:43:10.716-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:43:10.716-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:43:10.717-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:43:10.717-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:43:10.717-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:43:10.891-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO Utils: Successfully started service 'sparkDriver' on port 46383.
[2023-07-13T22:43:10.920-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:43:10.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:43:10.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:43:10.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:43:10.981-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:43:10.999-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d1cb17c9-6438-4604-87a9-11c593b8bca9
[2023-07-13T22:43:11.014-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:43:11.029-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:43:11.231-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:43:11.309-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:43:11.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:43:11.327-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34017.
[2023-07-13T22:43:11.328-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO NettyBlockTransferService: Server created on 192.168.0.177:34017
[2023-07-13T22:43:11.329-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:43:11.334-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 34017, None)
[2023-07-13T22:43:11.338-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:34017 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 34017, None)
[2023-07-13T22:43:11.340-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 34017, None)
[2023-07-13T22:43:11.341-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 34017, None)
[2023-07-13T22:43:11.694-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:43:11.701-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:11 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:43:12.390-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:12 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.
[2023-07-13T22:43:12.496-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:12 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:43:14.130-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:43:14.131-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:43:14.134-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:43:14.363-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:43:14.405-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:14.407-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:34017 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:14.411-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:14.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613119 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:14.552-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:14.566-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:14.566-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:14.566-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:14.567-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:14.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:14.666-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:43:14.668-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:14.668-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:34017 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:14.669-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:14.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:14.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:43:14.726-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:14.738-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:43:14.835-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [empty row]
[2023-07-13T22:43:14.980-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:14 INFO CodeGenerator: Code generated in 114.70554 ms
[2023-07-13T22:43:15.016-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [empty row]
[2023-07-13T22:43:15.023-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:43:15.027-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:43:15.031-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [empty row]
[2023-07-13T22:43:15.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [empty row]
[2023-07-13T22:43:15.036-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [empty row]
[2023-07-13T22:43:15.039-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [empty row]
[2023-07-13T22:43:15.053-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:43:15.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 344 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:15.061-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:43:15.065-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,473 s
[2023-07-13T22:43:15.068-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:15.068-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:43:15.070-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,518131 s
[2023-07-13T22:43:15.491-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:34017 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:15.505-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:34017 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:43:15.548-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:43:15.551-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:43:15.552-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:43:15.553-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:43:15.639-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:15.640-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:15.641-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:15.918-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO CodeGenerator: Code generated in 131.974574 ms
[2023-07-13T22:43:15.929-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:43:15.946-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:43:15.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:34017 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:43:15.949-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:15.955-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613119 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:16.046-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:16.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:16.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:16.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:16.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:16.051-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:16.093-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.5 KiB, free 433.9 MiB)
[2023-07-13T22:43:16.097-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:43:16.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:34017 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:43:16.100-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:16.102-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:16.102-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:43:16.106-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:16.107-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:43:16.155-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:16.156-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:16.156-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:16.182-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:43:16.203-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO CodeGenerator: Code generated in 16.454067 ms
[2023-07-13T22:43:16.220-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO CodeGenerator: Code generated in 3.693988 ms
[2023-07-13T22:43:16.242-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:43:16.249-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:43:16.255-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:43:16.261-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:43:16.266-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:43:16.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:43:16.275-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:43:16.287-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileOutputCommitter: Saved output of task 'attempt_202307132243157706094428566224877_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-09/_temporary/0/task_202307132243157706094428566224877_0001_m_000000
[2023-07-13T22:43:16.287-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkHadoopMapRedUtil: attempt_202307132243157706094428566224877_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:43:16.293-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:43:16.295-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 190 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:16.295-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:43:16.296-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,243 s
[2023-07-13T22:43:16.296-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:16.296-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:43:16.297-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,250837 s
[2023-07-13T22:43:16.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileFormatWriter: Start to commit write Job 5bd124aa-36a5-45e8-ab53-be569dc1793c.
[2023-07-13T22:43:16.310-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileFormatWriter: Write Job 5bd124aa-36a5-45e8-ab53-be569dc1793c committed. Elapsed time: 10 ms.
[2023-07-13T22:43:16.313-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileFormatWriter: Finished processing stats for write job 5bd124aa-36a5-45e8-ab53-be569dc1793c.
[2023-07-13T22:43:16.339-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:43:16.339-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:43:16.339-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:43:16.339-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:43:16.347-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:16.348-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:16.348-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:16.382-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO CodeGenerator: Code generated in 16.134362 ms
[2023-07-13T22:43:16.386-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:43:16.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:43:16.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:34017 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:43:16.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:16.399-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613119 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:43:16.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:34017 in memory (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:43:16.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:34017 in memory (size: 83.4 KiB, free: 434.4 MiB)
[2023-07-13T22:43:16.433-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:43:16.433-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:43:16.433-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:43:16.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:43:16.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:43:16.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:43:16.452-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.5 KiB, free 434.0 MiB)
[2023-07-13T22:43:16.454-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.9 MiB)
[2023-07-13T22:43:16.455-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:34017 (size: 77.2 KiB, free: 434.3 MiB)
[2023-07-13T22:43:16.455-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:43:16.456-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:43:16.456-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:43:16.458-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:43:16.458-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:43:16.476-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:43:16.476-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:43:16.477-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:43:16.492-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:43:16.506-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO CodeGenerator: Code generated in 10.412319 ms
[2023-07-13T22:43:16.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:43:16.515-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:43:16.518-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:43:16.522-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:43:16.526-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:43:16.531-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:43:16.534-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:43:16.539-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileOutputCommitter: Saved output of task 'attempt_202307132243168439266981991127164_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-09/_temporary/0/task_202307132243168439266981991127164_0002_m_000000
[2023-07-13T22:43:16.540-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkHadoopMapRedUtil: attempt_202307132243168439266981991127164_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-13T22:43:16.542-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:43:16.543-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 86 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:43:16.544-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:43:16.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,109 s
[2023-07-13T22:43:16.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:43:16.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:43:16.547-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,112805 s
[2023-07-13T22:43:16.547-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileFormatWriter: Start to commit write Job 02787904-78b2-43d6-92db-5c9a771d5a32.
[2023-07-13T22:43:16.559-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileFormatWriter: Write Job 02787904-78b2-43d6-92db-5c9a771d5a32 committed. Elapsed time: 12 ms.
[2023-07-13T22:43:16.560-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO FileFormatWriter: Finished processing stats for write job 02787904-78b2-43d6-92db-5c9a771d5a32.
[2023-07-13T22:43:16.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:43:16.607-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:43:16.618-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:43:16.626-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:43:16.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO BlockManager: BlockManager stopped
[2023-07-13T22:43:16.631-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:43:16.633-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:43:16.637-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:43:16.638-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:43:16.638-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-721c7f5c-e087-4a3a-a9cd-00a2cfa30a71
[2023-07-13T22:43:16.641-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-c02651b1-a241-4761-9a00-6b0f1a261fb8
[2023-07-13T22:43:16.644-0300] {spark_submit.py:492} INFO - 23/07/13 22:43:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-c02651b1-a241-4761-9a00-6b0f1a261fb8/pyspark-84ed5957-ba8d-4675-a52a-ca9b0c5890f5
[2023-07-13T22:43:16.695-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T014308, end_date=20230714T014316
[2023-07-13T22:43:16.723-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:43:16.730-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:49:30.806-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:49:30.810-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:49:30.810-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:49:30.817-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-13T22:49:30.819-0300] {standard_task_runner.py:57} INFO - Started process 46801 to run task
[2023-07-13T22:49:30.822-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpgrt7gbcs']
[2023-07-13T22:49:30.822-0300] {standard_task_runner.py:85} INFO - Job 26: Subtask transform_twitter_datascience
[2023-07-13T22:49:30.842-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:49:30.880-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-13T22:49:30.884-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:49:30.886-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-09
[2023-07-13T22:49:32.201-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:32 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:49:32.203-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:49:32.860-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:32 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:49:32.911-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:49:32.992-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:32 INFO ResourceUtils: ==============================================================
[2023-07-13T22:49:32.992-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:49:32.993-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:32 INFO ResourceUtils: ==============================================================
[2023-07-13T22:49:32.993-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:32 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:49:33.010-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:49:33.019-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:49:33.020-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:49:33.055-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:49:33.056-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:49:33.056-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:49:33.057-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:49:33.057-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:49:33.250-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO Utils: Successfully started service 'sparkDriver' on port 45411.
[2023-07-13T22:49:33.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:49:33.297-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:49:33.310-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:49:33.310-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:49:33.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:49:33.330-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f4012ed4-3528-451d-8ee2-7cdb6b3ed0d7
[2023-07-13T22:49:33.344-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:49:33.358-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:49:33.563-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:49:33.645-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:49:33.652-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:49:33.668-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39309.
[2023-07-13T22:49:33.668-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO NettyBlockTransferService: Server created on 192.168.0.177:39309
[2023-07-13T22:49:33.670-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:49:33.676-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 39309, None)
[2023-07-13T22:49:33.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:39309 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 39309, None)
[2023-07-13T22:49:33.682-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 39309, None)
[2023-07-13T22:49:33.684-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 39309, None)
[2023-07-13T22:49:34.133-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:49:34.141-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:34 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:49:34.866-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:34 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
[2023-07-13T22:49:34.961-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:34 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:49:36.330-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:49:36.331-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:49:36.333-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:49:36.523-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:49:36.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:49:36.560-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:39309 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:36.563-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:36.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649760 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:49:36.689-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:36.702-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:49:36.703-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:49:36.703-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:49:36.704-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:49:36.707-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:49:36.788-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:49:36.790-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:49:36.790-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:39309 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:36.791-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:49:36.799-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:49:36.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:49:36.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:49:36.841-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:49:36.920-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:36 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:49:37.040-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO CodeGenerator: Code generated in 94.938275 ms
[2023-07-13T22:49:37.075-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [empty row]
[2023-07-13T22:49:37.081-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [empty row]
[2023-07-13T22:49:37.085-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [empty row]
[2023-07-13T22:49:37.088-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:49:37.091-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:49:37.094-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [empty row]
[2023-07-13T22:49:37.096-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [empty row]
[2023-07-13T22:49:37.108-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:49:37.115-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 290 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:49:37.117-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:49:37.121-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,403 s
[2023-07-13T22:49:37.123-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:49:37.124-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:49:37.125-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,436154 s
[2023-07-13T22:49:37.193-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:39309 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:37.222-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:39309 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:49:37.505-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:49:37.507-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:49:37.508-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:49:37.508-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:49:37.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:49:37.571-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:49:37.572-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:49:37.733-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO CodeGenerator: Code generated in 92.115107 ms
[2023-07-13T22:49:37.737-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:49:37.747-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:49:37.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:39309 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:49:37.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:37.752-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649760 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:49:37.804-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:37.805-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:49:37.805-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:49:37.805-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:49:37.806-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:49:37.806-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:49:37.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.5 KiB, free 433.9 MiB)
[2023-07-13T22:49:37.832-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:49:37.833-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:39309 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:49:37.833-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:49:37.834-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:49:37.834-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:49:37.837-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:49:37.838-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:49:37.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:49:37.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:49:37.899-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:49:37.923-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:49:37.946-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO CodeGenerator: Code generated in 20.104634 ms
[2023-07-13T22:49:37.965-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO CodeGenerator: Code generated in 6.345333 ms
[2023-07-13T22:49:37.990-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [19546]
[2023-07-13T22:49:37.999-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:49:38.006-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [19548]
[2023-07-13T22:49:38.012-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:49:38.019-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:49:38.023-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:49:38.027-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [19547]
[2023-07-13T22:49:38.039-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileOutputCommitter: Saved output of task 'attempt_202307132249378819121601226026199_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-09/_temporary/0/task_202307132249378819121601226026199_0001_m_000000
[2023-07-13T22:49:38.039-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SparkHadoopMapRedUtil: attempt_202307132249378819121601226026199_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:49:38.045-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:49:38.048-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 212 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:49:38.048-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:49:38.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,242 s
[2023-07-13T22:49:38.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:49:38.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:49:38.050-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,245664 s
[2023-07-13T22:49:38.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileFormatWriter: Start to commit write Job e546a2ec-e788-4ece-95cc-acdb8c41042e.
[2023-07-13T22:49:38.066-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileFormatWriter: Write Job e546a2ec-e788-4ece-95cc-acdb8c41042e committed. Elapsed time: 12 ms.
[2023-07-13T22:49:38.068-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileFormatWriter: Finished processing stats for write job e546a2ec-e788-4ece-95cc-acdb8c41042e.
[2023-07-13T22:49:38.103-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:49:38.103-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:49:38.103-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:49:38.103-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:49:38.113-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:49:38.113-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:49:38.114-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:49:38.140-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO CodeGenerator: Code generated in 12.226409 ms
[2023-07-13T22:49:38.144-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:49:38.154-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:49:38.154-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:39309 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:49:38.155-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:38.156-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649760 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:49:38.179-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:49:38.180-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:49:38.181-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:49:38.181-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:49:38.181-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:49:38.182-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:49:38.206-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.5 KiB, free 433.4 MiB)
[2023-07-13T22:49:38.208-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:49:38.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:39309 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:49:38.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:49:38.210-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:49:38.210-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:49:38.212-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:49:38.212-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:49:38.227-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:49:38.227-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:49:38.227-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:49:38.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:49:38.250-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO CodeGenerator: Code generated in 9.190619 ms
[2023-07-13T22:49:38.255-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18204, partition values: [19546]
[2023-07-13T22:49:38.258-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:49:38.261-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9157, partition values: [19548]
[2023-07-13T22:49:38.264-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:49:38.267-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:49:38.269-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:49:38.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4481, partition values: [19547]
[2023-07-13T22:49:38.275-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileOutputCommitter: Saved output of task 'attempt_202307132249383308926841602314742_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-09/_temporary/0/task_202307132249383308926841602314742_0002_m_000000
[2023-07-13T22:49:38.276-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SparkHadoopMapRedUtil: attempt_202307132249383308926841602314742_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-13T22:49:38.277-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:49:38.278-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 67 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:49:38.278-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:49:38.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,097 s
[2023-07-13T22:49:38.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:49:38.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:49:38.280-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,101089 s
[2023-07-13T22:49:38.280-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileFormatWriter: Start to commit write Job e7afe525-d9d3-4e36-958b-fde8ddafa9ef.
[2023-07-13T22:49:38.289-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileFormatWriter: Write Job e7afe525-d9d3-4e36-958b-fde8ddafa9ef committed. Elapsed time: 8 ms.
[2023-07-13T22:49:38.290-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO FileFormatWriter: Finished processing stats for write job e7afe525-d9d3-4e36-958b-fde8ddafa9ef.
[2023-07-13T22:49:38.318-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:49:38.326-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:49:38.336-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:49:38.343-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:49:38.344-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO BlockManager: BlockManager stopped
[2023-07-13T22:49:38.346-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:49:38.348-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:49:38.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:49:38.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:49:38.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff28ceb2-0d68-41b5-b8ea-d1c61dafd73c
[2023-07-13T22:49:38.353-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a91a1b3-3b5b-4516-81e1-c59299024c1c/pyspark-6ae55da3-7eb9-448a-9238-d7f2abb23c20
[2023-07-13T22:49:38.355-0300] {spark_submit.py:492} INFO - 23/07/13 22:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a91a1b3-3b5b-4516-81e1-c59299024c1c
[2023-07-13T22:49:38.406-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T014930, end_date=20230714T014938
[2023-07-13T22:49:38.436-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:49:38.442-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:51:21.713-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:51:21.719-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:51:21.719-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:51:21.733-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-13T22:51:21.736-0300] {standard_task_runner.py:57} INFO - Started process 48526 to run task
[2023-07-13T22:51:21.741-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1xmwbd4n']
[2023-07-13T22:51:21.742-0300] {standard_task_runner.py:85} INFO - Job 26: Subtask transform_twitter_datascience
[2023-07-13T22:51:21.779-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:51:21.823-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-13T22:51:21.826-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:51:21.827-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest FormacaoAirflow/dados_transformation --process-date 2023-07-09
[2023-07-13T22:51:23.214-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:23 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:51:23.216-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:51:23.982-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:23 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:51:24.042-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:51:24.123-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO ResourceUtils: ==============================================================
[2023-07-13T22:51:24.124-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:51:24.124-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO ResourceUtils: ==============================================================
[2023-07-13T22:51:24.125-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:51:24.144-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:51:24.157-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:51:24.159-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:51:24.227-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:51:24.228-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:51:24.233-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:51:24.235-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:51:24.236-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:51:24.492-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO Utils: Successfully started service 'sparkDriver' on port 40175.
[2023-07-13T22:51:24.513-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:51:24.541-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:51:24.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:51:24.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:51:24.563-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:51:24.581-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-352f43ba-75a0-48d6-9a25-da068e2cc142
[2023-07-13T22:51:24.595-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:51:24.611-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:51:24.781-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:51:24.863-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:51:24.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:51:24.884-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43533.
[2023-07-13T22:51:24.884-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO NettyBlockTransferService: Server created on 192.168.0.177:43533
[2023-07-13T22:51:24.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:51:24.896-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 43533, None)
[2023-07-13T22:51:24.898-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:43533 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 43533, None)
[2023-07-13T22:51:24.900-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 43533, None)
[2023-07-13T22:51:24.901-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 43533, None)
[2023-07-13T22:51:25.262-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:51:25.270-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:25 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:51:26.141-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:26 INFO InMemoryFileIndex: It took 89 ms to list leaf files for 1 paths.
[2023-07-13T22:51:26.392-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:26 INFO InMemoryFileIndex: It took 16 ms to list leaf files for 8 paths.
[2023-07-13T22:51:28.415-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:51:28.417-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:51:28.419-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:51:28.629-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:51:28.678-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:51:28.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:43533 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:28.684-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:28.692-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649517 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:28.834-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:28.845-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:28.846-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:28.846-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:28.847-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:28.850-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:29.012-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:51:29.014-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:51:29.014-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:43533 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:29.015-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:29.027-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:29.032-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:51:29.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:29.153-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:51:29.331-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:51:29.465-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO CodeGenerator: Code generated in 106.246557 ms
[2023-07-13T22:51:29.501-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [empty row]
[2023-07-13T22:51:29.508-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [empty row]
[2023-07-13T22:51:29.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:51:29.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:51:29.519-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [empty row]
[2023-07-13T22:51:29.524-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [empty row]
[2023-07-13T22:51:29.529-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [empty row]
[2023-07-13T22:51:29.542-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:51:29.549-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 447 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:29.551-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:51:29.555-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,695 s
[2023-07-13T22:51:29.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:29.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:51:29.560-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,726303 s
[2023-07-13T22:51:29.888-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:51:29.890-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:51:29.891-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:51:29.892-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:51:29.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:29.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:29.955-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:30.070-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:43533 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:30.087-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:43533 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:51:30.172-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO CodeGenerator: Code generated in 116.157021 ms
[2023-07-13T22:51:30.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:51:30.189-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:51:30.190-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:43533 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:51:30.191-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:30.195-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649517 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:30.251-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:30.253-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:30.253-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:30.254-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:30.254-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:30.255-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:30.308-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.4 KiB, free 433.9 MiB)
[2023-07-13T22:51:30.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:51:30.315-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:43533 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:51:30.317-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:30.318-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:30.319-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:51:30.325-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:30.326-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:51:30.374-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:30.374-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:30.375-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:30.406-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:51:30.448-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO CodeGenerator: Code generated in 36.703145 ms
[2023-07-13T22:51:30.507-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO CodeGenerator: Code generated in 13.240652 ms
[2023-07-13T22:51:30.568-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [19546]
[2023-07-13T22:51:30.590-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [19548]
[2023-07-13T22:51:30.615-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:51:30.626-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:51:30.642-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [19550]
[2023-07-13T22:51:30.655-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [19547]
[2023-07-13T22:51:30.669-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [19549]
[2023-07-13T22:51:30.698-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_202307132251308209225413334603559_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-09/_temporary/0/task_202307132251308209225413334603559_0001_m_000000
[2023-07-13T22:51:30.699-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO SparkHadoopMapRedUtil: attempt_202307132251308209225413334603559_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:51:30.707-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:51:30.715-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 393 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:30.715-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:51:30.716-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,460 s
[2023-07-13T22:51:30.717-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:30.717-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:51:30.718-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,466419 s
[2023-07-13T22:51:30.722-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileFormatWriter: Start to commit write Job 31f9455b-b1ae-4633-b717-60405ff4e108.
[2023-07-13T22:51:30.756-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileFormatWriter: Write Job 31f9455b-b1ae-4633-b717-60405ff4e108 committed. Elapsed time: 32 ms.
[2023-07-13T22:51:30.767-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileFormatWriter: Finished processing stats for write job 31f9455b-b1ae-4633-b717-60405ff4e108.
[2023-07-13T22:51:30.851-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:51:30.851-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:51:30.851-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:51:30.851-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:51:30.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:30.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:30.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:30.935-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO CodeGenerator: Code generated in 32.49259 ms
[2023-07-13T22:51:30.943-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:51:30.968-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:51:30.969-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:43533 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:51:30.970-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:30.973-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33649517 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:51:31.031-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:51:31.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:51:31.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:51:31.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:51:31.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:51:31.035-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:51:31.067-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:51:31.071-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:51:31.073-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:43533 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:51:31.074-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:51:31.074-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:51:31.075-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:51:31.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:51:31.083-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:51:31.113-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:51:31.113-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:51:31.114-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:51:31.133-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:51:31.157-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO CodeGenerator: Code generated in 21.427377 ms
[2023-07-13T22:51:31.169-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-18029, partition values: [19546]
[2023-07-13T22:51:31.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-13537, partition values: [19548]
[2023-07-13T22:51:31.184-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:51:31.188-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:51:31.196-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-8976, partition values: [19550]
[2023-07-13T22:51:31.199-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4568, partition values: [19547]
[2023-07-13T22:51:31.201-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4547, partition values: [19549]
[2023-07-13T22:51:31.205-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileOutputCommitter: Saved output of task 'attempt_20230713225130515566643241079226_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/FormacaoAirflow/dados_transformation/user/process_date=2023-07-09/_temporary/0/task_20230713225130515566643241079226_0002_m_000000
[2023-07-13T22:51:31.205-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO SparkHadoopMapRedUtil: attempt_20230713225130515566643241079226_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:51:31.206-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:51:31.208-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 129 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:51:31.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:51:31.210-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,173 s
[2023-07-13T22:51:31.211-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:51:31.211-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:51:31.211-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,178502 s
[2023-07-13T22:51:31.211-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileFormatWriter: Start to commit write Job 8c61241d-8e5a-4395-9048-13d8532d2ca7.
[2023-07-13T22:51:31.225-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileFormatWriter: Write Job 8c61241d-8e5a-4395-9048-13d8532d2ca7 committed. Elapsed time: 14 ms.
[2023-07-13T22:51:31.226-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO FileFormatWriter: Finished processing stats for write job 8c61241d-8e5a-4395-9048-13d8532d2ca7.
[2023-07-13T22:51:31.263-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:51:31.274-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:51:31.285-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:51:31.294-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:51:31.294-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO BlockManager: BlockManager stopped
[2023-07-13T22:51:31.298-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:51:31.300-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:51:31.304-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:51:31.304-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:51:31.305-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-10c78660-509d-4d37-9c95-73647ad5b132/pyspark-6e7bf686-5747-475d-a80d-166b0d52e20e
[2023-07-13T22:51:31.307-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-ad1e9e54-7ba2-4303-a108-3f87906d2747
[2023-07-13T22:51:31.310-0300] {spark_submit.py:492} INFO - 23/07/13 22:51:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-10c78660-509d-4d37-9c95-73647ad5b132
[2023-07-13T22:51:31.365-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T015121, end_date=20230714T015131
[2023-07-13T22:51:31.376-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:51:31.381-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:52:56.966-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:52:56.970-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-13T22:52:56.970-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:52:56.979-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-13T22:52:56.982-0300] {standard_task_runner.py:57} INFO - Started process 50473 to run task
[2023-07-13T22:52:56.983-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpgz7c5_wz']
[2023-07-13T22:52:56.984-0300] {standard_task_runner.py:85} INFO - Job 26: Subtask transform_twitter_datascience
[2023-07-13T22:52:57.008-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:52:57.063-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-13T22:52:57.066-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:52:57.067-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest dados_transformation --process-date 2023-07-09
[2023-07-13T22:52:58.275-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:58 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:52:58.277-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:52:58.915-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:58 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:52:58.958-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:52:59.020-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO ResourceUtils: ==============================================================
[2023-07-13T22:52:59.020-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:52:59.021-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO ResourceUtils: ==============================================================
[2023-07-13T22:52:59.021-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:52:59.038-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:52:59.048-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:52:59.049-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:52:59.089-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:52:59.089-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:52:59.089-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:52:59.090-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:52:59.090-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:52:59.259-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO Utils: Successfully started service 'sparkDriver' on port 41539.
[2023-07-13T22:52:59.279-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:52:59.305-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:52:59.319-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:52:59.319-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:52:59.323-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:52:59.339-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a6e99842-568c-4d22-8149-d1a21c0bfc34
[2023-07-13T22:52:59.352-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:52:59.366-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:52:59.535-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:52:59.608-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:52:59.614-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:52:59.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45231.
[2023-07-13T22:52:59.628-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO NettyBlockTransferService: Server created on 192.168.0.177:45231
[2023-07-13T22:52:59.629-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:52:59.633-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 45231, None)
[2023-07-13T22:52:59.636-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:45231 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 45231, None)
[2023-07-13T22:52:59.638-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 45231, None)
[2023-07-13T22:52:59.639-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 45231, None)
[2023-07-13T22:52:59.968-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:52:59.973-0300] {spark_submit.py:492} INFO - 23/07/13 22:52:59 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:53:00.653-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:00 INFO InMemoryFileIndex: It took 54 ms to list leaf files for 1 paths.
[2023-07-13T22:53:00.771-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:00 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:53:02.230-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:02.231-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:02.234-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:53:02.440-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:53:02.479-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:02.482-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:45231 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:02.485-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:02.493-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33690425 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:02.615-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:02.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:02.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:02.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:02.629-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:02.632-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:02.728-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:53:02.730-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:02.730-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:45231 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:02.731-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:02.741-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:02.741-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:53:02.779-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:02.790-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:53:02.901-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:02 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [empty row]
[2023-07-13T22:53:03.025-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO CodeGenerator: Code generated in 92.931831 ms
[2023-07-13T22:53:03.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:53:03.066-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [empty row]
[2023-07-13T22:53:03.071-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [empty row]
[2023-07-13T22:53:03.074-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [empty row]
[2023-07-13T22:53:03.077-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:53:03.079-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:53:03.081-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [empty row]
[2023-07-13T22:53:03.094-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:53:03.099-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 328 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:03.101-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:53:03.105-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,463 s
[2023-07-13T22:53:03.107-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:03.107-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:53:03.109-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,493941 s
[2023-07-13T22:53:03.280-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:45231 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:03.286-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:45231 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:03.466-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:03.468-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:53:03.469-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:53:03.469-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:53:03.526-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:03.527-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:03.528-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:03.664-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO CodeGenerator: Code generated in 75.365219 ms
[2023-07-13T22:53:03.668-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:03.676-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:53:03.676-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:45231 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:53:03.677-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:03.681-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33690425 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:03.733-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:03.734-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:03.734-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:03.734-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:03.735-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:03.735-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:03.754-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.3 KiB, free 433.9 MiB)
[2023-07-13T22:53:03.757-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:53:03.758-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:45231 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:53:03.758-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:03.759-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:03.759-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:53:03.762-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:03.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:53:03.796-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:03.797-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:03.797-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:03.820-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:03.844-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO CodeGenerator: Code generated in 18.668826 ms
[2023-07-13T22:53:03.859-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO CodeGenerator: Code generated in 3.62969 ms
[2023-07-13T22:53:03.892-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:53:03.900-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [19549]
[2023-07-13T22:53:03.908-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:03.913-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:03.918-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:03.922-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:03.927-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [19550]
[2023-07-13T22:53:03.938-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253037577157912508466255_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-09/_temporary/0/task_202307132253037577157912508466255_0001_m_000000
[2023-07-13T22:53:03.939-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO SparkHadoopMapRedUtil: attempt_202307132253037577157912508466255_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:53:03.945-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:53:03.946-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 186 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:03.947-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:53:03.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,211 s
[2023-07-13T22:53:03.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:03.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:53:03.949-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,215119 s
[2023-07-13T22:53:03.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileFormatWriter: Start to commit write Job 672ae5b3-0770-45b8-b8fb-9068ce17cac9.
[2023-07-13T22:53:03.961-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileFormatWriter: Write Job 672ae5b3-0770-45b8-b8fb-9068ce17cac9 committed. Elapsed time: 8 ms.
[2023-07-13T22:53:03.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileFormatWriter: Finished processing stats for write job 672ae5b3-0770-45b8-b8fb-9068ce17cac9.
[2023-07-13T22:53:03.992-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:03.993-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:03.993-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:03.993-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:03 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:53:04.003-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:04.003-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:04.003-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:04.026-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO CodeGenerator: Code generated in 8.95736 ms
[2023-07-13T22:53:04.029-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:53:04.037-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:53:04.037-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:45231 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:53:04.038-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:04.039-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33690425 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:04.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:04.060-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:04.060-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:04.060-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:04.060-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:04.061-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:04.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:53:04.078-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:53:04.079-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:45231 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:53:04.080-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:04.080-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:04.080-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:53:04.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:04.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:53:04.096-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:04.096-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:04.097-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:04.107-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:04.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO CodeGenerator: Code generated in 10.485396 ms
[2023-07-13T22:53:04.126-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:53:04.130-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-27128, partition values: [19549]
[2023-07-13T22:53:04.134-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:04.136-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:04.139-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:04.141-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:04.144-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-9011, partition values: [19550]
[2023-07-13T22:53:04.148-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253048177029135751937371_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-09/_temporary/0/task_202307132253048177029135751937371_0002_m_000000
[2023-07-13T22:53:04.149-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SparkHadoopMapRedUtil: attempt_202307132253048177029135751937371_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:53:04.150-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:53:04.151-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 70 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:04.151-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:53:04.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,089 s
[2023-07-13T22:53:04.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:04.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:53:04.152-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,093107 s
[2023-07-13T22:53:04.153-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileFormatWriter: Start to commit write Job fd3869db-4d32-4e64-8e08-66d14df405a7.
[2023-07-13T22:53:04.161-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileFormatWriter: Write Job fd3869db-4d32-4e64-8e08-66d14df405a7 committed. Elapsed time: 7 ms.
[2023-07-13T22:53:04.161-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO FileFormatWriter: Finished processing stats for write job fd3869db-4d32-4e64-8e08-66d14df405a7.
[2023-07-13T22:53:04.181-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:53:04.189-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:53:04.197-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:53:04.203-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:53:04.203-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO BlockManager: BlockManager stopped
[2023-07-13T22:53:04.205-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:53:04.206-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:53:04.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:53:04.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:53:04.210-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-0c45241d-dbf0-4429-8422-fbc7b8322b2b
[2023-07-13T22:53:04.212-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-dfba5ba6-693b-4889-b989-33025e90e33b/pyspark-04ccd68d-5901-4438-a747-ef226f281945
[2023-07-13T22:53:04.213-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-dfba5ba6-693b-4889-b989-33025e90e33b
[2023-07-13T22:53:04.253-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T015256, end_date=20230714T015304
[2023-07-13T22:53:04.279-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:53:04.285-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T15:26:02.505-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T15:26:02.521-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T15:26:02.521-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T15:26:02.552-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-14T15:26:02.560-0300] {standard_task_runner.py:57} INFO - Started process 14160 to run task
[2023-07-14T15:26:02.565-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '44', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_a1epbvr']
[2023-07-14T15:26:02.567-0300] {standard_task_runner.py:85} INFO - Job 44: Subtask transform_twitter_datascience
[2023-07-14T15:26:02.641-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T15:26:02.775-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-14T15:26:02.786-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T15:26:02.788-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09 --dest /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09 --process-date 2023-07-09
[2023-07-14T15:26:06.442-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:06 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T15:26:06.448-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T15:26:08.193-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T15:26:08.287-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T15:26:08.451-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO ResourceUtils: ==============================================================
[2023-07-14T15:26:08.452-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T15:26:08.452-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO ResourceUtils: ==============================================================
[2023-07-14T15:26:08.453-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T15:26:08.498-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T15:26:08.523-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T15:26:08.525-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T15:26:08.637-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T15:26:08.638-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T15:26:08.638-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T15:26:08.639-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T15:26:08.640-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T15:26:09.160-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO Utils: Successfully started service 'sparkDriver' on port 42275.
[2023-07-14T15:26:09.216-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T15:26:09.305-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T15:26:09.354-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T15:26:09.356-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T15:26:09.367-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T15:26:09.424-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ae8b02e9-777e-456c-9fac-c3a2b3a01170
[2023-07-14T15:26:09.485-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T15:26:09.548-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T15:26:10.158-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T15:26:10.259-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T15:26:10.274-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T15:26:10.299-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33837.
[2023-07-14T15:26:10.299-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO NettyBlockTransferService: Server created on 192.168.0.102:33837
[2023-07-14T15:26:10.304-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T15:26:10.321-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 33837, None)
[2023-07-14T15:26:10.332-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:33837 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 33837, None)
[2023-07-14T15:26:10.340-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 33837, None)
[2023-07-14T15:26:10.341-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 33837, None)
[2023-07-14T15:26:10.893-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T15:26:10.901-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:10 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T15:26:12.822-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:12 INFO InMemoryFileIndex: It took 147 ms to list leaf files for 1 paths.
[2023-07-14T15:26:13.041-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:13 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
[2023-07-14T15:26:17.340-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:17 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:26:17.342-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:17 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:26:17.346-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:17 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T15:26:17.902-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T15:26:17.975-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T15:26:17.980-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:33837 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:26:17.991-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:17 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:18.009-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203420 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:26:18.356-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:18.391-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:26:18.391-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:26:18.392-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:26:18.395-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:26:18.407-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:26:18.661-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T15:26:18.669-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T15:26:18.672-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:33837 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:26:18.673-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:26:18.700-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:26:18.705-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T15:26:18.818-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5007 bytes) taskResourceAssignments Map()
[2023-07-14T15:26:18.868-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T15:26:19.251-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-9116, partition values: [empty row]
[2023-07-14T15:26:19.672-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO CodeGenerator: Code generated in 322.238986 ms
[2023-07-14T15:26:19.794-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T15:26:19.812-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1015 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:26:19.821-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T15:26:19.832-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,389 s
[2023-07-14T15:26:19.840-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:26:19.841-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T15:26:19.846-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:19 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,486995 s
[2023-07-14T15:26:20.199-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:33837 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:26:20.213-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:33837 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:26:20.718-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T15:26:20.724-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:20 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T15:26:20.726-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:20 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T15:26:20.932-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:26:20.932-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:26:20.934-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:26:21.316-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO CodeGenerator: Code generated in 189.409634 ms
[2023-07-14T15:26:21.330-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T15:26:21.356-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T15:26:21.357-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:33837 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T15:26:21.359-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:21.370-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203420 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:26:21.483-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:21.489-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:26:21.489-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:26:21.490-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:26:21.490-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:26:21.491-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:26:21.540-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T15:26:21.545-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T15:26:21.546-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:33837 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T15:26:21.547-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:26:21.549-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:26:21.550-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T15:26:21.558-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:26:21.559-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T15:26:21.679-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:26:21.679-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:26:21.681-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:26:21.759-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-9116, partition values: [empty row]
[2023-07-14T15:26:21.873-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO CodeGenerator: Code generated in 103.73548 ms
[2023-07-14T15:26:21.935-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:21 INFO CodeGenerator: Code generated in 23.282374 ms
[2023-07-14T15:26:22.049-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileOutputCommitter: Saved output of task 'attempt_202307141526215796727177905501362_0001_m_000000_1' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09/tweet/process_date=2023-07-09/_temporary/0/task_202307141526215796727177905501362_0001_m_000000
[2023-07-14T15:26:22.051-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SparkHadoopMapRedUtil: attempt_202307141526215796727177905501362_0001_m_000000_1: Committed. Elapsed time: 4 ms.
[2023-07-14T15:26:22.071-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-14T15:26:22.083-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 527 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:26:22.086-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T15:26:22.091-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,596 s
[2023-07-14T15:26:22.091-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:26:22.091-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T15:26:22.095-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,610084 s
[2023-07-14T15:26:22.105-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileFormatWriter: Start to commit write Job 8e76038b-9e40-4c0f-9bd6-2c1cebb5d5b4.
[2023-07-14T15:26:22.165-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileFormatWriter: Write Job 8e76038b-9e40-4c0f-9bd6-2c1cebb5d5b4 committed. Elapsed time: 54 ms.
[2023-07-14T15:26:22.174-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileFormatWriter: Finished processing stats for write job 8e76038b-9e40-4c0f-9bd6-2c1cebb5d5b4.
[2023-07-14T15:26:22.267-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:26:22.270-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:26:22.274-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T15:26:22.310-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:26:22.312-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:26:22.315-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:26:22.440-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO CodeGenerator: Code generated in 48.693511 ms
[2023-07-14T15:26:22.452-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T15:26:22.478-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T15:26:22.480-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:33837 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T15:26:22.482-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:22.488-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203420 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:26:22.528-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:26:22.530-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:26:22.530-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:26:22.530-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:26:22.530-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:26:22.536-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:26:22.569-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T15:26:22.573-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T15:26:22.574-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:33837 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T15:26:22.575-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:26:22.575-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:26:22.576-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T15:26:22.578-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:26:22.579-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T15:26:22.618-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:26:22.619-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:26:22.620-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:26:22.646-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-9116, partition values: [empty row]
[2023-07-14T15:26:22.675-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO CodeGenerator: Code generated in 20.724555 ms
[2023-07-14T15:26:22.692-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileOutputCommitter: Saved output of task 'attempt_202307141526221746132174698707707_0002_m_000000_2' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09/user/process_date=2023-07-09/_temporary/0/task_202307141526221746132174698707707_0002_m_000000
[2023-07-14T15:26:22.692-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SparkHadoopMapRedUtil: attempt_202307141526221746132174698707707_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-14T15:26:22.695-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T15:26:22.704-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 124 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:26:22.706-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T15:26:22.708-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,169 s
[2023-07-14T15:26:22.708-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:26:22.709-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T15:26:22.709-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,181064 s
[2023-07-14T15:26:22.712-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileFormatWriter: Start to commit write Job 3be6d4aa-421d-4bcb-bf83-45653ed9e073.
[2023-07-14T15:26:22.742-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileFormatWriter: Write Job 3be6d4aa-421d-4bcb-bf83-45653ed9e073 committed. Elapsed time: 29 ms.
[2023-07-14T15:26:22.744-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO FileFormatWriter: Finished processing stats for write job 3be6d4aa-421d-4bcb-bf83-45653ed9e073.
[2023-07-14T15:26:22.841-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T15:26:22.860-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T15:26:22.871-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T15:26:22.883-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO MemoryStore: MemoryStore cleared
[2023-07-14T15:26:22.884-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO BlockManager: BlockManager stopped
[2023-07-14T15:26:22.887-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T15:26:22.889-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T15:26:22.893-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T15:26:22.893-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T15:26:22.894-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-54aeb42e-08e7-4456-aacc-24fb130929f7
[2023-07-14T15:26:22.896-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7f94ec51-16ac-4e70-a7e1-6cb7dc7a7bf2/pyspark-80562250-7bd6-4477-86a0-f15a0c344f8e
[2023-07-14T15:26:22.899-0300] {spark_submit.py:492} INFO - 23/07/14 15:26:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7f94ec51-16ac-4e70-a7e1-6cb7dc7a7bf2
[2023-07-14T15:26:22.983-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T182602, end_date=20230714T182622
[2023-07-14T15:26:23.036-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T15:26:23.065-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:11:17.990-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T16:11:17.995-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T16:11:17.995-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:11:18.004-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-14T16:11:18.006-0300] {standard_task_runner.py:57} INFO - Started process 21548 to run task
[2023-07-14T16:11:18.008-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpie8y1mnn']
[2023-07-14T16:11:18.009-0300] {standard_task_runner.py:85} INFO - Job 46: Subtask transform_twitter_datascience
[2023-07-14T16:11:18.029-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:11:18.097-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-14T16:11:18.104-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T16:11:18.107-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09 --process-date 2023-07-09
[2023-07-14T16:11:19.690-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:19 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T16:11:19.693-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T16:11:20.776-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:20 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T16:11:20.872-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T16:11:21.044-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO ResourceUtils: ==============================================================
[2023-07-14T16:11:21.044-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T16:11:21.044-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO ResourceUtils: ==============================================================
[2023-07-14T16:11:21.045-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T16:11:21.060-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T16:11:21.069-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T16:11:21.069-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T16:11:21.103-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T16:11:21.104-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T16:11:21.104-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T16:11:21.104-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T16:11:21.105-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T16:11:21.323-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO Utils: Successfully started service 'sparkDriver' on port 35257.
[2023-07-14T16:11:21.352-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T16:11:21.384-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T16:11:21.399-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T16:11:21.400-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T16:11:21.404-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T16:11:21.424-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fe12e362-f064-45e6-bf1a-8bd840ab0954
[2023-07-14T16:11:21.444-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T16:11:21.465-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T16:11:21.639-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T16:11:21.749-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T16:11:21.755-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T16:11:21.774-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40623.
[2023-07-14T16:11:21.774-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO NettyBlockTransferService: Server created on 192.168.0.102:40623
[2023-07-14T16:11:21.776-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T16:11:21.783-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 40623, None)
[2023-07-14T16:11:21.786-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:40623 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 40623, None)
[2023-07-14T16:11:21.788-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 40623, None)
[2023-07-14T16:11:21.790-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 40623, None)
[2023-07-14T16:11:22.357-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T16:11:22.362-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:22 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T16:11:23.105-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:23 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.
[2023-07-14T16:11:23.160-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-07-14T16:11:24.819-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:24 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:11:24.819-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:11:24.821-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:24 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T16:11:25.013-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T16:11:25.050-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T16:11:25.053-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:40623 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:25.056-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:25.061-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198814 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:11:25.177-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:25.188-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:11:25.189-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:11:25.189-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:11:25.190-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:11:25.194-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:11:25.300-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T16:11:25.302-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T16:11:25.303-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:40623 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:25.304-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:11:25.319-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:11:25.320-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T16:11:25.380-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T16:11:25.399-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T16:11:25.665-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4510, partition values: [empty row]
[2023-07-14T16:11:25.871-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO CodeGenerator: Code generated in 166.819355 ms
[2023-07-14T16:11:25.941-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-07-14T16:11:25.950-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 583 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:11:25.953-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T16:11:25.964-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,759 s
[2023-07-14T16:11:25.968-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:11:25.971-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T16:11:25.987-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:25 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,797980 s
[2023-07-14T16:11:26.106-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:40623 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:26.111-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:40623 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:11:26.368-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T16:11:26.370-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T16:11:26.370-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T16:11:26.439-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:11:26.440-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:11:26.441-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:11:26.586-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO CodeGenerator: Code generated in 81.107968 ms
[2023-07-14T16:11:26.591-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T16:11:26.600-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T16:11:26.601-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:40623 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:11:26.602-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:26.604-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198814 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:11:26.656-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:26.657-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:11:26.658-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:11:26.658-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:11:26.658-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:11:26.659-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:11:26.683-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T16:11:26.686-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T16:11:26.686-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:40623 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T16:11:26.687-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:11:26.687-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:11:26.688-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T16:11:26.691-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:11:26.691-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T16:11:26.748-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:11:26.748-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:11:26.749-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:11:26.777-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4510, partition values: [empty row]
[2023-07-14T16:11:26.804-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO CodeGenerator: Code generated in 22.663845 ms
[2023-07-14T16:11:26.827-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO CodeGenerator: Code generated in 5.805612 ms
[2023-07-14T16:11:26.854-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileOutputCommitter: Saved output of task 'attempt_202307141611267122472631705628217_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09/tweet/process_date=2023-07-09/_temporary/0/task_202307141611267122472631705628217_0001_m_000000
[2023-07-14T16:11:26.855-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SparkHadoopMapRedUtil: attempt_202307141611267122472631705628217_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T16:11:26.862-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-14T16:11:26.864-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 176 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:11:26.864-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T16:11:26.865-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,205 s
[2023-07-14T16:11:26.865-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:11:26.865-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T16:11:26.866-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,210133 s
[2023-07-14T16:11:26.868-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileFormatWriter: Start to commit write Job f76e668d-9a83-46bb-9ad1-b0bebb98ed10.
[2023-07-14T16:11:26.881-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileFormatWriter: Write Job f76e668d-9a83-46bb-9ad1-b0bebb98ed10 committed. Elapsed time: 12 ms.
[2023-07-14T16:11:26.884-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileFormatWriter: Finished processing stats for write job f76e668d-9a83-46bb-9ad1-b0bebb98ed10.
[2023-07-14T16:11:26.914-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:11:26.914-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:11:26.914-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T16:11:26.921-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:11:26.922-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:11:26.922-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:11:26.951-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO CodeGenerator: Code generated in 13.864714 ms
[2023-07-14T16:11:26.956-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T16:11:26.965-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T16:11:26.966-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:40623 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T16:11:26.966-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:26.968-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198814 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:11:26.988-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:11:26.989-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:11:26.989-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:11:26.989-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:11:26.989-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:11:26.990-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:26 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:11:27.007-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T16:11:27.009-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.8 KiB, free 433.4 MiB)
[2023-07-14T16:11:27.010-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:40623 (size: 76.8 KiB, free: 434.2 MiB)
[2023-07-14T16:11:27.010-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:11:27.011-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:11:27.011-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T16:11:27.012-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:11:27.013-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T16:11:27.034-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:11:27.034-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:11:27.035-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:11:27.047-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4510, partition values: [empty row]
[2023-07-14T16:11:27.058-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO CodeGenerator: Code generated in 9.19278 ms
[2023-07-14T16:11:27.065-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO FileOutputCommitter: Saved output of task 'attempt_202307141611264886118354684357875_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09/user/process_date=2023-07-09/_temporary/0/task_202307141611264886118354684357875_0002_m_000000
[2023-07-14T16:11:27.065-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO SparkHadoopMapRedUtil: attempt_202307141611264886118354684357875_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-14T16:11:27.066-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T16:11:27.068-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 56 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:11:27.068-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T16:11:27.069-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,078 s
[2023-07-14T16:11:27.070-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:11:27.070-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T16:11:27.070-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,082124 s
[2023-07-14T16:11:27.071-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO FileFormatWriter: Start to commit write Job d42a13f0-dab2-46e8-8de7-687e10fc2337.
[2023-07-14T16:11:27.082-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO FileFormatWriter: Write Job d42a13f0-dab2-46e8-8de7-687e10fc2337 committed. Elapsed time: 11 ms.
[2023-07-14T16:11:27.083-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO FileFormatWriter: Finished processing stats for write job d42a13f0-dab2-46e8-8de7-687e10fc2337.
[2023-07-14T16:11:27.111-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T16:11:27.122-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T16:11:27.130-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T16:11:27.137-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO MemoryStore: MemoryStore cleared
[2023-07-14T16:11:27.137-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO BlockManager: BlockManager stopped
[2023-07-14T16:11:27.139-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T16:11:27.141-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T16:11:27.145-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T16:11:27.145-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T16:11:27.145-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-82ecd140-941e-4291-8d66-32954d97b3e9/pyspark-49955fff-6bf9-4b3d-a4aa-ec687c4a1ed9
[2023-07-14T16:11:27.148-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-82ecd140-941e-4291-8d66-32954d97b3e9
[2023-07-14T16:11:27.150-0300] {spark_submit.py:492} INFO - 23/07/14 16:11:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f35585f-91e0-4620-b833-f44741630409
[2023-07-14T16:11:27.208-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T191117, end_date=20230714T191127
[2023-07-14T16:11:27.273-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:11:27.298-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:28:53.502-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T16:28:53.512-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T16:28:53.512-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:28:53.529-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-14T16:28:53.532-0300] {standard_task_runner.py:57} INFO - Started process 28345 to run task
[2023-07-14T16:28:53.539-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '48', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpgcjhws3b']
[2023-07-14T16:28:53.542-0300] {standard_task_runner.py:85} INFO - Job 48: Subtask transform_twitter_datascience
[2023-07-14T16:28:53.598-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:28:53.747-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-14T16:28:53.758-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T16:28:53.763-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09 --process-date 2023-07-09
[2023-07-14T16:28:55.886-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:55 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T16:28:55.893-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T16:28:57.087-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T16:28:57.210-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T16:28:57.423-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO ResourceUtils: ==============================================================
[2023-07-14T16:28:57.425-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T16:28:57.426-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO ResourceUtils: ==============================================================
[2023-07-14T16:28:57.426-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T16:28:57.453-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T16:28:57.469-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T16:28:57.469-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T16:28:57.534-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T16:28:57.534-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T16:28:57.535-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T16:28:57.535-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T16:28:57.536-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T16:28:57.832-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO Utils: Successfully started service 'sparkDriver' on port 37427.
[2023-07-14T16:28:57.865-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T16:28:57.913-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T16:28:57.933-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T16:28:57.935-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T16:28:57.944-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T16:28:57.975-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07e5ccd5-75be-4964-b547-f17beaf0d296
[2023-07-14T16:28:57.997-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:57 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T16:28:58.019-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T16:28:58.300-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T16:28:58.312-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T16:28:58.476-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T16:28:58.485-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T16:28:58.515-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33211.
[2023-07-14T16:28:58.515-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO NettyBlockTransferService: Server created on 192.168.0.102:33211
[2023-07-14T16:28:58.517-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T16:28:58.529-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 33211, None)
[2023-07-14T16:28:58.533-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:33211 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 33211, None)
[2023-07-14T16:28:58.538-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 33211, None)
[2023-07-14T16:28:58.543-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 33211, None)
[2023-07-14T16:28:59.114-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T16:28:59.124-0300] {spark_submit.py:492} INFO - 23/07/14 16:28:59 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T16:29:00.203-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:00 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
[2023-07-14T16:29:00.312-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:00 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2023-07-14T16:29:03.161-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:03 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:29:03.162-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:03 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:29:03.165-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T16:29:03.710-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T16:29:03.812-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T16:29:03.815-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:33211 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:29:03.825-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:03.835-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198954 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:29:04.050-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:04.065-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:29:04.065-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:29:04.065-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:29:04.067-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:29:04.071-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:29:04.185-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T16:29:04.188-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T16:29:04.189-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:33211 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:29:04.190-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:29:04.221-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:29:04.223-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T16:29:04.291-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T16:29:04.324-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T16:29:04.537-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4650, partition values: [empty row]
[2023-07-14T16:29:04.804-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO CodeGenerator: Code generated in 213.424542 ms
[2023-07-14T16:29:04.895-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-07-14T16:29:04.905-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 632 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:29:04.912-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T16:29:04.920-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,827 s
[2023-07-14T16:29:04.927-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:29:04.929-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T16:29:04.933-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,882613 s
[2023-07-14T16:29:05.648-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T16:29:05.652-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:05 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T16:29:05.653-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:05 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T16:29:05.796-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:29:05.796-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:29:05.797-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:29:06.126-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO CodeGenerator: Code generated in 137.915128 ms
[2023-07-14T16:29:06.129-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:33211 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:29:06.135-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:33211 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:29:06.136-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T16:29:06.151-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T16:29:06.153-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:33211 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:29:06.154-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:06.159-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198954 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:29:06.237-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:06.238-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:29:06.239-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:29:06.239-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:29:06.239-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:29:06.240-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:29:06.261-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T16:29:06.266-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T16:29:06.267-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:33211 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T16:29:06.268-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:29:06.269-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:29:06.269-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T16:29:06.273-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:29:06.274-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T16:29:06.343-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:29:06.343-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:29:06.344-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:29:06.385-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4650, partition values: [empty row]
[2023-07-14T16:29:06.429-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO CodeGenerator: Code generated in 39.91502 ms
[2023-07-14T16:29:06.474-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO CodeGenerator: Code generated in 8.431871 ms
[2023-07-14T16:29:06.516-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileOutputCommitter: Saved output of task 'attempt_202307141629068650263062515406826_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09/tweet/process_date=2023-07-09/_temporary/0/task_202307141629068650263062515406826_0001_m_000000
[2023-07-14T16:29:06.517-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkHadoopMapRedUtil: attempt_202307141629068650263062515406826_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T16:29:06.528-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T16:29:06.530-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 260 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:29:06.530-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T16:29:06.531-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,291 s
[2023-07-14T16:29:06.532-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:29:06.532-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T16:29:06.533-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,295500 s
[2023-07-14T16:29:06.536-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileFormatWriter: Start to commit write Job 290c8f1c-26d5-471d-93fe-01eac2705785.
[2023-07-14T16:29:06.560-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileFormatWriter: Write Job 290c8f1c-26d5-471d-93fe-01eac2705785 committed. Elapsed time: 23 ms.
[2023-07-14T16:29:06.563-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileFormatWriter: Finished processing stats for write job 290c8f1c-26d5-471d-93fe-01eac2705785.
[2023-07-14T16:29:06.619-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:29:06.619-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:29:06.620-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T16:29:06.636-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:29:06.636-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:29:06.637-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:29:06.679-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO CodeGenerator: Code generated in 15.64549 ms
[2023-07-14T16:29:06.683-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T16:29:06.698-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T16:29:06.699-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:33211 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T16:29:06.700-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:06.702-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198954 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:29:06.731-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:29:06.732-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:29:06.732-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:29:06.732-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:29:06.732-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:29:06.734-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:29:06.764-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T16:29:06.770-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.8 KiB, free 433.4 MiB)
[2023-07-14T16:29:06.774-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:33211 (size: 76.8 KiB, free: 434.2 MiB)
[2023-07-14T16:29:06.775-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:29:06.776-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:29:06.776-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T16:29:06.778-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:29:06.778-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T16:29:06.805-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:29:06.805-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:29:06.807-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:29:06.836-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4650, partition values: [empty row]
[2023-07-14T16:29:06.853-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO CodeGenerator: Code generated in 10.88115 ms
[2023-07-14T16:29:06.865-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileOutputCommitter: Saved output of task 'attempt_202307141629066073308597603113683_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-09/user/process_date=2023-07-09/_temporary/0/task_202307141629066073308597603113683_0002_m_000000
[2023-07-14T16:29:06.866-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkHadoopMapRedUtil: attempt_202307141629066073308597603113683_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-14T16:29:06.868-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T16:29:06.870-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 93 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:29:06.871-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T16:29:06.876-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,137 s
[2023-07-14T16:29:06.877-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:29:06.877-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T16:29:06.878-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,146881 s
[2023-07-14T16:29:06.879-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileFormatWriter: Start to commit write Job 1b2b1854-1b8c-4b92-a3fc-036a183ee87e.
[2023-07-14T16:29:06.896-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileFormatWriter: Write Job 1b2b1854-1b8c-4b92-a3fc-036a183ee87e committed. Elapsed time: 17 ms.
[2023-07-14T16:29:06.896-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO FileFormatWriter: Finished processing stats for write job 1b2b1854-1b8c-4b92-a3fc-036a183ee87e.
[2023-07-14T16:29:06.959-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T16:29:06.981-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T16:29:06.999-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T16:29:07.013-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO MemoryStore: MemoryStore cleared
[2023-07-14T16:29:07.013-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO BlockManager: BlockManager stopped
[2023-07-14T16:29:07.016-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T16:29:07.020-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T16:29:07.027-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T16:29:07.028-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T16:29:07.028-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-3d5d377a-0ddf-4a1f-8351-06f14279d233
[2023-07-14T16:29:07.030-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-a7373861-8006-4787-ba15-386de7443721/pyspark-55facf58-6a5c-4564-b746-742e059b0026
[2023-07-14T16:29:07.032-0300] {spark_submit.py:492} INFO - 23/07/14 16:29:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-a7373861-8006-4787-ba15-386de7443721
[2023-07-14T16:29:07.089-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T192853, end_date=20230714T192907
[2023-07-14T16:29:07.127-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:29:07.142-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:29:52.471-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T17:29:52.486-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T17:29:52.486-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:29:52.510-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-14T17:29:52.519-0300] {standard_task_runner.py:57} INFO - Started process 35606 to run task
[2023-07-14T17:29:52.530-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '57', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmphq2pq9zx']
[2023-07-14T17:29:52.533-0300] {standard_task_runner.py:85} INFO - Job 57: Subtask transform_twitter_datascience
[2023-07-14T17:29:52.601-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:29:52.747-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-14T17:29:52.763-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T17:29:52.769-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-09
[2023-07-14T17:29:56.183-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:56 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T17:29:56.186-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T17:29:57.520-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T17:29:57.618-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T17:29:57.785-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO ResourceUtils: ==============================================================
[2023-07-14T17:29:57.785-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T17:29:57.786-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO ResourceUtils: ==============================================================
[2023-07-14T17:29:57.787-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T17:29:57.818-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T17:29:57.834-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T17:29:57.835-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T17:29:57.936-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T17:29:57.936-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T17:29:57.937-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T17:29:57.938-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T17:29:57.938-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T17:29:58.305-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO Utils: Successfully started service 'sparkDriver' on port 44067.
[2023-07-14T17:29:58.345-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T17:29:58.408-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T17:29:58.436-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T17:29:58.437-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T17:29:58.449-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T17:29:58.499-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8e5752d9-5e59-4ba3-8f46-20e0f00343bc
[2023-07-14T17:29:58.527-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T17:29:58.555-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T17:29:58.834-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T17:29:58.849-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:58 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T17:29:59.039-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T17:29:59.054-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T17:29:59.093-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34967.
[2023-07-14T17:29:59.094-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO NettyBlockTransferService: Server created on 192.168.0.102:34967
[2023-07-14T17:29:59.101-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T17:29:59.113-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 34967, None)
[2023-07-14T17:29:59.125-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:34967 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 34967, None)
[2023-07-14T17:29:59.133-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 34967, None)
[2023-07-14T17:29:59.136-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 34967, None)
[2023-07-14T17:29:59.809-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T17:29:59.815-0300] {spark_submit.py:492} INFO - 23/07/14 17:29:59 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T17:30:01.092-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:01 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
[2023-07-14T17:30:01.178-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:01 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2023-07-14T17:30:03.949-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:03 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:30:03.951-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:03 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:30:03.953-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T17:30:04.336-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T17:30:04.405-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T17:30:04.411-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:34967 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:04.419-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:04.432-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203299 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:04.656-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:04.694-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:04.696-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:04.700-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:04.702-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:04.706-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:04.939-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T17:30:04.943-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T17:30:04.947-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:34967 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:30:04.949-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:04.966-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:04.968-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T17:30:05.045-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:05.071-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T17:30:05.227-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-8995, partition values: [empty row]
[2023-07-14T17:30:05.504-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO CodeGenerator: Code generated in 216.778208 ms
[2023-07-14T17:30:05.587-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T17:30:05.601-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 578 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:05.602-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T17:30:05.610-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,869 s
[2023-07-14T17:30:05.618-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:05.619-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T17:30:05.638-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:05 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,982345 s
[2023-07-14T17:30:06.318-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T17:30:06.319-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T17:30:06.319-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T17:30:06.429-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:06.430-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:06.432-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:06.825-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO CodeGenerator: Code generated in 198.526262 ms
[2023-07-14T17:30:06.836-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-14T17:30:06.849-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-14T17:30:06.850-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:34967 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:30:06.851-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:06.854-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203299 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:06.945-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:06.949-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:06.950-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:06.950-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:06.950-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:06.951-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:06.999-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.7 MiB)
[2023-07-14T17:30:07.001-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.6 MiB)
[2023-07-14T17:30:07.004-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:34967 (size: 82.1 KiB, free: 434.2 MiB)
[2023-07-14T17:30:07.004-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:07.006-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:07.006-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T17:30:07.012-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:07.016-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T17:30:07.020-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:34967 in memory (size: 7.0 KiB, free: 434.3 MiB)
[2023-07-14T17:30:07.075-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:07.076-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:07.077-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:07.123-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-8995, partition values: [empty row]
[2023-07-14T17:30:07.151-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO CodeGenerator: Code generated in 21.505974 ms
[2023-07-14T17:30:07.175-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO CodeGenerator: Code generated in 4.172864 ms
[2023-07-14T17:30:07.221-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileOutputCommitter: Saved output of task 'attempt_202307141730067746093221432671675_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-09/_temporary/0/task_202307141730067746093221432671675_0001_m_000000
[2023-07-14T17:30:07.222-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SparkHadoopMapRedUtil: attempt_202307141730067746093221432671675_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T17:30:07.234-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T17:30:07.236-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 228 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:07.236-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T17:30:07.237-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,285 s
[2023-07-14T17:30:07.238-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:07.238-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T17:30:07.239-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,292961 s
[2023-07-14T17:30:07.242-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileFormatWriter: Start to commit write Job 9e817f1d-6407-4a2e-8fe2-1e8a6deec101.
[2023-07-14T17:30:07.271-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileFormatWriter: Write Job 9e817f1d-6407-4a2e-8fe2-1e8a6deec101 committed. Elapsed time: 27 ms.
[2023-07-14T17:30:07.274-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileFormatWriter: Finished processing stats for write job 9e817f1d-6407-4a2e-8fe2-1e8a6deec101.
[2023-07-14T17:30:07.380-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:30:07.380-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:30:07.380-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T17:30:07.410-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:07.411-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:07.411-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:07.477-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO CodeGenerator: Code generated in 30.137469 ms
[2023-07-14T17:30:07.489-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.4 MiB)
[2023-07-14T17:30:07.510-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.4 MiB)
[2023-07-14T17:30:07.511-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:34967 (size: 33.9 KiB, free: 434.2 MiB)
[2023-07-14T17:30:07.514-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:07.520-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203299 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:30:07.567-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:30:07.569-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:30:07.569-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:30:07.570-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:30:07.570-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:30:07.571-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:30:07.605-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.2 MiB)
[2023-07-14T17:30:07.637-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.1 MiB)
[2023-07-14T17:30:07.638-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:34967 in memory (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T17:30:07.639-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:34967 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T17:30:07.644-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:30:07.646-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:30:07.646-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T17:30:07.649-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:30:07.649-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:34967 in memory (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:30:07.651-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T17:30:07.687-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:30:07.687-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:30:07.689-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:30:07.724-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:34967 in memory (size: 34.0 KiB, free: 434.3 MiB)
[2023-07-14T17:30:07.769-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-8995, partition values: [empty row]
[2023-07-14T17:30:07.800-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO CodeGenerator: Code generated in 20.405401 ms
[2023-07-14T17:30:07.822-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileOutputCommitter: Saved output of task 'attempt_202307141730073034770236318135860_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-09/_temporary/0/task_202307141730073034770236318135860_0002_m_000000
[2023-07-14T17:30:07.823-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SparkHadoopMapRedUtil: attempt_202307141730073034770236318135860_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-07-14T17:30:07.833-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2622 bytes result sent to driver
[2023-07-14T17:30:07.836-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 188 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:30:07.837-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T17:30:07.840-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,266 s
[2023-07-14T17:30:07.843-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:30:07.845-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T17:30:07.846-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,276951 s
[2023-07-14T17:30:07.849-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileFormatWriter: Start to commit write Job 6a4556fe-3dc2-433f-88ef-917318b75971.
[2023-07-14T17:30:07.877-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileFormatWriter: Write Job 6a4556fe-3dc2-433f-88ef-917318b75971 committed. Elapsed time: 27 ms.
[2023-07-14T17:30:07.879-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO FileFormatWriter: Finished processing stats for write job 6a4556fe-3dc2-433f-88ef-917318b75971.
[2023-07-14T17:30:07.986-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:07 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T17:30:08.000-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T17:30:08.016-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T17:30:08.026-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO MemoryStore: MemoryStore cleared
[2023-07-14T17:30:08.026-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO BlockManager: BlockManager stopped
[2023-07-14T17:30:08.034-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T17:30:08.036-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T17:30:08.041-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T17:30:08.041-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T17:30:08.041-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-69bf16ec-6313-451f-94a9-b76a73f5fe9c
[2023-07-14T17:30:08.046-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-caa59298-dce9-4f54-a935-c17ca0da8400
[2023-07-14T17:30:08.050-0300] {spark_submit.py:492} INFO - 23/07/14 17:30:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-caa59298-dce9-4f54-a935-c17ca0da8400/pyspark-53618054-708e-4bda-8764-fea0c4a9a1ec
[2023-07-14T17:30:08.110-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230714T202952, end_date=20230714T203008
[2023-07-14T17:30:08.128-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:30:08.136-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:37:49.167-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T23:37:49.190-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T23:37:49.191-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:37:49.231-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-14T23:37:49.239-0300] {standard_task_runner.py:57} INFO - Started process 21031 to run task
[2023-07-14T23:37:49.245-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '70', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpd0u0enaa']
[2023-07-14T23:37:49.247-0300] {standard_task_runner.py:85} INFO - Job 70: Subtask transform_twitter_datascience
[2023-07-14T23:37:49.316-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:37:49.417-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-14T23:37:49.422-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T23:37:49.425-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-09
[2023-07-14T23:37:51.699-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:51 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-14T23:37:51.703-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T23:37:53.050-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T23:37:53.133-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T23:37:53.292-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO ResourceUtils: ==============================================================
[2023-07-14T23:37:53.295-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T23:37:53.296-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO ResourceUtils: ==============================================================
[2023-07-14T23:37:53.297-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T23:37:53.339-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T23:37:53.361-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T23:37:53.363-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T23:37:53.438-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T23:37:53.438-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T23:37:53.439-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T23:37:53.439-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T23:37:53.440-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T23:37:53.741-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO Utils: Successfully started service 'sparkDriver' on port 45735.
[2023-07-14T23:37:53.829-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T23:37:53.922-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T23:37:53.949-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T23:37:53.949-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T23:37:53.953-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T23:37:53.976-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-38a3ebdd-cd7a-447f-b7d0-865f0e4418e1
[2023-07-14T23:37:53.999-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T23:37:54.019-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T23:37:54.277-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T23:37:54.295-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T23:37:54.432-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-14T23:37:54.439-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T23:37:54.460-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34043.
[2023-07-14T23:37:54.460-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO NettyBlockTransferService: Server created on 192.168.0.177:34043
[2023-07-14T23:37:54.465-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T23:37:54.472-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 34043, None)
[2023-07-14T23:37:54.475-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:34043 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 34043, None)
[2023-07-14T23:37:54.481-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 34043, None)
[2023-07-14T23:37:54.484-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 34043, None)
[2023-07-14T23:37:55.025-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T23:37:55.035-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:55 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T23:37:56.199-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:56 INFO InMemoryFileIndex: It took 69 ms to list leaf files for 1 paths.
[2023-07-14T23:37:56.322-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:56 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2023-07-14T23:37:59.017-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:37:59.019-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:37:59.022-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T23:37:59.611-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T23:37:59.694-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T23:37:59.701-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:34043 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T23:37:59.706-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:37:59.718-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203344 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:37:59.974-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:37:59.998-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:37:59.998-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:37:59.999-0300] {spark_submit.py:492} INFO - 23/07/14 23:37:59 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:38:00.002-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:38:00.007-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:38:00.154-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T23:38:00.159-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T23:38:00.160-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:34043 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T23:38:00.161-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T23:38:00.201-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:38:00.202-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T23:38:00.291-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T23:38:00.316-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T23:38:00.499-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-9040, partition values: [empty row]
[2023-07-14T23:38:00.733-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO CodeGenerator: Code generated in 184.385281 ms
[2023-07-14T23:38:00.840-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T23:38:00.854-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 583 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:38:00.856-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T23:38:00.866-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,836 s
[2023-07-14T23:38:00.871-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:38:00.871-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T23:38:00.874-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:00 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,899687 s
[2023-07-14T23:38:01.472-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:34043 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T23:38:01.483-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:34043 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T23:38:01.522-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T23:38:01.524-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T23:38:01.524-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T23:38:01.641-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:38:01.641-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:38:01.643-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:38:01.922-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO CodeGenerator: Code generated in 175.818482 ms
[2023-07-14T23:38:01.932-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T23:38:01.945-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T23:38:01.947-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:34043 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T23:38:01.948-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:01.952-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203344 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:38:02.052-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:02.053-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:38:02.054-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:38:02.054-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:38:02.054-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:38:02.055-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:38:02.090-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T23:38:02.094-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T23:38:02.096-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:34043 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T23:38:02.098-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T23:38:02.101-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:38:02.102-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T23:38:02.108-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T23:38:02.109-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T23:38:02.191-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:38:02.191-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:38:02.192-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:38:02.240-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-9040, partition values: [empty row]
[2023-07-14T23:38:02.292-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO CodeGenerator: Code generated in 46.759558 ms
[2023-07-14T23:38:02.351-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO CodeGenerator: Code generated in 13.830189 ms
[2023-07-14T23:38:02.407-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileOutputCommitter: Saved output of task 'attempt_202307142338014159773261042190787_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-09/_temporary/0/task_202307142338014159773261042190787_0001_m_000000
[2023-07-14T23:38:02.408-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SparkHadoopMapRedUtil: attempt_202307142338014159773261042190787_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T23:38:02.418-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T23:38:02.421-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 316 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:38:02.422-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T23:38:02.424-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,367 s
[2023-07-14T23:38:02.424-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:38:02.425-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T23:38:02.427-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,373335 s
[2023-07-14T23:38:02.437-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileFormatWriter: Start to commit write Job 85c1b448-7410-4fb9-b101-51f70e71da8f.
[2023-07-14T23:38:02.459-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileFormatWriter: Write Job 85c1b448-7410-4fb9-b101-51f70e71da8f committed. Elapsed time: 24 ms.
[2023-07-14T23:38:02.468-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileFormatWriter: Finished processing stats for write job 85c1b448-7410-4fb9-b101-51f70e71da8f.
[2023-07-14T23:38:02.540-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:38:02.540-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:38:02.541-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T23:38:02.561-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:38:02.563-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:38:02.564-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:38:02.623-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO CodeGenerator: Code generated in 26.544394 ms
[2023-07-14T23:38:02.631-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T23:38:02.705-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:34043 in memory (size: 82.1 KiB, free: 434.4 MiB)
[2023-07-14T23:38:02.709-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.177:34043 in memory (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T23:38:02.721-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T23:38:02.723-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:34043 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T23:38:02.730-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:02.735-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203344 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:38:02.797-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:38:02.803-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:38:02.803-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:38:02.803-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:38:02.804-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:38:02.808-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:38:02.855-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 434.0 MiB)
[2023-07-14T23:38:02.862-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.8 KiB, free 433.9 MiB)
[2023-07-14T23:38:02.864-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:34043 (size: 76.8 KiB, free: 434.3 MiB)
[2023-07-14T23:38:02.866-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T23:38:02.868-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:38:02.868-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T23:38:02.870-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T23:38:02.871-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T23:38:02.892-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:38:02.892-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:38:02.893-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:38:02.910-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-9040, partition values: [empty row]
[2023-07-14T23:38:02.929-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO CodeGenerator: Code generated in 14.462828 ms
[2023-07-14T23:38:02.940-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileOutputCommitter: Saved output of task 'attempt_202307142338021640361275936172654_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-09/_temporary/0/task_202307142338021640361275936172654_0002_m_000000
[2023-07-14T23:38:02.940-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SparkHadoopMapRedUtil: attempt_202307142338021640361275936172654_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-14T23:38:02.941-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T23:38:02.944-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 74 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:38:02.945-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T23:38:02.947-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,139 s
[2023-07-14T23:38:02.948-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:38:02.948-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T23:38:02.949-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,149735 s
[2023-07-14T23:38:02.949-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileFormatWriter: Start to commit write Job ea11e529-7f61-4176-a3df-67db0d32ce2d.
[2023-07-14T23:38:02.964-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileFormatWriter: Write Job ea11e529-7f61-4176-a3df-67db0d32ce2d committed. Elapsed time: 14 ms.
[2023-07-14T23:38:02.965-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO FileFormatWriter: Finished processing stats for write job ea11e529-7f61-4176-a3df-67db0d32ce2d.
[2023-07-14T23:38:02.999-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:02 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T23:38:03.007-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4041
[2023-07-14T23:38:03.022-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T23:38:03.033-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO MemoryStore: MemoryStore cleared
[2023-07-14T23:38:03.033-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO BlockManager: BlockManager stopped
[2023-07-14T23:38:03.036-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T23:38:03.038-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T23:38:03.042-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T23:38:03.042-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T23:38:03.043-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-b8eda0cb-f607-4943-abc4-b447a3ab5829
[2023-07-14T23:38:03.047-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-daa133d1-64eb-4d1d-87f4-e10284c87e56
[2023-07-14T23:38:03.050-0300] {spark_submit.py:492} INFO - 23/07/14 23:38:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-b8eda0cb-f607-4943-abc4-b447a3ab5829/pyspark-03515a8e-8205-4573-88d1-d5dbf3ef9945
[2023-07-14T23:38:03.116-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230715T023749, end_date=20230715T023803
[2023-07-14T23:38:03.169-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:38:03.198-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:43:25.936-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T23:43:25.943-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-14T23:43:25.944-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:43:25.956-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-14T23:43:25.959-0300] {standard_task_runner.py:57} INFO - Started process 23828 to run task
[2023-07-14T23:43:25.962-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '72', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp5z0bm2gf']
[2023-07-14T23:43:25.964-0300] {standard_task_runner.py:85} INFO - Job 72: Subtask transform_twitter_datascience
[2023-07-14T23:43:26.006-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:43:26.135-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-14T23:43:26.146-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T23:43:26.150-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-09
[2023-07-14T23:43:28.061-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:28 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-14T23:43:28.061-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T23:43:28.071-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-14T23:43:28.071-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-14T23:43:28.071-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-14T23:43:28.071-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-14T23:43:28.071-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-14T23:43:28.463-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T23:43:29.162-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-14T23:43:29.177-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SparkContext: Running Spark version 3.1.3
[2023-07-14T23:43:29.274-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO ResourceUtils: ==============================================================
[2023-07-14T23:43:29.275-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T23:43:29.276-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO ResourceUtils: ==============================================================
[2023-07-14T23:43:29.277-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T23:43:29.337-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T23:43:29.374-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T23:43:29.375-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T23:43:29.443-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T23:43:29.443-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T23:43:29.444-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T23:43:29.444-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T23:43:29.444-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T23:43:29.673-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO Utils: Successfully started service 'sparkDriver' on port 36733.
[2023-07-14T23:43:29.700-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T23:43:29.729-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T23:43:29.751-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T23:43:29.751-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T23:43:29.755-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T23:43:29.768-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3039d76e-081a-41fa-a3ac-b2baec052566
[2023-07-14T23:43:29.790-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T23:43:29.806-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T23:43:30.221-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T23:43:30.233-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T23:43:30.304-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4041
[2023-07-14T23:43:30.491-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-14T23:43:30.518-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38587.
[2023-07-14T23:43:30.518-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO NettyBlockTransferService: Server created on 192.168.0.177:38587
[2023-07-14T23:43:30.520-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T23:43:30.525-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 38587, None)
[2023-07-14T23:43:30.530-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:38587 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 38587, None)
[2023-07-14T23:43:30.534-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 38587, None)
[2023-07-14T23:43:30.535-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 38587, None)
[2023-07-14T23:43:31.036-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-14T23:43:31.036-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:31 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T23:43:31.960-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:31 INFO InMemoryFileIndex: It took 54 ms to list leaf files for 1 paths.
[2023-07-14T23:43:32.071-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:32 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-07-14T23:43:34.060-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:43:34.060-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:43:34.063-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T23:43:34.328-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-14T23:43:34.378-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-14T23:43:34.380-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:38587 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-14T23:43:34.386-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:43:34.393-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198766 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:43:34.569-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:43:34.587-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:43:34.587-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:43:34.587-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:43:34.588-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:43:34.592-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:43:34.696-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-14T23:43:34.698-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-14T23:43:34.700-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:38587 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:43:34.702-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:43:34.719-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:43:34.720-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T23:43:34.813-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-14T23:43:34.840-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T23:43:35.062-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4462, partition values: [empty row]
[2023-07-14T23:43:35.329-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO CodeGenerator: Code generated in 166.544968 ms
[2023-07-14T23:43:35.373-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2023-07-14T23:43:35.380-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 589 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:43:35.385-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T23:43:35.391-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,782 s
[2023-07-14T23:43:35.394-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:43:35.394-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T23:43:35.396-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:35 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,826338 s
[2023-07-14T23:43:36.032-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T23:43:36.035-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-14T23:43:36.037-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T23:43:36.149-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:43:36.149-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:43:36.153-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:43:36.287-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO CodeGenerator: Code generated in 47.750659 ms
[2023-07-14T23:43:36.343-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO CodeGenerator: Code generated in 34.527061 ms
[2023-07-14T23:43:36.353-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-14T23:43:36.361-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)
[2023-07-14T23:43:36.362-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:38587 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-14T23:43:36.363-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:43:36.366-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198766 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:43:36.434-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:43:36.436-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:43:36.437-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:43:36.437-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:43:36.437-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:43:36.438-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:43:36.480-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 433.8 MiB)
[2023-07-14T23:43:36.486-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 433.7 MiB)
[2023-07-14T23:43:36.487-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:38587 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-14T23:43:36.487-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:43:36.489-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:43:36.489-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T23:43:36.494-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:43:36.494-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T23:43:36.570-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:38587 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2023-07-14T23:43:36.581-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:43:36.581-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:43:36.582-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:43:36.647-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO CodeGenerator: Code generated in 23.550816 ms
[2023-07-14T23:43:36.653-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4462, partition values: [empty row]
[2023-07-14T23:43:36.694-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO CodeGenerator: Code generated in 35.312173 ms
[2023-07-14T23:43:36.722-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO CodeGenerator: Code generated in 8.611033 ms
[2023-07-14T23:43:36.760-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileOutputCommitter: Saved output of task 'attempt_202307142343368978158685771959922_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-09/_temporary/0/task_202307142343368978158685771959922_0001_m_000000
[2023-07-14T23:43:36.761-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO SparkHadoopMapRedUtil: attempt_202307142343368978158685771959922_0001_m_000000_1: Committed
[2023-07-14T23:43:36.766-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-14T23:43:36.771-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 280 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:43:36.771-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T23:43:36.772-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,333 s
[2023-07-14T23:43:36.773-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:43:36.773-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T23:43:36.774-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,339495 s
[2023-07-14T23:43:36.795-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileFormatWriter: Write Job df355932-d6fe-434c-97fd-31e0d688bfed committed.
[2023-07-14T23:43:36.799-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileFormatWriter: Finished processing stats for write job df355932-d6fe-434c-97fd-31e0d688bfed.
[2023-07-14T23:43:36.885-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:43:36.886-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:43:36.886-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T23:43:36.904-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:43:36.905-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:43:36.906-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:43:36.951-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO CodeGenerator: Code generated in 17.39564 ms
[2023-07-14T23:43:36.959-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.6 MiB)
[2023-07-14T23:43:36.974-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.6 MiB)
[2023-07-14T23:43:36.975-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:38587 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-14T23:43:36.976-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:43:36.978-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198766 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:43:37.013-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:43:37.016-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:43:37.017-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:43:37.017-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:43:37.018-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:43:37.021-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:43:37.054-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.4 MiB)
[2023-07-14T23:43:37.057-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.3 MiB)
[2023-07-14T23:43:37.058-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:38587 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-14T23:43:37.059-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:43:37.060-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:43:37.060-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T23:43:37.062-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:43:37.063-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T23:43:37.098-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:43:37.098-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:43:37.099-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:43:37.141-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO CodeGenerator: Code generated in 17.008688 ms
[2023-07-14T23:43:37.144-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4462, partition values: [empty row]
[2023-07-14T23:43:37.170-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO CodeGenerator: Code generated in 22.469989 ms
[2023-07-14T23:43:37.184-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO FileOutputCommitter: Saved output of task 'attempt_202307142343369001036983357128576_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-09/_temporary/0/task_202307142343369001036983357128576_0002_m_000000
[2023-07-14T23:43:37.185-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO SparkHadoopMapRedUtil: attempt_202307142343369001036983357128576_0002_m_000000_2: Committed
[2023-07-14T23:43:37.188-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-14T23:43:37.191-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 130 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:43:37.191-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T23:43:37.192-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,170 s
[2023-07-14T23:43:37.193-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:43:37.193-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T23:43:37.193-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,179631 s
[2023-07-14T23:43:37.213-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO FileFormatWriter: Write Job eedd467c-61b8-4187-a3d1-39819147a9d7 committed.
[2023-07-14T23:43:37.213-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO FileFormatWriter: Finished processing stats for write job eedd467c-61b8-4187-a3d1-39819147a9d7.
[2023-07-14T23:43:37.269-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T23:43:37.280-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4041
[2023-07-14T23:43:37.299-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T23:43:37.312-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO MemoryStore: MemoryStore cleared
[2023-07-14T23:43:37.313-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO BlockManager: BlockManager stopped
[2023-07-14T23:43:37.318-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T23:43:37.323-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T23:43:37.329-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T23:43:37.329-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T23:43:37.330-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ac58de9-cdbe-42a3-9f55-e678b0dc21f0/pyspark-6d97859a-21ba-4e69-bf17-04f164804f88
[2023-07-14T23:43:37.335-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ac58de9-cdbe-42a3-9f55-e678b0dc21f0
[2023-07-14T23:43:37.340-0300] {spark_submit.py:492} INFO - 23/07/14 23:43:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-73929a0d-1fca-4189-ac80-67e1e4ed3720
[2023-07-14T23:43:37.405-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230715T024325, end_date=20230715T024337
[2023-07-14T23:43:37.442-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:43:37.460-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-15T12:11:09.884-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-15T12:11:09.923-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-15T12:11:09.924-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-15T12:11:09.979-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-09 00:00:00+00:00
[2023-07-15T12:11:09.993-0300] {standard_task_runner.py:57} INFO - Started process 12079 to run task
[2023-07-15T12:11:10.010-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '92', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmptm3w5m7v']
[2023-07-15T12:11:10.014-0300] {standard_task_runner.py:85} INFO - Job 92: Subtask transform_twitter_datascience
[2023-07-15T12:11:10.141-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-09T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-15T12:11:10.283-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-15T12:11:10.292-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-15T12:11:10.294-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-09
[2023-07-15T12:11:13.530-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:13 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-15T12:11:13.531-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-15T12:11:13.618-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-15T12:11:13.620-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-15T12:11:13.622-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-15T12:11:13.622-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-15T12:11:13.623-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-15T12:11:14.658-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-15T12:11:15.909-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-15T12:11:15.934-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:15 INFO SparkContext: Running Spark version 3.1.3
[2023-07-15T12:11:16.048-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO ResourceUtils: ==============================================================
[2023-07-15T12:11:16.048-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-15T12:11:16.049-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO ResourceUtils: ==============================================================
[2023-07-15T12:11:16.050-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-15T12:11:16.114-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-15T12:11:16.150-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO ResourceProfile: Limiting resource is cpu
[2023-07-15T12:11:16.155-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-15T12:11:16.283-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO SecurityManager: Changing view acls to: lucas
[2023-07-15T12:11:16.286-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-15T12:11:16.287-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO SecurityManager: Changing view acls groups to:
[2023-07-15T12:11:16.288-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO SecurityManager: Changing modify acls groups to:
[2023-07-15T12:11:16.288-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-15T12:11:16.968-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:16 INFO Utils: Successfully started service 'sparkDriver' on port 42543.
[2023-07-15T12:11:17.047-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:17 INFO SparkEnv: Registering MapOutputTracker
[2023-07-15T12:11:17.129-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:17 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-15T12:11:17.179-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-15T12:11:17.182-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-15T12:11:17.204-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-15T12:11:17.296-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1a264898-374e-4c6e-9863-d9458303d24e
[2023-07-15T12:11:17.399-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:17 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-15T12:11:17.471-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-15T12:11:18.091-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-15T12:11:18.244-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[2023-07-15T12:11:18.850-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-15T12:11:18.914-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46429.
[2023-07-15T12:11:18.915-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO NettyBlockTransferService: Server created on 192.168.0.102:46429
[2023-07-15T12:11:18.921-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-15T12:11:18.944-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 46429, None)
[2023-07-15T12:11:18.961-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:46429 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 46429, None)
[2023-07-15T12:11:18.965-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 46429, None)
[2023-07-15T12:11:18.967-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 46429, None)
[2023-07-15T12:11:20.440-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-15T12:11:20.447-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:20 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-15T12:11:22.962-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:22 INFO InMemoryFileIndex: It took 196 ms to list leaf files for 1 paths.
[2023-07-15T12:11:23.293-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:23 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
[2023-07-15T12:11:28.876-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:28 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:11:28.879-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:11:28.887-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-15T12:11:29.889-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-15T12:11:30.033-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-15T12:11:30.040-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:46429 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:11:30.052-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:11:30.086-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198737 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:11:30.579-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:11:30.621-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:11:30.622-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:11:30.623-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:11:30.625-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:11:30.640-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:11:30.909-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-15T12:11:30.916-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-15T12:11:30.918-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:46429 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-15T12:11:30.921-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:11:30.957-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:11:30.961-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-15T12:11:31.101-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-15T12:11:31.144-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-15T12:11:31.451-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4433, partition values: [empty row]
[2023-07-15T12:11:32.186-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:32 INFO CodeGenerator: Code generated in 474.316796 ms
[2023-07-15T12:11:32.357-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2023-07-15T12:11:32.408-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1339 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:11:32.424-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-15T12:11:32.445-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:32 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,765 s
[2023-07-15T12:11:32.466-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:11:32.470-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-15T12:11:32.480-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:32 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,896592 s
[2023-07-15T12:11:33.868-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-15T12:11:33.874-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:33 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-15T12:11:33.875-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:33 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-15T12:11:34.031-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:11:34.031-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:11:34.033-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:11:34.180-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO CodeGenerator: Code generated in 53.934421 ms
[2023-07-15T12:11:34.274-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO CodeGenerator: Code generated in 58.719885 ms
[2023-07-15T12:11:34.290-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-15T12:11:34.315-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.0 MiB)
[2023-07-15T12:11:34.317-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:46429 (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-15T12:11:34.320-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:11:34.325-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198737 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:11:34.457-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:11:34.459-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:11:34.459-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:11:34.460-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:11:34.460-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:11:34.462-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:11:34.568-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 433.8 MiB)
[2023-07-15T12:11:34.576-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 433.7 MiB)
[2023-07-15T12:11:34.577-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:46429 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-15T12:11:34.577-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:11:34.578-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:11:34.579-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-15T12:11:34.594-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:11:34.596-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-15T12:11:34.767-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:11:34.768-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:11:34.769-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:11:34.809-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:46429 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2023-07-15T12:11:34.908-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO CodeGenerator: Code generated in 41.776862 ms
[2023-07-15T12:11:34.914-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4433, partition values: [empty row]
[2023-07-15T12:11:34.977-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:34 INFO CodeGenerator: Code generated in 54.025789 ms
[2023-07-15T12:11:35.041-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO CodeGenerator: Code generated in 11.999221 ms
[2023-07-15T12:11:35.108-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileOutputCommitter: Saved output of task 'attempt_202307151211345602969280417534102_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-09/_temporary/0/task_202307151211345602969280417534102_0001_m_000000
[2023-07-15T12:11:35.110-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SparkHadoopMapRedUtil: attempt_202307151211345602969280417534102_0001_m_000000_1: Committed
[2023-07-15T12:11:35.117-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-15T12:11:35.125-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 542 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:11:35.125-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-15T12:11:35.127-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,660 s
[2023-07-15T12:11:35.128-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:11:35.128-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-15T12:11:35.129-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,671264 s
[2023-07-15T12:11:35.178-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileFormatWriter: Write Job a9928015-d877-48e5-b7d0-eb411549666b committed.
[2023-07-15T12:11:35.195-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileFormatWriter: Finished processing stats for write job a9928015-d877-48e5-b7d0-eb411549666b.
[2023-07-15T12:11:35.332-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:11:35.332-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:11:35.333-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-15T12:11:35.361-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:11:35.361-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:11:35.366-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:11:35.438-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO CodeGenerator: Code generated in 32.401877 ms
[2023-07-15T12:11:35.446-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.6 MiB)
[2023-07-15T12:11:35.466-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 433.6 MiB)
[2023-07-15T12:11:35.468-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:46429 (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-15T12:11:35.472-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:11:35.475-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198737 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:11:35.519-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:11:35.523-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:11:35.524-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:11:35.524-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:11:35.524-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:11:35.525-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:11:35.564-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.4 MiB)
[2023-07-15T12:11:35.571-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.3 MiB)
[2023-07-15T12:11:35.577-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:46429 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-15T12:11:35.580-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:11:35.581-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:11:35.583-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-15T12:11:35.585-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:11:35.588-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-15T12:11:35.640-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:11:35.641-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:11:35.642-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:11:35.698-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO CodeGenerator: Code generated in 19.518707 ms
[2023-07-15T12:11:35.706-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4433, partition values: [empty row]
[2023-07-15T12:11:35.753-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO CodeGenerator: Code generated in 38.084928 ms
[2023-07-15T12:11:35.777-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileOutputCommitter: Saved output of task 'attempt_202307151211351549202390513645688_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-09/_temporary/0/task_202307151211351549202390513645688_0002_m_000000
[2023-07-15T12:11:35.778-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SparkHadoopMapRedUtil: attempt_202307151211351549202390513645688_0002_m_000000_2: Committed
[2023-07-15T12:11:35.780-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-15T12:11:35.784-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 199 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:11:35.784-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-15T12:11:35.788-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,258 s
[2023-07-15T12:11:35.789-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:11:35.790-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-15T12:11:35.790-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,270897 s
[2023-07-15T12:11:35.812-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileFormatWriter: Write Job a38b742c-9527-4016-813e-efe3e3f0a048 committed.
[2023-07-15T12:11:35.814-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO FileFormatWriter: Finished processing stats for write job a38b742c-9527-4016-813e-efe3e3f0a048.
[2023-07-15T12:11:35.944-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-15T12:11:35.970-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-15T12:11:35.999-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-15T12:11:36.025-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO MemoryStore: MemoryStore cleared
[2023-07-15T12:11:36.026-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO BlockManager: BlockManager stopped
[2023-07-15T12:11:36.029-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-15T12:11:36.035-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-15T12:11:36.044-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO SparkContext: Successfully stopped SparkContext
[2023-07-15T12:11:36.045-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO ShutdownHookManager: Shutdown hook called
[2023-07-15T12:11:36.046-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-cccd2ff9-0cfd-47e6-b369-0f7f2f925d55
[2023-07-15T12:11:36.051-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b78fa96-93fe-4de1-a1bc-98ca8c449f7d
[2023-07-15T12:11:36.057-0300] {spark_submit.py:492} INFO - 23/07/15 12:11:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b78fa96-93fe-4de1-a1bc-98ca8c449f7d/pyspark-97e2e44f-ddef-4afa-9e00-f6688f94963f
[2023-07-15T12:11:36.141-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230709T000000, start_date=20230715T151109, end_date=20230715T151136
[2023-07-15T12:11:36.179-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-15T12:11:36.225-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
