[2023-07-13T22:25:41.439-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:25:41.443-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:25:41.443-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:25:41.453-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-13T22:25:41.456-0300] {standard_task_runner.py:57} INFO - Started process 36852 to run task
[2023-07-13T22:25:41.460-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp8onusbj6']
[2023-07-13T22:25:41.461-0300] {standard_task_runner.py:85} INFO - Job 14: Subtask transform_twitter_datascience
[2023-07-13T22:25:41.498-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:25:41.544-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-13T22:25:41.548-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:25:41.549-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-13
[2023-07-13T22:25:43.073-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:43 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:25:43.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:25:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:25:43.092-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:25:43.092-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:25:43.092-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:25:43.092-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:25:43.092-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:25:43.142-0300] {spark_submit.py:492} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2023-07-13T22:25:43.142-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:631)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:271)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:234)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:119)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1022)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1022)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
[2023-07-13T22:25:43.143-0300] {spark_submit.py:492} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-07-13T22:25:43.170-0300] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 423, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-13. Error code is: 1.
[2023-07-13T22:25:43.176-0300] {taskinstance.py:1345} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230714T012541, end_date=20230714T012543
[2023-07-13T22:25:43.187-0300] {standard_task_runner.py:104} ERROR - Failed to execute job 14 for task transform_twitter_datascience (Cannot execute: spark-submit --master yarn --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-13. Error code is: 1.; 36852)
[2023-07-13T22:25:43.200-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-13T22:25:43.208-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:31:52.722-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:31:52.726-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:31:52.727-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:31:52.735-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-13T22:31:52.738-0300] {standard_task_runner.py:57} INFO - Started process 39851 to run task
[2023-07-13T22:31:52.741-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp0vahc135']
[2023-07-13T22:31:52.741-0300] {standard_task_runner.py:85} INFO - Job 14: Subtask transform_twitter_datascience
[2023-07-13T22:31:52.765-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:31:52.831-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-13T22:31:52.838-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:31:52.840-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-13
[2023-07-13T22:31:54.097-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:54 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:31:54.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:31:54.109-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-13T22:31:54.109-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-13T22:31:54.109-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-13T22:31:54.109-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-13T22:31:54.109-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-13T22:31:54.475-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:31:55.032-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-13T22:31:55.041-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SparkContext: Running Spark version 3.1.3
[2023-07-13T22:31:55.077-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:55.077-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:31:55.077-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO ResourceUtils: ==============================================================
[2023-07-13T22:31:55.078-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:31:55.098-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:31:55.111-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:31:55.112-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:31:55.157-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:31:55.158-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:31:55.158-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:31:55.158-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:31:55.158-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:31:55.400-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO Utils: Successfully started service 'sparkDriver' on port 45415.
[2023-07-13T22:31:55.424-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:31:55.450-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:31:55.465-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:31:55.466-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:31:55.470-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:31:55.481-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ef5bc1c8-3c05-482b-82fc-17458e5884fe
[2023-07-13T22:31:55.500-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:31:55.517-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:31:55.706-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:31:55.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4040
[2023-07-13T22:31:55.930-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:31:55.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43525.
[2023-07-13T22:31:55.952-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO NettyBlockTransferService: Server created on 192.168.0.177:43525
[2023-07-13T22:31:55.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:31:55.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 43525, None)
[2023-07-13T22:31:55.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:43525 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 43525, None)
[2023-07-13T22:31:55.963-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 43525, None)
[2023-07-13T22:31:55.964-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 43525, None)
[2023-07-13T22:31:56.350-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-13T22:31:56.351-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:56 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:31:57.107-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:57 INFO InMemoryFileIndex: It took 55 ms to list leaf files for 1 paths.
[2023-07-13T22:31:57.237-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:57 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 8 paths.
[2023-07-13T22:31:58.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:58 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:31:58.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:58 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:31:58.751-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:58 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:31:58.968-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-13T22:31:59.011-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)
[2023-07-13T22:31:59.013-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:43525 (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:31:59.026-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:59.033-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613286 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:31:59.175-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:31:59.186-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:31:59.187-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:31:59.187-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:31:59.188-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:31:59.194-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:31:59.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-13T22:31:59.274-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-13T22:31:59.275-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:43525 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-13T22:31:59.276-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:31:59.285-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:31:59.285-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:31:59.328-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6291 bytes) taskResourceAssignments Map()
[2023-07-13T22:31:59.341-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:31:59.443-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [empty row]
[2023-07-13T22:31:59.697-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO CodeGenerator: Code generated in 140.862359 ms
[2023-07-13T22:31:59.729-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:31:59.733-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [empty row]
[2023-07-13T22:31:59.737-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:31:59.742-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [empty row]
[2023-07-13T22:31:59.744-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [empty row]
[2023-07-13T22:31:59.746-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [empty row]
[2023-07-13T22:31:59.748-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [empty row]
[2023-07-13T22:31:59.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2023-07-13T22:31:59.770-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 452 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:31:59.772-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:31:59.777-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,572 s
[2023-07-13T22:31:59.780-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:31:59.780-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:31:59.782-0300] {spark_submit.py:492} INFO - 23/07/13 22:31:59 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,605898 s
[2023-07-13T22:32:00.173-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:32:00.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:32:00.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-13T22:32:00.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:32:00.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:32:00.238-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:32:00.239-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:32:00.321-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO CodeGenerator: Code generated in 29.682672 ms
[2023-07-13T22:32:00.380-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO CodeGenerator: Code generated in 39.530077 ms
[2023-07-13T22:32:00.385-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-13T22:32:00.397-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)
[2023-07-13T22:32:00.398-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:43525 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:32:00.399-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:32:00.402-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613286 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:32:00.459-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:32:00.460-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:32:00.461-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:32:00.461-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:32:00.461-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:32:00.462-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:32:00.509-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 195.4 KiB, free 433.8 MiB)
[2023-07-13T22:32:00.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 68.5 KiB, free 433.7 MiB)
[2023-07-13T22:32:00.512-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:43525 (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:32:00.513-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:32:00.514-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:32:00.514-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:32:00.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:43525 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2023-07-13T22:32:00.517-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:32:00.518-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:32:00.522-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:43525 in memory (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:32:00.585-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:32:00.585-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:32:00.586-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:32:00.662-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO CodeGenerator: Code generated in 19.515478 ms
[2023-07-13T22:32:00.664-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:32:00.687-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO CodeGenerator: Code generated in 19.679701 ms
[2023-07-13T22:32:00.713-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO CodeGenerator: Code generated in 5.464526 ms
[2023-07-13T22:32:00.736-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:32:00.746-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:32:00.753-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:32:00.762-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:32:00.768-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:32:00.774-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:32:00.778-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:32:00.792-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileOutputCommitter: Saved output of task 'attempt_202307132232001136993469349226767_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-13/_temporary/0/task_202307132232001136993469349226767_0001_m_000000
[2023-07-13T22:32:00.793-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SparkHadoopMapRedUtil: attempt_202307132232001136993469349226767_0001_m_000000_1: Committed
[2023-07-13T22:32:00.798-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3119 bytes result sent to driver
[2023-07-13T22:32:00.799-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 284 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:32:00.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:32:00.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,337 s
[2023-07-13T22:32:00.801-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:32:00.801-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:32:00.801-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,341914 s
[2023-07-13T22:32:00.817-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileFormatWriter: Write Job f7a1a57f-bb40-4be8-9f89-c167dffc197a committed.
[2023-07-13T22:32:00.820-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileFormatWriter: Finished processing stats for write job f7a1a57f-bb40-4be8-9f89-c167dffc197a.
[2023-07-13T22:32:00.856-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:32:00.857-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:32:00.857-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:32:00.857-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:32:00.873-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:32:00.873-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:32:00.874-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:32:00.897-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO CodeGenerator: Code generated in 11.085518 ms
[2023-07-13T22:32:00.916-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO CodeGenerator: Code generated in 15.182826 ms
[2023-07-13T22:32:00.919-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-13T22:32:00.928-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.7 MiB)
[2023-07-13T22:32:00.928-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:43525 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-13T22:32:00.929-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:32:00.930-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33613286 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:32:00.952-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:32:00.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:32:00.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:32:00.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:32:00.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:32:00.954-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:32:00.972-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 184.3 KiB, free 433.6 MiB)
[2023-07-13T22:32:00.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 433.5 MiB)
[2023-07-13T22:32:00.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:43525 (size: 65.7 KiB, free: 434.2 MiB)
[2023-07-13T22:32:00.976-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-13T22:32:00.977-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:32:00.977-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:32:00.978-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes) taskResourceAssignments Map()
[2023-07-13T22:32:00.978-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:32:01.002-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:32:01.002-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:32:01.003-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:32:01.034-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO CodeGenerator: Code generated in 13.838499 ms
[2023-07-13T22:32:01.036-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13511, partition values: [19551]
[2023-07-13T22:32:01.051-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO CodeGenerator: Code generated in 10.659961 ms
[2023-07-13T22:32:01.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:32:01.062-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-9062, partition values: [19546]
[2023-07-13T22:32:01.065-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:32:01.067-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-4573, partition values: [19548]
[2023-07-13T22:32:01.070-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-4565, partition values: [19549]
[2023-07-13T22:32:01.073-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4543, partition values: [19547]
[2023-07-13T22:32:01.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4480, partition values: [19550]
[2023-07-13T22:32:01.081-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileOutputCommitter: Saved output of task 'attempt_202307132232003614744460011849322_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-13/_temporary/0/task_202307132232003614744460011849322_0002_m_000000
[2023-07-13T22:32:01.081-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO SparkHadoopMapRedUtil: attempt_202307132232003614744460011849322_0002_m_000000_2: Committed
[2023-07-13T22:32:01.082-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2971 bytes result sent to driver
[2023-07-13T22:32:01.083-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 106 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:32:01.084-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:32:01.084-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,129 s
[2023-07-13T22:32:01.084-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:32:01.085-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:32:01.085-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,132214 s
[2023-07-13T22:32:01.096-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileFormatWriter: Write Job 8279efef-e5b2-4789-9bbc-522fd1388978 committed.
[2023-07-13T22:32:01.097-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO FileFormatWriter: Finished processing stats for write job 8279efef-e5b2-4789-9bbc-522fd1388978.
[2023-07-13T22:32:01.130-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:32:01.139-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.177:43525 in memory (size: 65.7 KiB, free: 434.3 MiB)
[2023-07-13T22:32:01.142-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.177:43525 in memory (size: 68.5 KiB, free: 434.3 MiB)
[2023-07-13T22:32:01.145-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:32:01.145-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.177:43525 in memory (size: 27.6 KiB, free: 434.4 MiB)
[2023-07-13T22:32:01.156-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:32:01.167-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:32:01.167-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO BlockManager: BlockManager stopped
[2023-07-13T22:32:01.169-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:32:01.171-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:32:01.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:32:01.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:32:01.176-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc030c86-c414-49ae-a191-47e4a9b3e9dd
[2023-07-13T22:32:01.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2ff1780-82d8-47ae-8d38-a6251c6c9d11/pyspark-76f1aafc-a443-4af4-87f5-40b93b765cd3
[2023-07-13T22:32:01.180-0300] {spark_submit.py:492} INFO - 23/07/13 22:32:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2ff1780-82d8-47ae-8d38-a6251c6c9d11
[2023-07-13T22:32:01.222-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230714T013152, end_date=20230714T013201
[2023-07-13T22:32:01.240-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:32:01.246-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:43:58.973-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:43:58.977-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:43:58.977-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:43:58.985-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-13T22:43:58.988-0300] {standard_task_runner.py:57} INFO - Started process 44995 to run task
[2023-07-13T22:43:58.989-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpsib7i6o3']
[2023-07-13T22:43:58.990-0300] {standard_task_runner.py:85} INFO - Job 33: Subtask transform_twitter_datascience
[2023-07-13T22:43:59.013-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:43:59.060-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-13T22:43:59.065-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:43:59.067-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation --process-date 2023-07-13
[2023-07-13T22:44:00.465-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:00 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:44:00.470-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:44:01.158-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:44:01.207-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:44:01.298-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO ResourceUtils: ==============================================================
[2023-07-13T22:44:01.298-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:44:01.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO ResourceUtils: ==============================================================
[2023-07-13T22:44:01.299-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:44:01.322-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:44:01.333-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:44:01.334-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:44:01.393-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:44:01.394-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:44:01.395-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:44:01.395-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:44:01.396-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:44:01.675-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO Utils: Successfully started service 'sparkDriver' on port 42023.
[2023-07-13T22:44:01.705-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:44:01.739-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:44:01.754-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:44:01.754-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:44:01.758-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:44:01.776-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ece6cd90-8bc1-469b-85c9-a3e414dedbd4
[2023-07-13T22:44:01.791-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:44:01.806-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:44:01.979-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:44:02.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:44:02.084-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:44:02.104-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44931.
[2023-07-13T22:44:02.105-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO NettyBlockTransferService: Server created on 192.168.0.177:44931
[2023-07-13T22:44:02.107-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:44:02.115-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 44931, None)
[2023-07-13T22:44:02.118-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:44931 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 44931, None)
[2023-07-13T22:44:02.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 44931, None)
[2023-07-13T22:44:02.121-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 44931, None)
[2023-07-13T22:44:02.510-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:44:02.516-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:02 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:44:03.541-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:03 INFO InMemoryFileIndex: It took 82 ms to list leaf files for 1 paths.
[2023-07-13T22:44:03.673-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:03 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:44:05.513-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:44:05.515-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:44:05.517-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:44:05.747-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:44:05.792-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:44:05.794-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:44931 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:44:05.798-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:44:05.810-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635929 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:44:05.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:44:05.972-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:44:05.972-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:44:05.973-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:44:05.974-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:44:05.977-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:44:06.066-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:44:06.068-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:44:06.068-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:44931 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:44:06.069-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:44:06.078-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:44:06.078-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:44:06.120-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:44:06.133-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:44:06.237-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [empty row]
[2023-07-13T22:44:06.383-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO CodeGenerator: Code generated in 116.950884 ms
[2023-07-13T22:44:06.421-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [empty row]
[2023-07-13T22:44:06.427-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [empty row]
[2023-07-13T22:44:06.431-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:44:06.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:44:06.437-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [empty row]
[2023-07-13T22:44:06.440-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [empty row]
[2023-07-13T22:44:06.442-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [empty row]
[2023-07-13T22:44:06.456-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:44:06.463-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 353 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:44:06.464-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:44:06.468-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,471 s
[2023-07-13T22:44:06.471-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:44:06.471-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:44:06.473-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,514021 s
[2023-07-13T22:44:06.817-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:44:06.820-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:44:06.821-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:44:06.822-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:44:06.887-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:44:06.887-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:44:06.888-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:44:07.059-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO CodeGenerator: Code generated in 96.042043 ms
[2023-07-13T22:44:07.064-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-13T22:44:07.074-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-13T22:44:07.075-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:44931 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:44:07.076-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:44:07.079-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635929 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:44:07.146-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:44:07.148-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:44:07.148-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:44:07.148-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:44:07.148-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:44:07.149-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:44:07.168-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:44931 in memory (size: 7.0 KiB, free: 434.3 MiB)
[2023-07-13T22:44:07.172-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:44931 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:44:07.175-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.5 KiB, free 433.9 MiB)
[2023-07-13T22:44:07.177-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:44:07.177-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:44931 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:44:07.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:44:07.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:44:07.178-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:44:07.182-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:44:07.182-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:44:07.226-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:44:07.226-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:44:07.227-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:44:07.261-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:44:07.305-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO CodeGenerator: Code generated in 38.340341 ms
[2023-07-13T22:44:07.370-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO CodeGenerator: Code generated in 10.502683 ms
[2023-07-13T22:44:07.440-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:44:07.453-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:44:07.461-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:44:07.475-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:44:07.492-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:44:07.504-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:44:07.517-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:44:07.558-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileOutputCommitter: Saved output of task 'attempt_202307132244078668161484659242746_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-13/_temporary/0/task_202307132244078668161484659242746_0001_m_000000
[2023-07-13T22:44:07.565-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkHadoopMapRedUtil: attempt_202307132244078668161484659242746_0001_m_000000_1: Committed. Elapsed time: 6 ms.
[2023-07-13T22:44:07.575-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-13T22:44:07.580-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 401 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:44:07.580-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:44:07.582-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,431 s
[2023-07-13T22:44:07.583-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:44:07.583-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:44:07.584-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,436659 s
[2023-07-13T22:44:07.586-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileFormatWriter: Start to commit write Job 53d708cf-c023-4d14-8e4b-fca55612377d.
[2023-07-13T22:44:07.607-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileFormatWriter: Write Job 53d708cf-c023-4d14-8e4b-fca55612377d committed. Elapsed time: 20 ms.
[2023-07-13T22:44:07.613-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileFormatWriter: Finished processing stats for write job 53d708cf-c023-4d14-8e4b-fca55612377d.
[2023-07-13T22:44:07.668-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:44:07.669-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:44:07.669-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:44:07.670-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:44:07.685-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:44:07.685-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:44:07.686-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:44:07.740-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO CodeGenerator: Code generated in 18.844581 ms
[2023-07-13T22:44:07.746-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:44:07.760-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:44:07.761-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:44931 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:44:07.762-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:44:07.763-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33635929 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:44:07.789-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:44:07.790-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:44:07.790-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:44:07.790-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:44:07.790-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:44:07.791-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:44:07.809-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.5 KiB, free 433.4 MiB)
[2023-07-13T22:44:07.813-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:44:07.813-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:44931 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:44:07.814-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:44:07.814-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:44:07.814-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:44:07.816-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:44:07.816-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:44:07.830-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:44:07.830-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:44:07.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:44:07.843-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-27308, partition values: [19551]
[2023-07-13T22:44:07.855-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO CodeGenerator: Code generated in 9.446162 ms
[2023-07-13T22:44:07.861-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-13520, partition values: [19549]
[2023-07-13T22:44:07.864-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-9102, partition values: [19548]
[2023-07-13T22:44:07.866-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:44:07.869-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:44:07.872-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-4538, partition values: [19550]
[2023-07-13T22:44:07.875-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-4483, partition values: [19547]
[2023-07-13T22:44:07.877-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-4426, partition values: [19546]
[2023-07-13T22:44:07.882-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileOutputCommitter: Saved output of task 'attempt_202307132244077283823618758219889_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-13/_temporary/0/task_202307132244077283823618758219889_0002_m_000000
[2023-07-13T22:44:07.882-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkHadoopMapRedUtil: attempt_202307132244077283823618758219889_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-07-13T22:44:07.883-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:44:07.884-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 69 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:44:07.885-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:44:07.885-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,093 s
[2023-07-13T22:44:07.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:44:07.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:44:07.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,096632 s
[2023-07-13T22:44:07.886-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileFormatWriter: Start to commit write Job d88296b0-28a8-459c-8d98-e3f4f380f8de.
[2023-07-13T22:44:07.895-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileFormatWriter: Write Job d88296b0-28a8-459c-8d98-e3f4f380f8de committed. Elapsed time: 7 ms.
[2023-07-13T22:44:07.895-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO FileFormatWriter: Finished processing stats for write job d88296b0-28a8-459c-8d98-e3f4f380f8de.
[2023-07-13T22:44:07.920-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:44:07.929-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:44:07.940-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:44:07.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:44:07.948-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO BlockManager: BlockManager stopped
[2023-07-13T22:44:07.951-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:44:07.953-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:44:07.956-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:44:07.956-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:44:07.957-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b983313-deec-4f6a-909e-43ec355c3533/pyspark-84bbca7f-2cd0-4ef8-84ff-6b0aaad062c5
[2023-07-13T22:44:07.959-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b983313-deec-4f6a-909e-43ec355c3533
[2023-07-13T22:44:07.961-0300] {spark_submit.py:492} INFO - 23/07/13 22:44:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-5541ceee-5bea-412d-ad40-2757149ffbc5
[2023-07-13T22:44:08.009-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230714T014358, end_date=20230714T014408
[2023-07-13T22:44:08.026-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:44:08.032-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:53:41.503-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:53:41.510-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:53:41.510-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:53:41.523-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-13T22:53:41.526-0300] {standard_task_runner.py:57} INFO - Started process 51686 to run task
[2023-07-13T22:53:41.529-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpg_bm07ca']
[2023-07-13T22:53:41.530-0300] {standard_task_runner.py:85} INFO - Job 33: Subtask transform_twitter_datascience
[2023-07-13T22:53:41.551-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:53:41.588-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-13T22:53:41.591-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-13T22:53:41.592-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience --dest dados_transformation --process-date 2023-07-13
[2023-07-13T22:53:42.772-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:42 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-13T22:53:42.774-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-13T22:53:43.412-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SparkContext: Running Spark version 3.3.1
[2023-07-13T22:53:43.459-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-13T22:53:43.532-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO ResourceUtils: ==============================================================
[2023-07-13T22:53:43.532-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-13T22:53:43.533-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO ResourceUtils: ==============================================================
[2023-07-13T22:53:43.533-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-13T22:53:43.551-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-13T22:53:43.560-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO ResourceProfile: Limiting resource is cpu
[2023-07-13T22:53:43.561-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-13T22:53:43.596-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SecurityManager: Changing view acls to: lucas
[2023-07-13T22:53:43.597-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-13T22:53:43.597-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SecurityManager: Changing view acls groups to:
[2023-07-13T22:53:43.597-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SecurityManager: Changing modify acls groups to:
[2023-07-13T22:53:43.598-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-13T22:53:43.798-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO Utils: Successfully started service 'sparkDriver' on port 40893.
[2023-07-13T22:53:43.819-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SparkEnv: Registering MapOutputTracker
[2023-07-13T22:53:43.842-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-13T22:53:43.855-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-13T22:53:43.855-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-13T22:53:43.858-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-13T22:53:43.873-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-125b8167-867f-40d0-b88b-e0dd2d634053
[2023-07-13T22:53:43.887-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-13T22:53:43.907-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-13T22:53:44.102-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-13T22:53:44.185-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-13T22:53:44.191-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-13T22:53:44.207-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44735.
[2023-07-13T22:53:44.207-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO NettyBlockTransferService: Server created on 192.168.0.177:44735
[2023-07-13T22:53:44.209-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-13T22:53:44.214-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 44735, None)
[2023-07-13T22:53:44.217-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:44735 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 44735, None)
[2023-07-13T22:53:44.220-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 44735, None)
[2023-07-13T22:53:44.220-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 44735, None)
[2023-07-13T22:53:44.554-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-13T22:53:44.559-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:44 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-13T22:53:45.214-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:45 INFO InMemoryFileIndex: It took 52 ms to list leaf files for 1 paths.
[2023-07-13T22:53:45.314-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:45 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 8 paths.
[2023-07-13T22:53:46.703-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:46 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:46.704-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:46 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:46.706-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:46 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-13T22:53:46.925-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-13T22:53:46.973-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:46.975-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:44735 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:46.977-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:46 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:46.984-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672327 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:47.123-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:47.135-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:47.135-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:47.135-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:47.136-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:47.139-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:47.218-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-13T22:53:47.220-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:47.220-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:44735 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:47.221-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:47.229-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:47.230-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-13T22:53:47.271-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6449 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:47.283-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-13T22:53:47.403-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [empty row]
[2023-07-13T22:53:47.545-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO CodeGenerator: Code generated in 111.72445 ms
[2023-07-13T22:53:47.593-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [empty row]
[2023-07-13T22:53:47.603-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-18025, partition values: [empty row]
[2023-07-13T22:53:47.608-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [empty row]
[2023-07-13T22:53:47.614-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [empty row]
[2023-07-13T22:53:47.619-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [empty row]
[2023-07-13T22:53:47.623-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [empty row]
[2023-07-13T22:53:47.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4503, partition values: [empty row]
[2023-07-13T22:53:47.646-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-13T22:53:47.654-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 390 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:47.655-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-13T22:53:47.660-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,511 s
[2023-07-13T22:53:47.663-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:47.663-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-13T22:53:47.665-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,540987 s
[2023-07-13T22:53:47.825-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:44735 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:47.839-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:44735 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-13T22:53:48.048-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:48.050-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-13T22:53:48.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-13T22:53:48.052-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-13T22:53:48.109-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:48.110-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:48.110-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:48.286-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO CodeGenerator: Code generated in 108.422554 ms
[2023-07-13T22:53:48.291-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-13T22:53:48.304-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-13T22:53:48.305-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:44735 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-13T22:53:48.306-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:48.310-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672327 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:48.387-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:48.388-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:48.388-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:48.389-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:48.389-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:48.390-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:48.423-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.3 KiB, free 433.9 MiB)
[2023-07-13T22:53:48.427-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.4 KiB, free 433.9 MiB)
[2023-07-13T22:53:48.428-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:44735 (size: 83.4 KiB, free: 434.3 MiB)
[2023-07-13T22:53:48.428-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:48.429-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:48.429-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-13T22:53:48.433-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:48.434-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-13T22:53:48.482-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:48.482-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:48.483-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:48.507-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:48.546-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO CodeGenerator: Code generated in 35.416598 ms
[2023-07-13T22:53:48.568-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO CodeGenerator: Code generated in 5.286786 ms
[2023-07-13T22:53:48.604-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [19549]
[2023-07-13T22:53:48.617-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-18025, partition values: [19550]
[2023-07-13T22:53:48.627-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:48.635-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:48.644-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:48.651-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:48.658-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4503, partition values: [19551]
[2023-07-13T22:53:48.669-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253481900649023716303596_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/tweet/process_date=2023-07-13/_temporary/0/task_202307132253481900649023716303596_0001_m_000000
[2023-07-13T22:53:48.670-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkHadoopMapRedUtil: attempt_202307132253481900649023716303596_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-13T22:53:48.676-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-07-13T22:53:48.678-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 247 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:48.678-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-13T22:53:48.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,287 s
[2023-07-13T22:53:48.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:48.679-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-13T22:53:48.680-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,292971 s
[2023-07-13T22:53:48.682-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileFormatWriter: Start to commit write Job f04e225b-4fc3-4802-a5d0-8b6ff62a1dc9.
[2023-07-13T22:53:48.693-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileFormatWriter: Write Job f04e225b-4fc3-4802-a5d0-8b6ff62a1dc9 committed. Elapsed time: 9 ms.
[2023-07-13T22:53:48.695-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileFormatWriter: Finished processing stats for write job f04e225b-4fc3-4802-a5d0-8b6ff62a1dc9.
[2023-07-13T22:53:48.724-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DataSourceStrategy: Pruning directories with:
[2023-07-13T22:53:48.724-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileSourceStrategy: Pushed Filters:
[2023-07-13T22:53:48.725-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-13T22:53:48.725-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-13T22:53:48.734-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:48.734-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:48.734-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:48.762-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO CodeGenerator: Code generated in 12.431203 ms
[2023-07-13T22:53:48.764-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-13T22:53:48.773-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-13T22:53:48.774-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:44735 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-13T22:53:48.775-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:48.776-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33672327 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-13T22:53:48.799-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-13T22:53:48.799-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-13T22:53:48.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-13T22:53:48.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Parents of final stage: List()
[2023-07-13T22:53:48.800-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Missing parents: List()
[2023-07-13T22:53:48.801-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-13T22:53:48.825-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.3 KiB, free 433.4 MiB)
[2023-07-13T22:53:48.827-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.3 MiB)
[2023-07-13T22:53:48.828-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:44735 (size: 77.2 KiB, free: 434.2 MiB)
[2023-07-13T22:53:48.828-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-13T22:53:48.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-13T22:53:48.829-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-13T22:53:48.830-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 6835 bytes) taskResourceAssignments Map()
[2023-07-13T22:53:48.831-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-13T22:53:48.852-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-13T22:53:48.852-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-13T22:53:48.852-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-13T22:53:48.864-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-10/datascience_20230710.json, range: 0-27327, partition values: [19548]
[2023-07-13T22:53:48.882-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO CodeGenerator: Code generated in 14.164553 ms
[2023-07-13T22:53:48.891-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-11/datascience_20230711.json, range: 0-22821, partition values: [19549]
[2023-07-13T22:53:48.896-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-12/datascience_20230712.json, range: 0-18025, partition values: [19550]
[2023-07-13T22:53:48.901-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-09/datascience_20230709.json, range: 0-13573, partition values: [19547]
[2023-07-13T22:53:48.905-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-08/datascience_20230708.json, range: 0-13526, partition values: [19546]
[2023-07-13T22:53:48.909-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-07/datascience_20230707.json, range: 0-9074, partition values: [19545]
[2023-07-13T22:53:48.913-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-06/datascience_20230706.json, range: 0-9046, partition values: [19544]
[2023-07-13T22:53:48.915-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/datalake/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4503, partition values: [19551]
[2023-07-13T22:53:48.921-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileOutputCommitter: Saved output of task 'attempt_202307132253488145339584636681837_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/dados_transformation/user/process_date=2023-07-13/_temporary/0/task_202307132253488145339584636681837_0002_m_000000
[2023-07-13T22:53:48.921-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkHadoopMapRedUtil: attempt_202307132253488145339584636681837_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-13T22:53:48.923-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-13T22:53:48.924-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 94 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-13T22:53:48.924-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-13T22:53:48.925-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,123 s
[2023-07-13T22:53:48.926-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-13T22:53:48.926-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-13T22:53:48.926-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,127464 s
[2023-07-13T22:53:48.927-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileFormatWriter: Start to commit write Job db9d6a08-a1c8-4651-9b2f-9ec14c88de34.
[2023-07-13T22:53:48.935-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileFormatWriter: Write Job db9d6a08-a1c8-4651-9b2f-9ec14c88de34 committed. Elapsed time: 7 ms.
[2023-07-13T22:53:48.935-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO FileFormatWriter: Finished processing stats for write job db9d6a08-a1c8-4651-9b2f-9ec14c88de34.
[2023-07-13T22:53:48.962-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-13T22:53:48.973-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4040
[2023-07-13T22:53:48.982-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-13T22:53:48.989-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO MemoryStore: MemoryStore cleared
[2023-07-13T22:53:48.990-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO BlockManager: BlockManager stopped
[2023-07-13T22:53:48.992-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-13T22:53:48.994-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-13T22:53:48.998-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO SparkContext: Successfully stopped SparkContext
[2023-07-13T22:53:48.999-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO ShutdownHookManager: Shutdown hook called
[2023-07-13T22:53:48.999-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-d32074a4-1ec0-4c3d-b922-266241a3e330
[2023-07-13T22:53:49.002-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-d32074a4-1ec0-4c3d-b922-266241a3e330/pyspark-4b6e429b-3d87-4350-b3e5-01a753fa565f
[2023-07-13T22:53:49.004-0300] {spark_submit.py:492} INFO - 23/07/13 22:53:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-cec0c0ec-e2cf-47d5-b557-7b13c2426f8c
[2023-07-13T22:53:49.047-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230714T015341, end_date=20230714T015349
[2023-07-13T22:53:49.059-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:53:49.065-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T15:27:48.594-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T15:27:48.608-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T15:27:48.608-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T15:27:48.631-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T15:27:48.640-0300] {standard_task_runner.py:57} INFO - Started process 16298 to run task
[2023-07-14T15:27:48.643-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '51', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1vxtmgbg']
[2023-07-14T15:27:48.644-0300] {standard_task_runner.py:85} INFO - Job 51: Subtask transform_twitter_datascience
[2023-07-14T15:27:48.694-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T15:27:48.813-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T15:27:48.828-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T15:27:48.833-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13 --dest /home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-13 --process-date 2023-07-13
[2023-07-14T15:27:52.062-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:52 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T15:27:52.066-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T15:27:54.028-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T15:27:54.284-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T15:27:54.449-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO ResourceUtils: ==============================================================
[2023-07-14T15:27:54.450-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T15:27:54.453-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO ResourceUtils: ==============================================================
[2023-07-14T15:27:54.455-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T15:27:54.493-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T15:27:54.514-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T15:27:54.515-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T15:27:54.592-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T15:27:54.592-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T15:27:54.593-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T15:27:54.593-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T15:27:54.594-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T15:27:55.078-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO Utils: Successfully started service 'sparkDriver' on port 35059.
[2023-07-14T15:27:55.137-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T15:27:55.207-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T15:27:55.241-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T15:27:55.243-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T15:27:55.251-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T15:27:55.296-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-240ae250-7b19-4cea-b1c0-90f8beb0dc7a
[2023-07-14T15:27:55.341-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T15:27:55.377-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T15:27:55.862-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T15:27:55.999-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:55 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T15:27:56.010-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T15:27:56.047-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40601.
[2023-07-14T15:27:56.047-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO NettyBlockTransferService: Server created on 192.168.0.102:40601
[2023-07-14T15:27:56.050-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T15:27:56.063-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 40601, None)
[2023-07-14T15:27:56.073-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:40601 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 40601, None)
[2023-07-14T15:27:56.076-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 40601, None)
[2023-07-14T15:27:56.078-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 40601, None)
[2023-07-14T15:27:56.831-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T15:27:56.843-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:56 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T15:27:58.632-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:58 INFO InMemoryFileIndex: It took 160 ms to list leaf files for 1 paths.
[2023-07-14T15:27:58.886-0300] {spark_submit.py:492} INFO - 23/07/14 15:27:58 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
[2023-07-14T15:28:02.539-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:02 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:28:02.541-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:28:02.544-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T15:28:02.960-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T15:28:03.050-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T15:28:03.056-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:40601 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:28:03.062-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:28:03.079-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207893 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:28:03.358-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:28:03.397-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:28:03.398-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:28:03.399-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:28:03.405-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:28:03.412-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:28:03.644-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T15:28:03.648-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T15:28:03.650-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:40601 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:28:03.653-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:28:03.672-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:28:03.674-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T15:28:03.756-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5007 bytes) taskResourceAssignments Map()
[2023-07-14T15:28:03.780-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T15:28:03.976-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:03 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13589, partition values: [empty row]
[2023-07-14T15:28:04.254-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:04 INFO CodeGenerator: Code generated in 211.278283 ms
[2023-07-14T15:28:04.344-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T15:28:04.361-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 626 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:28:04.366-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T15:28:04.380-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,934 s
[2023-07-14T15:28:04.392-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:28:04.394-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T15:28:04.398-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,038623 s
[2023-07-14T15:28:05.055-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T15:28:05.058-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T15:28:05.058-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T15:28:05.188-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:28:05.189-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:28:05.190-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:28:05.492-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO CodeGenerator: Code generated in 153.201401 ms
[2023-07-14T15:28:05.499-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-14T15:28:05.511-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:40601 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T15:28:05.521-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.0 MiB)
[2023-07-14T15:28:05.524-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:40601 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T15:28:05.524-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:40601 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T15:28:05.526-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:28:05.530-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207893 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:28:05.632-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:28:05.637-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:28:05.638-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:28:05.639-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:28:05.639-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:28:05.641-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:28:05.681-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T15:28:05.689-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.0 KiB, free 433.9 MiB)
[2023-07-14T15:28:05.690-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:40601 (size: 82.0 KiB, free: 434.3 MiB)
[2023-07-14T15:28:05.690-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:28:05.691-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:28:05.691-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T15:28:05.696-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:28:05.697-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T15:28:05.775-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:28:05.775-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:28:05.776-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:28:05.828-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13589, partition values: [empty row]
[2023-07-14T15:28:05.884-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO CodeGenerator: Code generated in 42.612013 ms
[2023-07-14T15:28:05.926-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO CodeGenerator: Code generated in 5.59429 ms
[2023-07-14T15:28:05.993-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO FileOutputCommitter: Saved output of task 'attempt_202307141528056645886849648531140_0001_m_000000_1' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-13/tweet/process_date=2023-07-13/_temporary/0/task_202307141528056645886849648531140_0001_m_000000
[2023-07-14T15:28:05.994-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:05 INFO SparkHadoopMapRedUtil: attempt_202307141528056645886849648531140_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T15:28:06.006-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T15:28:06.009-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 316 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:28:06.009-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T15:28:06.011-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,368 s
[2023-07-14T15:28:06.012-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:28:06.013-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T15:28:06.015-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,382224 s
[2023-07-14T15:28:06.018-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileFormatWriter: Start to commit write Job ba6daa5b-6844-4700-975b-186ccd268476.
[2023-07-14T15:28:06.043-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileFormatWriter: Write Job ba6daa5b-6844-4700-975b-186ccd268476 committed. Elapsed time: 22 ms.
[2023-07-14T15:28:06.046-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileFormatWriter: Finished processing stats for write job ba6daa5b-6844-4700-975b-186ccd268476.
[2023-07-14T15:28:06.107-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T15:28:06.107-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T15:28:06.108-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T15:28:06.119-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:28:06.120-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:28:06.121-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:28:06.164-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO CodeGenerator: Code generated in 18.331498 ms
[2023-07-14T15:28:06.170-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T15:28:06.189-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T15:28:06.192-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:40601 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T15:28:06.195-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:28:06.199-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207893 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T15:28:06.258-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T15:28:06.259-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T15:28:06.259-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T15:28:06.259-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T15:28:06.259-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Missing parents: List()
[2023-07-14T15:28:06.260-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T15:28:06.290-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T15:28:06.294-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.8 KiB, free 433.4 MiB)
[2023-07-14T15:28:06.295-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:40601 (size: 76.8 KiB, free: 434.2 MiB)
[2023-07-14T15:28:06.295-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T15:28:06.297-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T15:28:06.298-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T15:28:06.303-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5236 bytes) taskResourceAssignments Map()
[2023-07-14T15:28:06.305-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T15:28:06.345-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T15:28:06.345-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T15:28:06.348-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T15:28:06.378-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileScanRDD: Reading File path: file:///home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-13589, partition values: [empty row]
[2023-07-14T15:28:06.412-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO CodeGenerator: Code generated in 27.287842 ms
[2023-07-14T15:28:06.432-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileOutputCommitter: Saved output of task 'attempt_20230714152806665348981758993487_0002_m_000000_2' to file:/home/lucas/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-13/user/process_date=2023-07-13/_temporary/0/task_20230714152806665348981758993487_0002_m_000000
[2023-07-14T15:28:06.434-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SparkHadoopMapRedUtil: attempt_20230714152806665348981758993487_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-07-14T15:28:06.439-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T15:28:06.445-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 145 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T15:28:06.445-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T15:28:06.446-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,185 s
[2023-07-14T15:28:06.447-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T15:28:06.447-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T15:28:06.448-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,189807 s
[2023-07-14T15:28:06.449-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileFormatWriter: Start to commit write Job 0d9a2e60-b149-4ab1-9a8b-f46e9aedc9af.
[2023-07-14T15:28:06.466-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileFormatWriter: Write Job 0d9a2e60-b149-4ab1-9a8b-f46e9aedc9af committed. Elapsed time: 16 ms.
[2023-07-14T15:28:06.467-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO FileFormatWriter: Finished processing stats for write job 0d9a2e60-b149-4ab1-9a8b-f46e9aedc9af.
[2023-07-14T15:28:06.545-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T15:28:06.574-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T15:28:06.608-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T15:28:06.629-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO MemoryStore: MemoryStore cleared
[2023-07-14T15:28:06.630-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO BlockManager: BlockManager stopped
[2023-07-14T15:28:06.638-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T15:28:06.644-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T15:28:06.651-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T15:28:06.651-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T15:28:06.655-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-38f25d82-03b8-4b20-8b45-228fbffea7d5/pyspark-5077fec8-9113-4c32-9ae0-4e7d0b1b82a4
[2023-07-14T15:28:06.659-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-38f25d82-03b8-4b20-8b45-228fbffea7d5
[2023-07-14T15:28:06.661-0300] {spark_submit.py:492} INFO - 23/07/14 15:28:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a0fe459-0967-40a4-83d7-9d0533041e11
[2023-07-14T15:28:06.761-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230714T182748, end_date=20230714T182806
[2023-07-14T15:28:06.819-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T15:28:06.845-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:12:22.334-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T16:12:22.338-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T16:12:22.338-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:12:22.346-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T16:12:22.348-0300] {standard_task_runner.py:57} INFO - Started process 24964 to run task
[2023-07-14T16:12:22.350-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '53', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_ud7rk15']
[2023-07-14T16:12:22.350-0300] {standard_task_runner.py:85} INFO - Job 53: Subtask transform_twitter_datascience
[2023-07-14T16:12:22.398-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:12:22.489-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T16:12:22.495-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T16:12:22.498-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-13 --process-date 2023-07-13
[2023-07-14T16:12:24.257-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:24 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T16:12:24.259-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T16:12:25.510-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T16:12:25.624-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T16:12:25.814-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 INFO ResourceUtils: ==============================================================
[2023-07-14T16:12:25.816-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T16:12:25.818-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 INFO ResourceUtils: ==============================================================
[2023-07-14T16:12:25.819-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T16:12:25.893-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T16:12:25.927-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T16:12:25.928-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T16:12:26.028-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T16:12:26.029-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T16:12:26.031-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T16:12:26.034-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T16:12:26.035-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T16:12:26.496-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO Utils: Successfully started service 'sparkDriver' on port 38827.
[2023-07-14T16:12:26.543-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T16:12:26.607-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T16:12:26.635-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T16:12:26.637-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T16:12:26.646-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T16:12:26.684-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a2ce04d3-ea5d-4304-b0bb-be04cf781257
[2023-07-14T16:12:26.714-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T16:12:26.751-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:26 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T16:12:27.213-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-14T16:12:27.419-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T16:12:27.436-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T16:12:27.484-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44163.
[2023-07-14T16:12:27.484-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO NettyBlockTransferService: Server created on 192.168.0.102:44163
[2023-07-14T16:12:27.487-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T16:12:27.501-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 44163, None)
[2023-07-14T16:12:27.511-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:44163 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 44163, None)
[2023-07-14T16:12:27.515-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 44163, None)
[2023-07-14T16:12:27.518-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 44163, None)
[2023-07-14T16:12:28.551-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T16:12:28.571-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:28 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T16:12:29.914-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:29 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2023-07-14T16:12:29.963-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-07-14T16:12:32.227-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:12:32.228-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:12:32.235-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T16:12:32.453-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T16:12:32.489-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T16:12:32.492-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:44163 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:32.496-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:32.502-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198886 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:12:32.619-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:32.636-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:12:32.636-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:12:32.637-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:12:32.638-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:12:32.643-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:12:32.737-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T16:12:32.740-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T16:12:32.741-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:44163 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:32.741-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:12:32.753-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:12:32.755-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T16:12:32.808-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T16:12:32.832-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T16:12:33.079-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4582, partition values: [empty row]
[2023-07-14T16:12:33.427-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO CodeGenerator: Code generated in 269.098063 ms
[2023-07-14T16:12:33.512-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-07-14T16:12:33.526-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 732 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:33.528-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T16:12:33.536-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,881 s
[2023-07-14T16:12:33.544-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:33.545-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T16:12:33.548-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:33 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,927641 s
[2023-07-14T16:12:34.230-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:44163 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:34.236-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:44163 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T16:12:34.374-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T16:12:34.376-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T16:12:34.377-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T16:12:34.448-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:34.448-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:34.449-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:34.628-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO CodeGenerator: Code generated in 88.811855 ms
[2023-07-14T16:12:34.634-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T16:12:34.647-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T16:12:34.647-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:44163 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:12:34.649-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:34.652-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198886 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:12:34.702-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:34.704-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:12:34.704-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:12:34.704-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:12:34.704-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:12:34.705-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:12:34.727-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T16:12:34.731-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T16:12:34.732-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:44163 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T16:12:34.733-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:12:34.734-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:12:34.734-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T16:12:34.739-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:12:34.740-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T16:12:34.796-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:34.797-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:34.798-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:34.834-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4582, partition values: [empty row]
[2023-07-14T16:12:34.856-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO CodeGenerator: Code generated in 17.755284 ms
[2023-07-14T16:12:34.878-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO CodeGenerator: Code generated in 6.744118 ms
[2023-07-14T16:12:34.908-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileOutputCommitter: Saved output of task 'attempt_20230714161234972037193363186925_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-13/tweet/process_date=2023-07-13/_temporary/0/task_20230714161234972037193363186925_0001_m_000000
[2023-07-14T16:12:34.909-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO SparkHadoopMapRedUtil: attempt_20230714161234972037193363186925_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T16:12:34.917-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T16:12:34.922-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 186 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:34.923-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T16:12:34.924-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,218 s
[2023-07-14T16:12:34.925-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:34.926-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T16:12:34.927-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,224033 s
[2023-07-14T16:12:34.930-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileFormatWriter: Start to commit write Job cdf79c07-5a11-4724-a024-f61a46e44d5a.
[2023-07-14T16:12:34.957-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileFormatWriter: Write Job cdf79c07-5a11-4724-a024-f61a46e44d5a committed. Elapsed time: 26 ms.
[2023-07-14T16:12:34.969-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:34 INFO FileFormatWriter: Finished processing stats for write job cdf79c07-5a11-4724-a024-f61a46e44d5a.
[2023-07-14T16:12:35.071-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T16:12:35.071-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T16:12:35.071-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T16:12:35.100-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:44163 in memory (size: 82.1 KiB, free: 434.4 MiB)
[2023-07-14T16:12:35.107-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:35.108-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:35.108-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:44163 in memory (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:12:35.109-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:35.190-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO CodeGenerator: Code generated in 34.696054 ms
[2023-07-14T16:12:35.203-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T16:12:35.223-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T16:12:35.227-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:44163 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T16:12:35.228-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:35.231-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198886 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T16:12:35.284-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T16:12:35.286-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T16:12:35.286-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T16:12:35.286-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T16:12:35.287-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: Missing parents: List()
[2023-07-14T16:12:35.293-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T16:12:35.329-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 434.0 MiB)
[2023-07-14T16:12:35.334-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.9 MiB)
[2023-07-14T16:12:35.335-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:44163 (size: 76.9 KiB, free: 434.3 MiB)
[2023-07-14T16:12:35.336-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T16:12:35.338-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T16:12:35.339-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T16:12:35.346-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T16:12:35.350-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T16:12:35.387-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T16:12:35.390-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T16:12:35.392-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T16:12:35.432-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4582, partition values: [empty row]
[2023-07-14T16:12:35.464-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO CodeGenerator: Code generated in 25.827784 ms
[2023-07-14T16:12:35.486-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileOutputCommitter: Saved output of task 'attempt_20230714161235333334389256893699_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/extract_date=2023-07-13/user/process_date=2023-07-13/_temporary/0/task_20230714161235333334389256893699_0002_m_000000
[2023-07-14T16:12:35.486-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SparkHadoopMapRedUtil: attempt_20230714161235333334389256893699_0002_m_000000_2: Committed. Elapsed time: 3 ms.
[2023-07-14T16:12:35.493-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T16:12:35.496-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 151 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T16:12:35.497-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T16:12:35.501-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,204 s
[2023-07-14T16:12:35.502-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T16:12:35.502-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T16:12:35.503-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,217831 s
[2023-07-14T16:12:35.504-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileFormatWriter: Start to commit write Job fd96718d-ec1c-4b80-a458-575c8a727412.
[2023-07-14T16:12:35.539-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileFormatWriter: Write Job fd96718d-ec1c-4b80-a458-575c8a727412 committed. Elapsed time: 33 ms.
[2023-07-14T16:12:35.542-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO FileFormatWriter: Finished processing stats for write job fd96718d-ec1c-4b80-a458-575c8a727412.
[2023-07-14T16:12:35.694-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T16:12:35.732-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-14T16:12:35.791-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T16:12:35.827-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO MemoryStore: MemoryStore cleared
[2023-07-14T16:12:35.828-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO BlockManager: BlockManager stopped
[2023-07-14T16:12:35.843-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T16:12:35.847-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T16:12:35.860-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T16:12:35.862-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T16:12:35.864-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-93eac4de-c546-4434-aa9a-2455c9fc2666
[2023-07-14T16:12:35.875-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-93eac4de-c546-4434-aa9a-2455c9fc2666/pyspark-28f754fd-1150-4986-8343-b8a736d18265
[2023-07-14T16:12:35.884-0300] {spark_submit.py:492} INFO - 23/07/14 16:12:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-3aa42751-f5a6-4efb-950c-5deb8cdfeaf0
[2023-07-14T16:12:36.041-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230714T191222, end_date=20230714T191236
[2023-07-14T16:12:36.099-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:12:36.119-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:25:56.566-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T17:25:56.581-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T17:25:56.582-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:25:56.609-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T17:25:56.617-0300] {standard_task_runner.py:57} INFO - Started process 33764 to run task
[2023-07-14T17:25:56.622-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '56', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpvzsu36om']
[2023-07-14T17:25:56.627-0300] {standard_task_runner.py:85} INFO - Job 56: Subtask transform_twitter_datascience
[2023-07-14T17:25:56.732-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:25:56.901-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T17:25:56.917-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T17:25:56.922-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-13
[2023-07-14T17:26:01.225-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:01 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T17:26:01.238-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T17:26:03.182-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T17:26:03.419-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T17:26:03.621-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO ResourceUtils: ==============================================================
[2023-07-14T17:26:03.622-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T17:26:03.622-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO ResourceUtils: ==============================================================
[2023-07-14T17:26:03.624-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T17:26:03.654-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T17:26:03.670-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T17:26:03.671-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T17:26:03.730-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T17:26:03.730-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T17:26:03.730-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T17:26:03.731-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T17:26:03.731-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T17:26:04.364-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:04 INFO Utils: Successfully started service 'sparkDriver' on port 46177.
[2023-07-14T17:26:04.498-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:04 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T17:26:04.653-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:04 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T17:26:04.719-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T17:26:04.722-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T17:26:04.744-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T17:26:04.834-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2a0ee41b-b21f-469d-98fb-4e5ddc88efe2
[2023-07-14T17:26:04.921-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T17:26:05.016-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T17:26:06.057-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T17:26:06.099-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T17:26:06.546-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T17:26:06.581-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T17:26:06.665-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44949.
[2023-07-14T17:26:06.666-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO NettyBlockTransferService: Server created on 192.168.0.102:44949
[2023-07-14T17:26:06.671-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T17:26:06.712-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 44949, None)
[2023-07-14T17:26:06.733-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:44949 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 44949, None)
[2023-07-14T17:26:06.748-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 44949, None)
[2023-07-14T17:26:06.760-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 44949, None)
[2023-07-14T17:26:07.828-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T17:26:07.834-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:07 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T17:26:09.082-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:09 INFO InMemoryFileIndex: It took 70 ms to list leaf files for 1 paths.
[2023-07-14T17:26:09.168-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-07-14T17:26:12.131-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:26:12.132-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:26:12.135-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T17:26:12.537-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T17:26:12.605-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T17:26:12.612-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:44949 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:26:12.616-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:26:12.629-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212399 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:26:12.816-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:26:12.840-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:26:12.843-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:26:12.845-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:26:12.846-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:26:12.850-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:26:13.017-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T17:26:13.020-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T17:26:13.021-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:44949 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:26:13.023-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:26:13.044-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:26:13.045-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T17:26:13.117-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T17:26:13.138-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T17:26:13.328-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-18095, partition values: [empty row]
[2023-07-14T17:26:13.553-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO CodeGenerator: Code generated in 181.174268 ms
[2023-07-14T17:26:13.644-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-07-14T17:26:13.651-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 549 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:26:13.654-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T17:26:13.666-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,797 s
[2023-07-14T17:26:13.671-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:26:13.671-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T17:26:13.679-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:13 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,862196 s
[2023-07-14T17:26:14.229-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T17:26:14.231-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T17:26:14.232-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T17:26:14.331-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:26:14.331-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:26:14.332-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:26:14.596-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO CodeGenerator: Code generated in 167.877575 ms
[2023-07-14T17:26:14.603-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
[2023-07-14T17:26:14.622-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
[2023-07-14T17:26:14.626-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:44949 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:26:14.629-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:26:14.632-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212399 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:26:14.728-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:26:14.730-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:26:14.730-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:26:14.730-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:26:14.730-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:26:14.731-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:26:14.756-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:44949 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:26:14.766-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:44949 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:26:14.770-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T17:26:14.777-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T17:26:14.778-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:44949 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T17:26:14.778-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:26:14.779-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:26:14.779-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T17:26:14.782-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:26:14.783-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T17:26:14.850-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:26:14.851-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:26:14.852-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:26:14.897-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-18095, partition values: [empty row]
[2023-07-14T17:26:14.928-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO CodeGenerator: Code generated in 28.400577 ms
[2023-07-14T17:26:14.960-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:14 INFO CodeGenerator: Code generated in 9.680648 ms
[2023-07-14T17:26:15.004-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileOutputCommitter: Saved output of task 'attempt_202307141726148888098813827486703_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-13/_temporary/0/task_202307141726148888098813827486703_0001_m_000000
[2023-07-14T17:26:15.005-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SparkHadoopMapRedUtil: attempt_202307141726148888098813827486703_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T17:26:15.016-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T17:26:15.019-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 238 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:26:15.020-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T17:26:15.022-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,287 s
[2023-07-14T17:26:15.023-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:26:15.023-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T17:26:15.024-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,296002 s
[2023-07-14T17:26:15.031-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileFormatWriter: Start to commit write Job 2e08e9b0-2329-4428-9829-0a9cec1486f5.
[2023-07-14T17:26:15.051-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileFormatWriter: Write Job 2e08e9b0-2329-4428-9829-0a9cec1486f5 committed. Elapsed time: 18 ms.
[2023-07-14T17:26:15.056-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileFormatWriter: Finished processing stats for write job 2e08e9b0-2329-4428-9829-0a9cec1486f5.
[2023-07-14T17:26:15.112-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:26:15.113-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:26:15.113-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T17:26:15.126-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:26:15.127-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:26:15.128-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:26:15.169-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO CodeGenerator: Code generated in 20.203754 ms
[2023-07-14T17:26:15.178-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T17:26:15.193-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T17:26:15.196-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:44949 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:26:15.199-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:26:15.200-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212399 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:26:15.230-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:26:15.231-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:26:15.232-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:26:15.232-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:26:15.232-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:26:15.233-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:26:15.254-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T17:26:15.262-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T17:26:15.263-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:44949 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T17:26:15.264-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:26:15.265-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:26:15.265-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T17:26:15.266-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:26:15.267-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T17:26:15.297-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:26:15.297-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:26:15.298-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:26:15.322-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-18095, partition values: [empty row]
[2023-07-14T17:26:15.350-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO CodeGenerator: Code generated in 19.254335 ms
[2023-07-14T17:26:15.367-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileOutputCommitter: Saved output of task 'attempt_202307141726155260257415834788289_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-13/_temporary/0/task_202307141726155260257415834788289_0002_m_000000
[2023-07-14T17:26:15.367-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SparkHadoopMapRedUtil: attempt_202307141726155260257415834788289_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-07-14T17:26:15.370-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T17:26:15.377-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 109 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:26:15.377-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T17:26:15.378-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,145 s
[2023-07-14T17:26:15.378-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:26:15.378-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T17:26:15.379-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,148233 s
[2023-07-14T17:26:15.379-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileFormatWriter: Start to commit write Job 8090d96b-bf1f-4ade-b419-e454cf64ad23.
[2023-07-14T17:26:15.391-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileFormatWriter: Write Job 8090d96b-bf1f-4ade-b419-e454cf64ad23 committed. Elapsed time: 10 ms.
[2023-07-14T17:26:15.394-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO FileFormatWriter: Finished processing stats for write job 8090d96b-bf1f-4ade-b419-e454cf64ad23.
[2023-07-14T17:26:15.446-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T17:26:15.456-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T17:26:15.480-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T17:26:15.491-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO MemoryStore: MemoryStore cleared
[2023-07-14T17:26:15.494-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO BlockManager: BlockManager stopped
[2023-07-14T17:26:15.497-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T17:26:15.499-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T17:26:15.504-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T17:26:15.504-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T17:26:15.505-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-524f9c52-1f7d-4712-8630-e58a4da9aa38/pyspark-14425a0a-e345-4d50-9841-6d2bcb576aab
[2023-07-14T17:26:15.511-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-09ace478-0d6e-442f-a424-b0a8f8be6972
[2023-07-14T17:26:15.513-0300] {spark_submit.py:492} INFO - 23/07/14 17:26:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-524f9c52-1f7d-4712-8630-e58a4da9aa38
[2023-07-14T17:26:15.564-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230714T202556, end_date=20230714T202615
[2023-07-14T17:26:15.594-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:26:15.601-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:31:19.218-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T17:31:19.233-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T17:31:19.233-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:31:19.252-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T17:31:19.256-0300] {standard_task_runner.py:57} INFO - Started process 37139 to run task
[2023-07-14T17:31:19.260-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '64', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpxry_51_6']
[2023-07-14T17:31:19.263-0300] {standard_task_runner.py:85} INFO - Job 64: Subtask transform_twitter_datascience
[2023-07-14T17:31:19.314-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:31:19.373-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T17:31:19.381-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T17:31:19.382-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-13
[2023-07-14T17:31:21.637-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:21 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-14T17:31:21.639-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T17:31:23.212-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO SparkContext: Running Spark version 3.3.1
[2023-07-14T17:31:23.410-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T17:31:23.697-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO ResourceUtils: ==============================================================
[2023-07-14T17:31:23.698-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T17:31:23.699-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO ResourceUtils: ==============================================================
[2023-07-14T17:31:23.703-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T17:31:23.769-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T17:31:23.797-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T17:31:23.800-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T17:31:23.910-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T17:31:23.912-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T17:31:23.914-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T17:31:23.915-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T17:31:23.915-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T17:31:24.251-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO Utils: Successfully started service 'sparkDriver' on port 40257.
[2023-07-14T17:31:24.316-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T17:31:24.376-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T17:31:24.403-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T17:31:24.404-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T17:31:24.407-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T17:31:24.435-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d4dd88ed-39d6-40e3-be22-666d503758f4
[2023-07-14T17:31:24.455-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T17:31:24.477-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T17:31:24.736-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T17:31:24.745-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T17:31:24.900-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-14T17:31:24.918-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-14T17:31:24.958-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44783.
[2023-07-14T17:31:24.958-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO NettyBlockTransferService: Server created on 192.168.0.102:44783
[2023-07-14T17:31:24.961-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T17:31:24.972-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 44783, None)
[2023-07-14T17:31:24.976-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:44783 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 44783, None)
[2023-07-14T17:31:24.985-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 44783, None)
[2023-07-14T17:31:24.986-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 44783, None)
[2023-07-14T17:31:25.640-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-14T17:31:25.653-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:25 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T17:31:27.177-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:27 INFO InMemoryFileIndex: It took 58 ms to list leaf files for 1 paths.
[2023-07-14T17:31:27.284-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:27 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
[2023-07-14T17:31:30.074-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:31:30.077-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:31:30.086-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T17:31:30.497-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2023-07-14T17:31:30.581-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-07-14T17:31:30.584-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:44783 (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:31:30.590-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:30.610-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198843 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:31:30.885-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:30.902-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:31:30.903-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:31:30.903-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:31:30.904-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:31:30.907-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:31:31.076-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.2 MiB)
[2023-07-14T17:31:31.080-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)
[2023-07-14T17:31:31.084-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:44783 (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:31:31.086-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:31:31.104-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:31:31.105-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T17:31:31.181-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5035 bytes) taskResourceAssignments Map()
[2023-07-14T17:31:31.203-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T17:31:31.431-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4539, partition values: [empty row]
[2023-07-14T17:31:31.744-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO CodeGenerator: Code generated in 236.814621 ms
[2023-07-14T17:31:31.807-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-07-14T17:31:31.817-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 649 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:31:31.821-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T17:31:31.832-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,902 s
[2023-07-14T17:31:31.836-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:31:31.836-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T17:31:31.838-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:31 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,952777 s
[2023-07-14T17:31:32.390-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T17:31:32.393-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-07-14T17:31:32.394-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T17:31:32.520-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:31:32.520-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:31:32.521-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:31:32.694-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:44783 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-07-14T17:31:32.706-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:44783 in memory (size: 34.0 KiB, free: 434.4 MiB)
[2023-07-14T17:31:32.853-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO CodeGenerator: Code generated in 227.264014 ms
[2023-07-14T17:31:32.862-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
[2023-07-14T17:31:32.880-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
[2023-07-14T17:31:32.885-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:44783 (size: 33.9 KiB, free: 434.4 MiB)
[2023-07-14T17:31:32.888-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:32.894-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198843 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:31:32.994-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:32.996-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:31:32.996-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:31:32.997-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:31:32.997-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:31:32.997-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:32 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:31:33.020-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.8 KiB, free 433.9 MiB)
[2023-07-14T17:31:33.029-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 433.9 MiB)
[2023-07-14T17:31:33.030-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:44783 (size: 82.1 KiB, free: 434.3 MiB)
[2023-07-14T17:31:33.031-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:31:33.032-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:31:33.032-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T17:31:33.042-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:31:33.044-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T17:31:33.139-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:31:33.139-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:31:33.140-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:31:33.198-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4539, partition values: [empty row]
[2023-07-14T17:31:33.247-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO CodeGenerator: Code generated in 39.348699 ms
[2023-07-14T17:31:33.279-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO CodeGenerator: Code generated in 8.739457 ms
[2023-07-14T17:31:33.332-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileOutputCommitter: Saved output of task 'attempt_202307141731327320351517693583449_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-13/_temporary/0/task_202307141731327320351517693583449_0001_m_000000
[2023-07-14T17:31:33.332-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkHadoopMapRedUtil: attempt_202307141731327320351517693583449_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-07-14T17:31:33.341-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-07-14T17:31:33.346-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 309 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:31:33.349-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,349 s
[2023-07-14T17:31:33.351-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:31:33.352-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T17:31:33.352-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T17:31:33.353-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,358264 s
[2023-07-14T17:31:33.355-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileFormatWriter: Start to commit write Job 9ecee250-3146-4365-9cda-f182d0f3d58f.
[2023-07-14T17:31:33.373-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileFormatWriter: Write Job 9ecee250-3146-4365-9cda-f182d0f3d58f committed. Elapsed time: 16 ms.
[2023-07-14T17:31:33.383-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileFormatWriter: Finished processing stats for write job 9ecee250-3146-4365-9cda-f182d0f3d58f.
[2023-07-14T17:31:33.471-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T17:31:33.471-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T17:31:33.472-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T17:31:33.493-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:31:33.494-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:31:33.494-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:31:33.552-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO CodeGenerator: Code generated in 26.092541 ms
[2023-07-14T17:31:33.557-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.0 KiB, free 433.7 MiB)
[2023-07-14T17:31:33.573-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.6 MiB)
[2023-07-14T17:31:33.574-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:44783 (size: 33.9 KiB, free: 434.3 MiB)
[2023-07-14T17:31:33.577-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:33.579-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198843 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T17:31:33.620-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T17:31:33.620-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T17:31:33.621-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T17:31:33.621-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T17:31:33.621-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Missing parents: List()
[2023-07-14T17:31:33.623-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T17:31:33.645-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.9 KiB, free 433.4 MiB)
[2023-07-14T17:31:33.654-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)
[2023-07-14T17:31:33.655-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:44783 (size: 76.9 KiB, free: 434.2 MiB)
[2023-07-14T17:31:33.655-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-07-14T17:31:33.656-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T17:31:33.656-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T17:31:33.658-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2023-07-14T17:31:33.659-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T17:31:33.687-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T17:31:33.688-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T17:31:33.688-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T17:31:33.713-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4539, partition values: [empty row]
[2023-07-14T17:31:33.730-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO CodeGenerator: Code generated in 14.051377 ms
[2023-07-14T17:31:33.744-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileOutputCommitter: Saved output of task 'attempt_202307141731331866130945495920106_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-13/_temporary/0/task_202307141731331866130945495920106_0002_m_000000
[2023-07-14T17:31:33.745-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkHadoopMapRedUtil: attempt_202307141731331866130945495920106_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-07-14T17:31:33.748-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-07-14T17:31:33.754-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 96 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-14T17:31:33.755-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T17:31:33.756-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,132 s
[2023-07-14T17:31:33.757-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T17:31:33.757-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T17:31:33.761-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,137470 s
[2023-07-14T17:31:33.762-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileFormatWriter: Start to commit write Job 1be9be24-6034-4858-949a-47ee67e8b903.
[2023-07-14T17:31:33.781-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileFormatWriter: Write Job 1be9be24-6034-4858-949a-47ee67e8b903 committed. Elapsed time: 20 ms.
[2023-07-14T17:31:33.781-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO FileFormatWriter: Finished processing stats for write job 1be9be24-6034-4858-949a-47ee67e8b903.
[2023-07-14T17:31:33.876-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T17:31:33.885-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
[2023-07-14T17:31:33.897-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T17:31:33.905-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO MemoryStore: MemoryStore cleared
[2023-07-14T17:31:33.905-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO BlockManager: BlockManager stopped
[2023-07-14T17:31:33.909-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T17:31:33.911-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T17:31:33.916-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T17:31:33.916-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T17:31:33.916-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-6054d1b8-ffb3-4ca0-a7b0-b25125073ef1
[2023-07-14T17:31:33.920-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-aed72570-32a8-4da2-aa08-4c28ca647954
[2023-07-14T17:31:33.923-0300] {spark_submit.py:492} INFO - 23/07/14 17:31:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-aed72570-32a8-4da2-aa08-4c28ca647954/pyspark-4f8f9021-20e3-4872-9988-b9d789856130
[2023-07-14T17:31:33.994-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230714T203119, end_date=20230714T203133
[2023-07-14T17:31:34.013-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:31:34.021-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:45:22.288-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T23:45:22.295-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T23:45:22.295-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:45:22.305-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T23:45:22.310-0300] {standard_task_runner.py:57} INFO - Started process 26440 to run task
[2023-07-14T23:45:22.313-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpx5f_9vit']
[2023-07-14T23:45:22.314-0300] {standard_task_runner.py:85} INFO - Job 84: Subtask transform_twitter_datascience
[2023-07-14T23:45:22.357-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:45:22.439-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T23:45:22.446-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-14T23:45:22.448-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-13
[2023-07-14T23:45:24.273-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:24 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.177 instead (on interface wlp8s0)
[2023-07-14T23:45:24.274-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-14T23:45:24.283-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-14T23:45:24.283-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-14T23:45:24.283-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-14T23:45:24.283-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-14T23:45:24.283-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-14T23:45:24.658-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-14T23:45:25.369-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-14T23:45:25.379-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SparkContext: Running Spark version 3.1.3
[2023-07-14T23:45:25.424-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO ResourceUtils: ==============================================================
[2023-07-14T23:45:25.425-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-14T23:45:25.425-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO ResourceUtils: ==============================================================
[2023-07-14T23:45:25.426-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-14T23:45:25.453-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-14T23:45:25.470-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO ResourceProfile: Limiting resource is cpu
[2023-07-14T23:45:25.470-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-14T23:45:25.516-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SecurityManager: Changing view acls to: lucas
[2023-07-14T23:45:25.517-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-14T23:45:25.517-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SecurityManager: Changing view acls groups to:
[2023-07-14T23:45:25.517-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SecurityManager: Changing modify acls groups to:
[2023-07-14T23:45:25.517-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-14T23:45:25.777-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO Utils: Successfully started service 'sparkDriver' on port 40855.
[2023-07-14T23:45:25.802-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SparkEnv: Registering MapOutputTracker
[2023-07-14T23:45:25.833-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-14T23:45:25.852-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-14T23:45:25.852-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-14T23:45:25.856-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-14T23:45:25.870-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2998a555-e19c-406c-82b2-7b0c5f3b7e63
[2023-07-14T23:45:25.893-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-14T23:45:25.913-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-14T23:45:26.104-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-07-14T23:45:26.112-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-07-14T23:45:26.180-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.177:4041
[2023-07-14T23:45:26.468-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO Executor: Starting executor ID driver on host 192.168.0.177
[2023-07-14T23:45:26.501-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41081.
[2023-07-14T23:45:26.502-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO NettyBlockTransferService: Server created on 192.168.0.177:41081
[2023-07-14T23:45:26.504-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-14T23:45:26.517-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.177, 41081, None)
[2023-07-14T23:45:26.524-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.177:41081 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.177, 41081, None)
[2023-07-14T23:45:26.528-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.177, 41081, None)
[2023-07-14T23:45:26.530-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.177, 41081, None)
[2023-07-14T23:45:27.023-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-14T23:45:27.023-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:27 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-14T23:45:27.862-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:27 INFO InMemoryFileIndex: It took 78 ms to list leaf files for 1 paths.
[2023-07-14T23:45:27.980-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:27 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
[2023-07-14T23:45:29.936-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:29 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:45:29.937-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:29 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:45:29.940-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:29 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-14T23:45:30.216-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-14T23:45:30.289-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-14T23:45:30.292-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.177:41081 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-14T23:45:30.298-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:45:30.307-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198762 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:45:30.467-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:45:30.479-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:45:30.481-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:45:30.482-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:45:30.484-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:45:30.487-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:45:30.574-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-14T23:45:30.577-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-14T23:45:30.578-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.177:41081 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:45:30.579-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:45:30.593-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:45:30.594-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-14T23:45:30.642-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-14T23:45:30.656-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-14T23:45:30.789-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:30 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4458, partition values: [empty row]
[2023-07-14T23:45:31.055-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO CodeGenerator: Code generated in 158.188679 ms
[2023-07-14T23:45:31.102-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2023-07-14T23:45:31.110-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 475 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:45:31.113-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-14T23:45:31.119-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,619 s
[2023-07-14T23:45:31.122-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:45:31.123-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-14T23:45:31.125-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,657558 s
[2023-07-14T23:45:31.599-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-14T23:45:31.600-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-14T23:45:31.601-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-14T23:45:31.668-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:45:31.669-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:45:31.670-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:45:31.762-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO CodeGenerator: Code generated in 43.550055 ms
[2023-07-14T23:45:31.833-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO CodeGenerator: Code generated in 46.721663 ms
[2023-07-14T23:45:31.841-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.0 MiB)
[2023-07-14T23:45:31.854-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)
[2023-07-14T23:45:31.855-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.177:41081 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-14T23:45:31.856-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:45:31.858-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198762 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:45:31.905-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.177:41081 in memory (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-14T23:45:31.912-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.177:41081 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-14T23:45:31.941-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:45:31.942-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:45:31.942-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:45:31.943-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:45:31.943-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:45:31.944-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:45:31.985-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 434.0 MiB)
[2023-07-14T23:45:31.988-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 434.0 MiB)
[2023-07-14T23:45:31.988-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.177:41081 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-14T23:45:31.989-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:45:31.989-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:45:31.989-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-14T23:45:31.995-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:45:31.997-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-14T23:45:32.074-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:45:32.074-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:45:32.075-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:45:32.137-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO CodeGenerator: Code generated in 23.23967 ms
[2023-07-14T23:45:32.140-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4458, partition values: [empty row]
[2023-07-14T23:45:32.166-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO CodeGenerator: Code generated in 22.653928 ms
[2023-07-14T23:45:32.188-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO CodeGenerator: Code generated in 4.700714 ms
[2023-07-14T23:45:32.224-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileOutputCommitter: Saved output of task 'attempt_202307142345318401732410834303295_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-13/_temporary/0/task_202307142345318401732410834303295_0001_m_000000
[2023-07-14T23:45:32.226-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SparkHadoopMapRedUtil: attempt_202307142345318401732410834303295_0001_m_000000_1: Committed
[2023-07-14T23:45:32.234-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-14T23:45:32.236-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 246 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:45:32.236-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-14T23:45:32.237-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,292 s
[2023-07-14T23:45:32.238-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:45:32.238-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-14T23:45:32.239-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,297369 s
[2023-07-14T23:45:32.263-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileFormatWriter: Write Job 5411ed8f-9e59-4241-84c1-447ff5593c2e committed.
[2023-07-14T23:45:32.269-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileFormatWriter: Finished processing stats for write job 5411ed8f-9e59-4241-84c1-447ff5593c2e.
[2023-07-14T23:45:32.323-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileSourceStrategy: Pushed Filters:
[2023-07-14T23:45:32.323-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-14T23:45:32.324-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-14T23:45:32.352-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:45:32.352-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:45:32.355-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:45:32.394-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO CodeGenerator: Code generated in 15.77568 ms
[2023-07-14T23:45:32.400-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-14T23:45:32.409-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.8 MiB)
[2023-07-14T23:45:32.411-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.177:41081 (size: 27.6 KiB, free: 434.3 MiB)
[2023-07-14T23:45:32.414-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:45:32.418-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198762 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-14T23:45:32.439-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-14T23:45:32.440-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-14T23:45:32.440-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-14T23:45:32.440-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Parents of final stage: List()
[2023-07-14T23:45:32.440-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Missing parents: List()
[2023-07-14T23:45:32.441-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-14T23:45:32.474-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.6 MiB)
[2023-07-14T23:45:32.479-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.5 MiB)
[2023-07-14T23:45:32.481-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.177:41081 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-14T23:45:32.483-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-14T23:45:32.484-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-14T23:45:32.485-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-14T23:45:32.487-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.177, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-14T23:45:32.487-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-14T23:45:32.508-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-14T23:45:32.508-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-14T23:45:32.509-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-14T23:45:32.558-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO CodeGenerator: Code generated in 16.124012 ms
[2023-07-14T23:45:32.560-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4458, partition values: [empty row]
[2023-07-14T23:45:32.588-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO CodeGenerator: Code generated in 22.157297 ms
[2023-07-14T23:45:32.604-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileOutputCommitter: Saved output of task 'attempt_202307142345328800642103914716869_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-13/_temporary/0/task_202307142345328800642103914716869_0002_m_000000
[2023-07-14T23:45:32.604-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SparkHadoopMapRedUtil: attempt_202307142345328800642103914716869_0002_m_000000_2: Committed
[2023-07-14T23:45:32.606-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-14T23:45:32.613-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 126 ms on 192.168.0.177 (executor driver) (1/1)
[2023-07-14T23:45:32.615-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-14T23:45:32.622-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,175 s
[2023-07-14T23:45:32.622-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-14T23:45:32.622-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-14T23:45:32.622-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,180093 s
[2023-07-14T23:45:32.642-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileFormatWriter: Write Job 07f6a321-e513-44fd-9a49-c9015f71cc8a committed.
[2023-07-14T23:45:32.642-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO FileFormatWriter: Finished processing stats for write job 07f6a321-e513-44fd-9a49-c9015f71cc8a.
[2023-07-14T23:45:32.692-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-14T23:45:32.699-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SparkUI: Stopped Spark web UI at http://192.168.0.177:4041
[2023-07-14T23:45:32.709-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-14T23:45:32.720-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO MemoryStore: MemoryStore cleared
[2023-07-14T23:45:32.720-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO BlockManager: BlockManager stopped
[2023-07-14T23:45:32.723-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-14T23:45:32.727-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-14T23:45:32.734-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO SparkContext: Successfully stopped SparkContext
[2023-07-14T23:45:32.735-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO ShutdownHookManager: Shutdown hook called
[2023-07-14T23:45:32.735-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-24e3dc81-0d95-42ff-a5fb-bcdb889477f2
[2023-07-14T23:45:32.738-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-24e3dc81-0d95-42ff-a5fb-bcdb889477f2/pyspark-65920902-f03f-4c3e-a6e1-df1edaf56150
[2023-07-14T23:45:32.740-0300] {spark_submit.py:492} INFO - 23/07/14 23:45:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-037f1255-af49-40a9-8d82-dbe65805366f
[2023-07-14T23:45:32.824-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230715T024522, end_date=20230715T024532
[2023-07-14T23:45:32.861-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:45:32.875-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-15T12:15:54.948-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-15T12:15:54.968-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-15T12:15:54.969-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-15T12:15:54.999-0300] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-15T12:15:55.008-0300] {standard_task_runner.py:57} INFO - Started process 15362 to run task
[2023-07-15T12:15:55.017-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '104', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpln1v_3so']
[2023-07-15T12:15:55.019-0300] {standard_task_runner.py:85} INFO - Job 104: Subtask transform_twitter_datascience
[2023-07-15T12:15:55.105-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-15T12:15:55.231-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-15T12:15:55.244-0300] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-15T12:15:55.247-0300] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/src/spark/transformation.py --src /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13 --dest /home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/ --process-date 2023-07-13
[2023-07-15T12:15:57.932-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:57 WARN Utils: Your hostname, lucas-Estudos resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)
[2023-07-15T12:15:57.934-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-07-15T12:15:58.032-0300] {spark_submit.py:492} INFO - WARNING: An illegal reflective access operation has occurred
[2023-07-15T12:15:58.032-0300] {spark_submit.py:492} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/%c3%81rea%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-3.1.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-07-15T12:15:58.033-0300] {spark_submit.py:492} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-07-15T12:15:58.033-0300] {spark_submit.py:492} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-07-15T12:15:58.034-0300] {spark_submit.py:492} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-07-15T12:15:58.917-0300] {spark_submit.py:492} INFO - 23/07/15 12:15:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-15T12:16:00.315-0300] {spark_submit.py:492} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-07-15T12:16:00.351-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO SparkContext: Running Spark version 3.1.3
[2023-07-15T12:16:00.516-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO ResourceUtils: ==============================================================
[2023-07-15T12:16:00.517-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-15T12:16:00.518-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO ResourceUtils: ==============================================================
[2023-07-15T12:16:00.519-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO SparkContext: Submitted application: twitter_transformation
[2023-07-15T12:16:00.574-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-15T12:16:00.608-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO ResourceProfile: Limiting resource is cpu
[2023-07-15T12:16:00.612-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-15T12:16:00.731-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO SecurityManager: Changing view acls to: lucas
[2023-07-15T12:16:00.731-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO SecurityManager: Changing modify acls to: lucas
[2023-07-15T12:16:00.731-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO SecurityManager: Changing view acls groups to:
[2023-07-15T12:16:00.731-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO SecurityManager: Changing modify acls groups to:
[2023-07-15T12:16:00.731-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2023-07-15T12:16:01.382-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO Utils: Successfully started service 'sparkDriver' on port 43423.
[2023-07-15T12:16:01.450-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO SparkEnv: Registering MapOutputTracker
[2023-07-15T12:16:01.518-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-15T12:16:01.553-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-15T12:16:01.554-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-15T12:16:01.563-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-15T12:16:01.588-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bd392408-b8b7-4b56-8cef-bc9748e39c54
[2023-07-15T12:16:01.641-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-15T12:16:01.678-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:01 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-15T12:16:02.089-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-15T12:16:02.240-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
[2023-07-15T12:16:02.739-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO Executor: Starting executor ID driver on host 192.168.0.102
[2023-07-15T12:16:02.811-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33889.
[2023-07-15T12:16:02.812-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO NettyBlockTransferService: Server created on 192.168.0.102:33889
[2023-07-15T12:16:02.816-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-15T12:16:02.833-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 33889, None)
[2023-07-15T12:16:02.845-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:33889 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.102, 33889, None)
[2023-07-15T12:16:02.849-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 33889, None)
[2023-07-15T12:16:02.853-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 33889, None)
[2023-07-15T12:16:03.781-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse').
[2023-07-15T12:16:03.781-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:03 INFO SharedState: Warehouse path is 'file:/home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/spark-warehouse'.
[2023-07-15T12:16:05.525-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:05 INFO InMemoryFileIndex: It took 110 ms to list leaf files for 1 paths.
[2023-07-15T12:16:05.720-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:05 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
[2023-07-15T12:16:10.481-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:10 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:16:10.483-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:10 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:16:10.488-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:10 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-07-15T12:16:11.062-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.7 KiB, free 434.2 MiB)
[2023-07-15T12:16:11.163-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-15T12:16:11.165-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:33889 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:16:11.172-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:16:11.193-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198854 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:16:11.549-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:16:11.586-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:16:11.587-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:16:11.589-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:16:11.593-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:16:11.604-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:16:11.888-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2023-07-15T12:16:11.901-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2023-07-15T12:16:11.905-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:33889 (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-15T12:16:11.914-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:16:11.953-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:16:11.956-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-15T12:16:12.092-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
[2023-07-15T12:16:12.118-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-15T12:16:12.366-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4550, partition values: [empty row]
[2023-07-15T12:16:12.850-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO CodeGenerator: Code generated in 288.112323 ms
[2023-07-15T12:16:12.940-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2023-07-15T12:16:12.961-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 893 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:16:12.965-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-15T12:16:12.974-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,342 s
[2023-07-15T12:16:12.983-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:16:12.984-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-07-15T12:16:12.990-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:12 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,438739 s
[2023-07-15T12:16:13.295-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:33889 in memory (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:16:13.305-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:33889 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2023-07-15T12:16:13.800-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-07-15T12:16:13.804-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:13 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2023-07-15T12:16:13.805-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:13 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-07-15T12:16:13.949-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:16:13.949-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:16:13.950-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:16:14.120-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO CodeGenerator: Code generated in 68.692958 ms
[2023-07-15T12:16:14.210-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO CodeGenerator: Code generated in 60.943258 ms
[2023-07-15T12:16:14.217-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.6 KiB, free 434.2 MiB)
[2023-07-15T12:16:14.233-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2023-07-15T12:16:14.234-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:33889 (size: 27.7 KiB, free: 434.4 MiB)
[2023-07-15T12:16:14.239-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:16:14.246-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198854 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:16:14.362-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:16:14.363-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:16:14.364-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:16:14.364-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:16:14.364-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:16:14.365-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:16:14.447-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.2 KiB, free 434.0 MiB)
[2023-07-15T12:16:14.451-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.9 KiB, free 434.0 MiB)
[2023-07-15T12:16:14.453-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:33889 (size: 66.9 KiB, free: 434.3 MiB)
[2023-07-15T12:16:14.455-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:16:14.456-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:16:14.457-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-07-15T12:16:14.467-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:16:14.467-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-07-15T12:16:14.629-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:16:14.629-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:16:14.631-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:16:14.798-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO CodeGenerator: Code generated in 58.812037 ms
[2023-07-15T12:16:14.803-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4550, partition values: [empty row]
[2023-07-15T12:16:14.866-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO CodeGenerator: Code generated in 51.597 ms
[2023-07-15T12:16:14.917-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO CodeGenerator: Code generated in 12.000727 ms
[2023-07-15T12:16:14.984-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO FileOutputCommitter: Saved output of task 'attempt_20230715121614606171065169909642_0001_m_000000_1' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/tweet/process_date=2023-07-13/_temporary/0/task_20230715121614606171065169909642_0001_m_000000
[2023-07-15T12:16:14.986-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO SparkHadoopMapRedUtil: attempt_20230715121614606171065169909642_0001_m_000000_1: Committed
[2023-07-15T12:16:14.998-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2944 bytes result sent to driver
[2023-07-15T12:16:15.000-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 540 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:16:15.001-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-07-15T12:16:15.004-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,636 s
[2023-07-15T12:16:15.004-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:16:15.004-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-07-15T12:16:15.006-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,643546 s
[2023-07-15T12:16:15.039-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileFormatWriter: Write Job ae43d51e-47dc-4749-ba05-94a4d0f0ce1b committed.
[2023-07-15T12:16:15.049-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileFormatWriter: Finished processing stats for write job ae43d51e-47dc-4749-ba05-94a4d0f0ce1b.
[2023-07-15T12:16:15.151-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileSourceStrategy: Pushed Filters:
[2023-07-15T12:16:15.152-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-07-15T12:16:15.153-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-07-15T12:16:15.183-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:16:15.183-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:16:15.184-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:16:15.263-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO CodeGenerator: Code generated in 32.382289 ms
[2023-07-15T12:16:15.273-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.6 KiB, free 433.8 MiB)
[2023-07-15T12:16:15.296-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 433.8 MiB)
[2023-07-15T12:16:15.297-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:33889 (size: 27.7 KiB, free: 434.3 MiB)
[2023-07-15T12:16:15.298-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:16:15.299-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198854 bytes, open cost is considered as scanning 4194304 bytes.
[2023-07-15T12:16:15.336-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-07-15T12:16:15.340-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-15T12:16:15.341-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-07-15T12:16:15.341-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Parents of final stage: List()
[2023-07-15T12:16:15.342-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Missing parents: List()
[2023-07-15T12:16:15.344-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-15T12:16:15.371-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.4 KiB, free 433.6 MiB)
[2023-07-15T12:16:15.378-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.6 KiB, free 433.5 MiB)
[2023-07-15T12:16:15.379-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:33889 (size: 64.6 KiB, free: 434.2 MiB)
[2023-07-15T12:16:15.380-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2023-07-15T12:16:15.380-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-15T12:16:15.381-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-15T12:16:15.382-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.102, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2023-07-15T12:16:15.383-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-07-15T12:16:15.413-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-07-15T12:16:15.414-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-07-15T12:16:15.414-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-07-15T12:16:15.467-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO CodeGenerator: Code generated in 22.415302 ms
[2023-07-15T12:16:15.473-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileScanRDD: Reading File path: file:///home/lucas/Área%20de%20Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Bronze/twitter_datascience/extract_date=2023-07-13/datascience_20230713.json, range: 0-4550, partition values: [empty row]
[2023-07-15T12:16:15.512-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO CodeGenerator: Code generated in 30.553634 ms
[2023-07-15T12:16:15.531-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileOutputCommitter: Saved output of task 'attempt_202307151216151821756585940413701_0002_m_000000_2' to file:/home/lucas/Área de Trabalho/Alura/EngenhariaDados/FormacaoAirflow/Datalake/Silver/twitter_datascience/user/process_date=2023-07-13/_temporary/0/task_202307151216151821756585940413701_0002_m_000000
[2023-07-15T12:16:15.532-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SparkHadoopMapRedUtil: attempt_202307151216151821756585940413701_0002_m_000000_2: Committed
[2023-07-15T12:16:15.534-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2023-07-15T12:16:15.539-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 156 ms on 192.168.0.102 (executor driver) (1/1)
[2023-07-15T12:16:15.539-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-15T12:16:15.540-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,194 s
[2023-07-15T12:16:15.540-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-15T12:16:15.540-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-15T12:16:15.541-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,204319 s
[2023-07-15T12:16:15.565-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileFormatWriter: Write Job a4538b40-29f2-4f51-9f41-ff4cdeaccfc4 committed.
[2023-07-15T12:16:15.565-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO FileFormatWriter: Finished processing stats for write job a4538b40-29f2-4f51-9f41-ff4cdeaccfc4.
[2023-07-15T12:16:15.639-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-15T12:16:15.652-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
[2023-07-15T12:16:15.678-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-15T12:16:15.704-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO MemoryStore: MemoryStore cleared
[2023-07-15T12:16:15.705-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO BlockManager: BlockManager stopped
[2023-07-15T12:16:15.709-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-15T12:16:15.716-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-15T12:16:15.722-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO SparkContext: Successfully stopped SparkContext
[2023-07-15T12:16:15.723-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO ShutdownHookManager: Shutdown hook called
[2023-07-15T12:16:15.724-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-3adb2c57-fdca-40a7-b5e0-cb09af79b867/pyspark-be2dec6a-8c24-4900-ab8e-87b062f6c9e9
[2023-07-15T12:16:15.731-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-3adb2c57-fdca-40a7-b5e0-cb09af79b867
[2023-07-15T12:16:15.733-0300] {spark_submit.py:492} INFO - 23/07/15 12:16:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-101481b2-26a4-4d8b-af7e-85b5fbbd4151
[2023-07-15T12:16:15.799-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230713T000000, start_date=20230715T151554, end_date=20230715T151615
[2023-07-15T12:16:15.819-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-15T12:16:15.834-0300] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
