[2023-07-13T22:25:34.995-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:25:34.999-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:25:34.999-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:25:35.008-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-13T22:25:35.012-0300] {standard_task_runner.py:57} INFO - Started process 36707 to run task
[2023-07-13T22:25:35.013-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpv2_7pcbq']
[2023-07-13T22:25:35.014-0300] {standard_task_runner.py:85} INFO - Job 12: Subtask twitter_datascience
[2023-07-13T22:25:35.037-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:25:35.102-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-13T22:25:35.106-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:25:35.107-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-13T22:25:35.702-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230714T012534, end_date=20230714T012535
[2023-07-13T22:25:35.751-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:25:35.760-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:31:38.603-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:31:38.607-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:31:38.608-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:31:38.618-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-13T22:31:38.620-0300] {standard_task_runner.py:57} INFO - Started process 39515 to run task
[2023-07-13T22:31:38.622-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp25as_7g7']
[2023-07-13T22:31:38.622-0300] {standard_task_runner.py:85} INFO - Job 12: Subtask twitter_datascience
[2023-07-13T22:31:38.644-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:31:38.712-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-13T22:31:38.721-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:31:38.722-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-13T22:31:39.925-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:31:40.020-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:31:40.120-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230714T013138, end_date=20230714T013140
[2023-07-13T22:31:40.159-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:31:40.171-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:43:44.762-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:43:44.766-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:43:44.766-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:43:44.774-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-13T22:43:44.776-0300] {standard_task_runner.py:57} INFO - Started process 44620 to run task
[2023-07-13T22:43:44.777-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '31', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpkjak22x_']
[2023-07-13T22:43:44.778-0300] {standard_task_runner.py:85} INFO - Job 31: Subtask twitter_datascience
[2023-07-13T22:43:44.798-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:43:44.852-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-13T22:43:44.857-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:43:44.859-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-13T22:43:46.033-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:43:46.126-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:43:46.221-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:43:46.317-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:43:46.416-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-13T22:43:46.517-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230714T014344, end_date=20230714T014346
[2023-07-13T22:43:46.555-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:43:46.569-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-13T22:53:30.294-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:53:30.297-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-13T22:53:30.298-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-13T22:53:30.306-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-13T22:53:30.308-0300] {standard_task_runner.py:57} INFO - Started process 51374 to run task
[2023-07-13T22:53:30.309-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '31', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpl1_y8epl']
[2023-07-13T22:53:30.310-0300] {standard_task_runner.py:85} INFO - Job 31: Subtask twitter_datascience
[2023-07-13T22:53:30.330-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-13T22:53:30.376-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-13T22:53:30.381-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-13T22:53:30.382-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-13T22:53:30.756-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230714T015330, end_date=20230714T015330
[2023-07-13T22:53:30.804-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-13T22:53:30.814-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T15:27:44.265-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T15:27:44.277-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T15:27:44.277-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T15:27:44.302-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T15:27:44.307-0300] {standard_task_runner.py:57} INFO - Started process 16232 to run task
[2023-07-14T15:27:44.311-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpboz4mfau']
[2023-07-14T15:27:44.312-0300] {standard_task_runner.py:85} INFO - Job 50: Subtask twitter_datascience
[2023-07-14T15:27:44.370-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T15:27:44.441-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T15:27:44.446-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T15:27:44.448-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-14T15:27:45.094-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-14T15:27:45.235-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-14T15:27:45.362-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230714T182744, end_date=20230714T182745
[2023-07-14T15:27:45.447-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T15:27:45.496-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T16:12:04.128-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T16:12:04.134-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T16:12:04.134-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T16:12:04.153-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T16:12:04.160-0300] {standard_task_runner.py:57} INFO - Started process 24099 to run task
[2023-07-14T16:12:04.162-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '51', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpil_zcn0b']
[2023-07-14T16:12:04.164-0300] {standard_task_runner.py:85} INFO - Job 51: Subtask twitter_datascience
[2023-07-14T16:12:04.209-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T16:12:04.296-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T16:12:04.302-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T16:12:04.303-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-14T16:12:05.009-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230714T191204, end_date=20230714T191205
[2023-07-14T16:12:05.059-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T16:12:05.084-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:25:20.742-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T17:25:20.774-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T17:25:20.774-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:25:20.813-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T17:25:20.823-0300] {standard_task_runner.py:57} INFO - Started process 33043 to run task
[2023-07-14T17:25:20.826-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '54', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpy0k6ds6f']
[2023-07-14T17:25:20.828-0300] {standard_task_runner.py:85} INFO - Job 54: Subtask twitter_datascience
[2023-07-14T17:25:20.943-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:25:21.085-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T17:25:21.102-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T17:25:21.105-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-14T17:25:21.848-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-14T17:25:22.041-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-14T17:25:22.243-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z&next_token=1234567890abcdef
[2023-07-14T17:25:22.467-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230714T202520, end_date=20230714T202522
[2023-07-14T17:25:22.527-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:25:22.545-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T17:31:15.968-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T17:31:15.976-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T17:31:15.977-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T17:31:15.992-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T17:31:16.000-0300] {standard_task_runner.py:57} INFO - Started process 37137 to run task
[2023-07-14T17:31:16.003-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '63', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp4dkkriw0']
[2023-07-14T17:31:16.003-0300] {standard_task_runner.py:85} INFO - Job 63: Subtask twitter_datascience
[2023-07-14T17:31:16.036-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T17:31:16.099-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T17:31:16.103-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T17:31:16.104-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-14T17:31:16.723-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230714T203115, end_date=20230714T203116
[2023-07-14T17:31:16.778-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T17:31:16.829-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-14T23:45:03.015-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T23:45:03.024-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-14T23:45:03.024-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-14T23:45:03.045-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-14T23:45:03.049-0300] {standard_task_runner.py:57} INFO - Started process 26069 to run task
[2023-07-14T23:45:03.053-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpwqsrsxwa']
[2023-07-14T23:45:03.054-0300] {standard_task_runner.py:85} INFO - Job 81: Subtask twitter_datascience
[2023-07-14T23:45:03.098-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-14T23:45:03.181-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-14T23:45:03.185-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-14T23:45:03.187-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-14T23:45:03.980-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230715T024503, end_date=20230715T024503
[2023-07-14T23:45:04.029-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-14T23:45:04.067-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-07-15T12:14:41.450-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-15T12:14:41.469-0300] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [queued]>
[2023-07-15T12:14:41.469-0300] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-15T12:14:41.500-0300] {taskinstance.py:1327} INFO - Executing <Task(TwitterOperator): twitter_datascience> on 2023-07-13 00:00:00+00:00
[2023-07-15T12:14:41.509-0300] {standard_task_runner.py:57} INFO - Started process 14615 to run task
[2023-07-15T12:14:41.513-0300] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'twitter_datascience', 'scheduled__2023-07-13T00:00:00+00:00', '--job-id', '100', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpvqgcm00w']
[2023-07-15T12:14:41.515-0300] {standard_task_runner.py:85} INFO - Job 100: Subtask twitter_datascience
[2023-07-15T12:14:41.599-0300] {task_command.py:410} INFO - Running <TaskInstance: TwitterDAG.twitter_datascience scheduled__2023-07-13T00:00:00+00:00 [running]> on host lucas-Estudos
[2023-07-15T12:14:41.764-0300] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='twitter_datascience' AIRFLOW_CTX_EXECUTION_DATE='2023-07-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-13T00:00:00+00:00'
[2023-07-15T12:14:41.778-0300] {base.py:73} INFO - Using connection ID 'twitter_default' for task execution.
[2023-07-15T12:14:41.780-0300] {twitter_hook.py:34} INFO - URL: https://labdados.com/2/tweets/search/recent?query=datascience&tweet.fields=author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,lang,text&expansions=author_id&user.fields=id,name,username,created_at&start_time=2023-07-13T00:00:00.00Z&end_time=2023-07-14T00:00:00.00Z
[2023-07-15T12:14:42.547-0300] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=twitter_datascience, execution_date=20230713T000000, start_date=20230715T151441, end_date=20230715T151442
[2023-07-15T12:14:42.611-0300] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-15T12:14:42.676-0300] {taskinstance.py:2653} INFO - 1 downstream tasks scheduled from follow-on schedule check
